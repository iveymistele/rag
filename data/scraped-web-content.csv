file_path,text
rc-website-fork/content/thank-you.md,"+++
date = ""2016-12-31T23:59:16-05:00""
images = [""""]
about = true
author = ""Staff""
description = """"
title = ""Support Request Submitted""
draft = false
type = ""about""
private = true
+++



  Thank you for contacting UVA's Research Computing group (RC). We will make every effort to respond to your request
  within 1 business day (9am-5pm EST, M-F, excluding holidays). System status updates are available on www.rc.virginia.edu.

"
rc-website-fork/content/resources.md,"+++
date = ""2022-02-02T23:59:16-05:00""
tags = [""support""]
categories = [""support""]
images = [""""]
about = true
author = ""Staff""
description = """"
title = ""Your Resources""
draft = true
type = ""resource""
private = true
+++




Hello 









Allocations
Review and manage your HPC allocations on Rivanna.



Name
Type
SUs Remain
% Remain





S = Standard  ¬†¬† P = Purchased ¬†¬† I = Instructional ¬†¬† D = DeanAllocation counts are refreshed 1x per day.

Request Allocations





Storage
Review and manage your Research Project or Research Standard storage shares.



Group Name
Type
Capacity





P = Project ¬†¬† V = ValueStorage quotas are refreshed 1x per day.

Request Storage ¬†
  




Container Services
Review and manage your microservices.



Service
Image
Count





Container inventories are refreshed 1x per day.

Request Microservices ¬†
  




Databases
Review and manage your databases.



Database
Type
Host





Database inventories are refreshed 1x per day.

Request a Database ¬†
  

"
rc-website-fork/content/search.md,"+++
date = ""2020-04-16T23:59:16-05:00""
tags = [""search""]
images = [""""]
author = ""Staff""
title = ""Search""
draft = false
type = ""search""
private = true
+++


"
rc-website-fork/content/name-contest.md,"+++
date = ""2022-02-02T23:59:16-05:00""
tags = [""support""]
categories = [""support""]
images = [""""]
about = true
author = ""Staff""
description = """"
title = ""Secure HPC Naming Contest""
draft = true
type = ""resource""
private = true
+++





Hello 









Allocations
Review and manage your HPC allocations on Rivanna.

Request Allocations


"
rc-website-fork/content/support.md,"+++
date = ""2023-01-31T23:59:16-05:00""
tags = [""support""]
categories = [""support""]
images = [""""]
about = true
author = ""Staff""
description = """"
title = ""Support Options""
draft = false
type = ""about""
aliases = [""/office-hours/""]
+++
{{< form-cookies >}}




Request Help
Open a support ticket with your specific questions or issues.

Open a Support Ticket





Allocations
Request, purchase, or modify your Afton/Rivanna HPC allocations.

Request Allocations





Storage
Manage your shared Research Project or Research Standard storage volumes.

Request Storage







Ivy Projects
Perform your research on a HIPAA-compliant computing platform.

Set up a Project





ACCORD
Request information or support for an ACCORD project. Learn more.

ACCORD Support





Container Services
Run your container-based service in our Kubernetes cluster. Learn more.

Request a Microservice







User Guides
Learn about systems, storage, data transfer, image analysis or tools.

Read the User Guides





FAQ / Knowledgebase
Search topics or post your own related to research computing.

Have a Question?





Pricing
Read more about pricing for SUs, Ivy virtual machines, storage, and more.

Learn More





Office Hours


Research Computing staff host weekly office hours. Tell us about a project idea or talk to us about our high performance computing platforms, storage and services. We're here to help you solve your computational problems.
Examples of the type of support we can provide are:

Data Transfer/Access
Parallel Coding in Fortran, C, Python, R, Matlab, Mathematica
Bioinformatics
Computational Chemistry
Software Installation and Containers
Image Processing
Writing Slurm Job Scripts
Maximizing Job Efficiency
Managing Computational Workflows

We offer office hours as online Zoom sessions twice a week. No appointments required.


Tuesdays 3:00-5:00pm
Join us 
via Zoom




Thursdays 10:00-12:00pm
Join us 
via Zoom



New to High-Performance Computing?
We offer orientation sessions to introduce you to the Afton & Rivanna HPC systems on Wednesdays (appointment required).


Wednesdays 3:00-4:00pm 

Sign up for an ""Intro to HPC"" 
session


"
rc-website-fork/content/signup.md,"+++
date = ""2016-12-31T23:59:16-05:00""
tags = [""signup""]
categories = [""""]
images = [""""]
about = true
author = ""Staff""
description = """"
title = ""Mailing List""
draft = false
type = ""about""
private = true
+++
Keep up to date with our latest news, updates, workshops and service announcements.






* indicates required

Email Address  *




First Name 



Last Name 





 







"
rc-website-fork/content/form/storage.md,"+++
date = ""2023-04-24T23:59:16-05:00""
tags = []
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Storage Request""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}
{{< form-cookies >}}



{{< enable-disable-form >}}








  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}

  

Type of Request *


 ¬† Create new storage share


 ¬† Increase size of existing share


 ¬† Decrease size of existing share


 ¬† Retire existing share




Space (TB) *

The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments.





Storage Platform *


 ¬† Research Project Storage ({{< extract_storage_cost type=""project"" >}})


 ¬† Research Standard Storage ({{< extract_storage_cost type=""standard"" >}})


 ¬† High-Security Research Standard Storage ({{< extract_storage_cost type=""hsz standard"" >}})



None of these storage options offer data backups or replication. Research Project storage provides week long snapshots of data. Snapshots are not available on Research Standard storage
Billing information is required. However, if you are within the 10TB of free Research Standard Storage, no charges will apply.




Internal Use / Public DataThis storage platform is appropriate for public or internal use data.
Sensitive / Highly Sensitive DataThis storage platform is appropriate for highly sensitive data such as HIPAA, FERPA, CUI, etc.




Grouper/MyGroups Ownership *

Grouper or MyGroups group name under your Eservices user ID. You will have access to Grouper management and will be able to add/remove users for your project.  Legacy MyGroups groups created before November 28th, 2023, can be accessed through the ‚ÄúLegacy MyGroups‚Äù folder on  Grouper.


Shared Space Name *

This is the name to be applied to your shared storage space. By default, the space will be named according to the Grouper/MyGroups group associated with the storage request. If you would prefer a different identifier, indicate the name for the space.

    {{% group_creation_tip %}}          
  


Project Title 


  {{% billing-fdm %}}
  
Data Agreement *

      The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
    


¬†¬† I understand
  


Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}




"
rc-website-fork/content/form/support-request-orig.md,"+++
date = ""2022-08-16T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Support Request""
draft = true
type = ""form""
private = true
+++





{{% form-userinfo %}}
  
Department/Organization *


 Academic Discipline *

- Select -
Astronomy
Biochemistry
Bioinformatics
Biology
Chemistry
Commerce
Computer Science
Data Science
Economics
Environmental Science
Engineering
Health Sciences
Informatics
Physics
Social Sciences
Other


 Other Academic Discipline




Support Category *

- Select -
General research computing question
HPC (Afton & Rivanna)
Ivy Secure Computing
Storage
Omero Image Analysis
Containerized Service
Consultation request
-----
CHASE Accounts/Data
Sentinel System/Software
Other

Use this form for general HPC support questions. Or submit an Allocation Request.
Use this form for storage questions. Or submit a storage request.
Use this form for general Omero questions. Or request Omero access.
Use this form for general queries about containerized services. Or request a container service.
Use this form for general Ivy questions. Or submit an Ivy Project Request.


Brief description of your request *



Details of your request * 








Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit






"
rc-website-fork/content/form/skyline.md,"+++
date = ""2020-09-11""
tags = [""skyline"",""virtual machine""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Skyline Virtual Machine Request""
draft = true
type = ""form""
private = true
+++
{{% jira-msg %}}
{{< form-cookies >}}


{{< enable-disable-form >}}

Skyline Virtual Machines (VMs) are designated for computation that involves public and moderately-sensitive data. Processing of highly sensitive data is not permitted. Learn about our Ivy environment for processing and storage of highly sensitive data that have HIPAA, ITAR, or CUI requirements.








  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo %}}
  
 Classification *
- Select -FacultyStaffPostdoctoral AssociateOther


Affiliation *

- Select -
College of Arts & Sciences
School of Data Science
School of Engineering and Applied Sciences
School of Medicine
Darden School of Business
UVA Health System
Other



 Name of MyGroups Account (lowercase only, no spaces). *

This group is used to define all members with access to the requested VM. You can use an existing group or set up a new MyGroup. 


Project Summary 


Please describe your project and the software you intend to use on your Skyline VM. 




VM Configuration *


 ¬† 2 cpu cores / 2GB memory ($4/month)


 ¬† 4 cpu cores / 16GB memory ($12/month)


 ¬† 8 cpu cores / 32GB memory ($48/month)


 ¬† 16 cpu cores / 64GB memory ($96/month)


 ¬† 16 cpu cores / 124GB memory ($176/month)






Operating System *







¬†CentOS 7.8:¬†Unlimited number of concurrent user logins.








Billing Tiers are selected and paid for by the PI. Submit this form again if you wish to change your VM configuration.

PTAO *



















Financial Contact *

Please enter the name and email address of your financial contact.



Data Agreement *

      The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
    


¬†¬† I understand
  


Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}





"
rc-website-fork/content/form/combined-request-form.md,"+++
date = ""2023-09-18T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Combined Request Form""
draft = false
type = ""form""
private = true
+++




{{% jira-msg %}}


{{< enable-disable-form >}}


















  {{% getstatus keyword=""jira"" %}}
  {{% form-userinfo-v2 %}}
  

PI/Owner UVA ID



Requestor ID (if different from User ID above)




Your Current Resources




Type
Project
Group
Resource Name
Tier
Size/Count
Status
Update Date








  No resources found to display.




Resource Type *

Allocation's (SU)
Storage







 Allocation's (SU) Request



New or Renewal *




New



Update/Renewal

If this is your first request, select New. Otherwise select Renewal.





Name of Grouper/MyGroups Account *

- Select a group -



Storage Grouper/MyGroups Account *

- Select a group -





Your Existing Service Units




Select
Project
Group
Resource Name
Tier
Size/Count
Status
Update Date











Project Nick Name *





Description of Research Project *
Briefly describe how you have used Rivanna/Afton in your research. Please include conference presentations, journal articles, other publications, or grant proposals that cite Rivanna. *







Tier Options *
For detailed information about each allocation tier option, please visit our Allocation Types Documentation.



Standard



Paid



Instructional





Additional SU's Requested *

The number of SU's requested.(Note: SU's  cannot be requested for Standard and Instructional resources but will be automatically applied/updated once submitted)




Storage Request





New or Change Existing*



Create new storage share



Update existing share



Retire existing share







Name of Grouper/MyGroups Account *

- Select a group -

Group names can only contain letters, numbers, dashes, and underscores (e.g., research-lab-1, data_science_2)





Your Existing Storage




Select
Project
Group
Resource Name
Tier
Size/Count
Status
Update Date











Project Nick Name *





Description of Research Project *
Briefly describe how you have used Rivanna/Afton in your research. Please include conference presentations, journal articles, other publications, or grant proposals that cite Rivanna. *







Tier Options *
For detailed information about each storage tier option, please visit our Storage Documentation.



SSZ Research Project ({{< extract_storage_cost type=""project"" >}})



SSZ Research Standard ({{< extract_storage_cost type=""standard"" >}})



High-Security Research Standard Storage ({{< extract_storage_cost type=""hsz standard"" >}})






Internal Use / Public DataThis storage platform is appropriate for public or internal use data.
          

Sensitive / Highly Sensitive DataThis storage platform is appropriate for highly sensitive data such as HIPAA, FERPA, CUI, etc.
          




 Total Space (TB) *

The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments.


 Free Space (TB) *

You have 10TB of free space, how much would you like to apply for this share?






Existing FDM's





Company
Cost Center
Business Unit
Funding Number
Fund
Function
Program
Activity
Assignee
Delete










+ New FDM



Payment Information





      {{% billing-fdm %}}
      
Add to FDM Details
Cancel






Data Agreement *

          The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
        


¬†¬† I understand
      


Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit
Cancel





{{< /enable-disable-form >}}




"
rc-website-fork/content/form/test-form.md,"+++
date = ""2019-06-30T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Test FDM Form""
draft = true
type = ""form""
private = true
+++







  {{% form-userinfo %}}
  
 Name of PI *



Is the PI of your account a UVA faculty member? *

 
 ¬†Yes

 
 ¬†No
        ¬†(Non-UVA personnel are charged $0.07/SU)
      


 I agree that this allocation will be used for research purposes only *

 
 ¬†Agree

 
 ¬†Disagree



 Title of Award (if applicable) 



 Total number of SUs requested *


 Total amount to be charged to FDM *

$





 SU expiration date (if applicable) 


 Apply this purchase to which allocation *



  {{% billing-fdm %}}
  

Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



"
rc-website-fork/content/form/README.md,"üèõ Combined Request Form - Research Computing
üìå Overview
The Combined Request Form is a dynamic web-based UI for Service Unit (SU) Requests and Storage Requests. The form handles new allocations and renewals, validates user inputs, generates API payloads, and submits requests using POST (New) and PUT (Renewal) methods.

üìñ Table of Contents

Form Structure
JavaScript Architecture
Event Handling & Interactivity
Fetching and Processing User Data
Payload Construction (New & Renewal Requests)
Form Submission & API Communication
Validation & Error Handling
UI Behaviors & Additional Features


üìë 1. Form Structure 
The form dynamically adjusts based on the selected request type.
Service Unit (SU) Requests

New Request: Users select a Group, Tier, and input billing details.
Renewal: Users select from an Existing SU Table, and only the update_date is changed.

Storage Requests

New Storage: Users pick a storage tier and specify allocation details.
Modify Storage: Users choose an existing project from the Existing Storage Table.

Billing Information

Displayed dynamically if a selected request requires billing.


‚öôÔ∏è 2. JavaScript Architecture 
The core JavaScript file combined-request-form.js manages:
- Dynamic form updates based on user selections.
- Fetching and displaying user resources.
- Validating inputs and preventing invalid submissions.
- Building and submitting API payloads.
Key JavaScript Functions
| Function | Description |
|-------------------------|----------------|
| fetchMetadata() | Fetches storage tier limits and billing rules from API. |
| fetchAndPopulateGroups() | Retrieves user groups and resources dynamically. |
| updatePayloadPreview() | Shows real-time request preview before submission. |
| validatePayload(payload) | Ensures API payload structure is correct. |
| submitForm(formData, payload) | Sends request via POST (New) or PUT (Renewal). |
| processUserResources(apiResponse) | Populates user‚Äôs Existing SU Table. |

üñ± 3. Event Handling & Interactivity 
JavaScript dynamically updates the UI based on user actions.
Key Event Listeners
| Event | Function Triggered | Description |
|-----------|------------------------|-----------------|
| Change on request-type | toggleRequestFields() | Toggles between Service Unit and Storage Requests. |
| Change on new-or-renewal | toggleAllocationFields() | Shows correct fields for New vs Renewal. |
| Change on existing-project-allocation | updatePayloadPreview() | Updates the API payload preview. |
| Change on any form input | updateBillingVisibility() | Shows/hides Billing Information. |
| Form Submit | handleFormSubmit(event) | Collects data, validates, and submits the request. |

üîÑ 4. Fetching and Processing User Data 
At page load, fetchAndPopulateGroups() retrieves:
- User Groups
- Existing Service Unit Allocations
- Storage Requests
Fetched data is stored in consoleData and updates:
1. Existing SU Table (populateExistingServiceUnitsTable())
2. Existing Storage Requests Table (if applicable)

üõ† 5. Payload Construction (New & Renewal Requests) 
New SU Requests (POST)

User selects Group and Tier.
Billing details are included if applicable.
Request count defaults to 1000 unless specified.
Payload structure:
json
[
  {
    ""group_name"": ""RC_Staff"",
    ""project_name"": ""Test Project"",
    ""project_desc"": ""This is free text"",
    ""data_agreement_signed"": true,
    ""pi_uid"": ""UVAComputingID"",
    ""resources"": {
      ""hpc_service_units"": {
        ""CACS_Staff"": {
          ""tier"": ""ssz_project"",
          ""request_count"": ""1000"",
          ""billing_details"": {
            ""fdm_billing_info"": [
              {
                ""financial_contact"": ""First Name Last Name"",
                ""company"": ""234324"",
                ""business_unit"": ""3"",
                ""cost_center"": ""224"",
                ""fund"": ""a Fund"",
                ""gift"": """",
                ""grant"": """",
                ""designated"": """",
                ""project"": """",
                ""program_code"": ""a program"",
                ""function"": ""A Function"",
                ""activity"": ""an activity"",
                ""assignee"": ""an assignee""
              }
            ]
          }
        }
      }
    },
    ""user_resources"": []
  }
]

Renewal Requests (PUT)

The user selects an existing SU instead of filling in a new group.
The only change allowed is updating the update_date.
The API payload includes the existing group, tier, and updated timestamp.
json
[
    {
        ""group_name"": ""RC_Staff"",
        ""project_name"": ""Existing Project"",
        ""resources"": {
            ""hpc_service_units"": {
                ""CACS_Staff-ssz_standard"": {
                    ""tier"": ""ssz_standard"",
                    ""request_count"": ""50000"",
                    ""update_date"": ""2025-02-12T10:30:00Z""
                    ""billing_details"": {
                        ""fdm_billing_info"": [
                            {
                                ""financial_contact"": ""First Name Last Name"",
                                ""company"": ""234324"",
                                ""business_unit"": ""3"",
                                ""cost_center"": ""224"",
                                ""fund"": ""a Fund"",
                                ""gift"": """",
                                ""grant"": """",
                                ""designated"": """",
                                ""project"": """",
                                ""program_code"": ""a program"",
                                ""function"": ""A Function"",
                                ""activity"": ""an activity"",
                                ""assignee"": ""an assignee""
                            }
                        ]
                    }
                }
            }
        }
    }
]


6. Form Submission & API Communication
The submitForm() function determines whether to POST or PUT based on the selected New vs Renewal option.


New Requests ‚Üí POST to:
https://uvarc-unified-service.pods.uvarc.io/uvarc/api/resource/rcwebform/user/{userId}


Renewal Requests ‚Üí PUT to:
https://uvarc-unified-service.pods.uvarc.io/uvarc/api/resource/rcwebform/user/{userId}/{resource_id}



7. Validation & Error Handling
Before submission, validatePayload(payload) checks for:

Missing required fields (group, tier, request count).
Duplicate group/tier combinations.
Correct formatting of billing details (when required).

Errors trigger showErrorMessage(), and invalid fields are marked red.

8. UI Behaviors & Additional Features
‚úÖ Real-Time Payload Preview

The function updatePayloadPreview() shows the request payload before submission.

‚úÖ Billing Visibility

The form automatically hides/shows billing details based on the selected SU or Storage option.

‚úÖ Sorting of Existing SU Table

Newest allocations appear first in the Existing SU table.
"
rc-website-fork/content/form/allocation-purchase.md,"+++
date = ""2023-04-22T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Purchase Service Units""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}


{{< enable-disable-form >}}









  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}
   Name of PI *



Is the PI of your account a UVA faculty member? *

 
 ¬†Yes

 
 ¬†No



 I agree that this allocation will be used for research purposes only *

 
 ¬†Agree

 
 ¬†Disagree



 Title of Award (if applicable) 



 Total number of SUs requested *


 Total amount to be charged to FDM *

$





 SU expiration date (if applicable) 


 Apply this purchase to which allocation *


      {{% group_creation_tip %}}
  

  {{% billing-fdm %}}
  
Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}




"
rc-website-fork/content/form/allocation-standard.md,"+++
date = ""2023-04-24T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Request or Renew a Standard Allocation""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}


{{< enable-disable-form >}}









  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}
   Name of Grouper/MyGroups Account *Lowercase only, no spaces, PI must create his/her Grouper group for new allocations.
  {{% group_creation_tip %}}
  




New or Renewal *


 
¬†New

 
¬†Renewal


If this is your first request, select New.  Otherwise select Renewal.

Standard allocations expire 12 months after they are disbursed.


Description of Research Project *
Briefly describe how you have used Rivanna/Afton in your research. Please include conference presentations, journal articles, other publications, or grant proposals that cite Rivanna. *







Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit


{{< /enable-disable-form >}}




"
rc-website-fork/content/form/support-request.md,"+++
date = ""2023-04-27T23:59:16-05:00""
tags = []
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Support Request""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}
{{< form-cookies >}}


{{< enable-disable-form >}}







  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}

  
Support Category *

- Select -
General research computing question
HPC (Afton & Rivanna)
Ivy Secure Computing
Storage
Container Service
Data Analytics Center
Digital Technology Core


Other

Use this form for general Rivanna support questions. Or submit an Allocation Request.
Use this form for storage questions. Or submit a storage request.
Use this form for general queries about containerized services. Or request a container service.
Use this form for general Ivy questions. Or submit an Ivy Project Request.
Use this form for questions related to services offered by our Data Analytics Center. Learn more about the Data Analytics Center.
Use this form for questions related to services offered by our Digital Technology Core. Learn more about the Digital Technology Core.

Brief description of your request *



Details of your request * 








Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}





"
rc-website-fork/content/form/support-request-vRedesign.md,"+++
date = ""2020-12-10T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Support Request""
draft = true
type = ""form""
private = true
+++





{{% form-userinfo-v2 %}}
  

Select a support category:
¬† General Support
¬† HPC (Afton & Rivanna)
¬† Ivy
¬† Consultation
Reset








Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm whether the submission completed or failed.
Submit




"
rc-website-fork/content/form/allocation-instructional.md,"+++
date = ""2023-09-12T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Request an Instructional Allocation""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}


{{< enable-disable-form >}}









  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}
   Name of Grouper/MyGroup Account *Lowercase only, no spaces, PI must create his/her Grouper group for new allocations.
    {{% group_creation_tip %}}
    



        Instructors are responsible for creating the class Grouper group and updating the roster for the chosen account through the Grouper portal.
     

 New or Renewal *


 
¬†New

 
¬†Renewal


If you have taught this same class before using Rivanna/Afton, select Renewal.



 Class ID *


 Academic Term *




 Class Size *
How many students are in your class? 


 Class Schedule *
What days/times does this class meet? Enter ‚Äún/a‚Äù if students will use the cluster at different times.



 Cores/Memory Required *
Estimate how many cores and how much memory each student will need to process his/her jobs. General descriptions are fine. A member of our user services team will contact you if we need additional information.





         PIs are eligible for 10TB of Research Standard storage at no charge. Instructors are encouraged to utilize this 10TB of storage for both research and teaching activities. 
        Read the full policy and guide for instructors.





Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}




"
rc-website-fork/content/form/containers.md,"+++
date = ""2023-05-11T23:59:16-05:00""
tags = []
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Container Service Request""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}



{{< enable-disable-form >}}








  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}
  
Project Summary 


Please describe your project and the container images you want to run.




Tier of Service *


 ¬† <= 5 containers ({{< extract_microservices_cost tier=light >}} total)


 ¬† 6 - 15 containers ({{< extract_microservices_cost tier=medium >}} total)


 ¬† > 15 containers ({{< extract_microservices_cost tier=heavy >}} total)




Billing Tiers are selected and paid for by the PI. Submit this form again if you wish to change your tier. Stopped containers do not incur charges, nor does local cluster storage or remote NFS mounts to /project storage. Project storage pricing can be found here.



Storage *


 ¬† No storage required


 ¬† Persistent cluster storage required


 ¬† NFS mount of project storage is required




Storage Capacity (GB)

The size of storage if required. Specify in 1GB increments.




SSL/HTTPS Required *


 ¬† No


 ¬† Yes




Netbadge Authentication *


 ¬† No


 ¬† Yes




  {{% billing-fdm %}}
  
Data Agreement *

      The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
    


¬†¬† I understand
  


Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}





"
rc-website-fork/content/form/accord.md,"+++
date = ""2021-06-10T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""ACCORD Support Request""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}
{{< getstatus keyword=""jira"" >}}


{{< enable-disable-form >}}









Name *




E-mail *




University/Institution *



Department/Organization *



Brief description of your request *



Details of your request * 








Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}



"
rc-website-fork/content/form/sns-test.md,"+++
date = ""2021-04-10T23:59:16-05:00""
tags = [""database""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Database Service Request""
draft = true
type = ""form""
private = true
+++

here is a response





{{% form-userinfo %}}
  




Group





Family




Submit




"
rc-website-fork/content/form/dedicated-computing.md,"+++
date = ""2024-01-4T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Dedicated Computing Request""
draft = false
type = ""form""
private = true
+++
{{% jira-msg %}}


{{< enable-disable-form >}}








  {{% getstatus keyword=""jira"" %}}

  {{% form-userinfo-v2 %}}

   Name of Grouper/MyGroup Account *Lowercase only, no spaces, PI must create his/her Grouper group for new allocations.
    {{% group_creation_tip %}}
    



    Type and quantity of hardware requested (Check out the specifications) 

  {{< dedicated-options-form >}}



    Proposed Start Date (Subject to Resource Availability)
  



Start Date



End Date







 Please select a valid start and end date. 


  {{% billing-fdm %}}
  
Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



{{< /enable-disable-form >}}





"
rc-website-fork/content/form/support-request-attachments.md,"+++
date = ""2022-08-16T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Support Request WITH ATTACHMENTS""
draft = true
type = ""form""
private = true
+++





{{% form-userinfo %}}
  
Support Category *

- Select -
General research computing question
HPC (Afton & Rivanna)
Ivy Secure Computing
Storage
Omero Image Analysis
Containerized Service
Consultation request
-----
CHASE Accounts/Data
Sentinel System/Software
Other

Use this form for general HPC support questions. Or submit an Allocation Request.
Use this form for storage questions. Or submit a storage request.
Use this form for general Omero questions. Or request Omero access.
Use this form for general queries about containerized services. Or request a container service.
Use this form for general Ivy questions. Or submit an Ivy Project Request.


Brief description of your request *



Department/Organization *



Details of your request * 







Attachments

Upload files
Select or drag files or screenshots here





Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit






"
rc-website-fork/content/post/2025-may-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2025-05-13T00:00:00-05:00""
title = ""HPC Maintenance: May 27, 2025""
draft = false
tags = [""rivanna"", ""afton""]
categories = [""feature""]
+++
{{< alert-green >}}The UVA HPC systems, Afton/Rivanna, will be down for maintenance on Tuesday, May 27, 2025 beginning at 6 am.{{< /alert-green >}}
All systems are expected to return to service by Wednesday, May 28 at 6 am.
How should I prepare and what to expect on May 27, 2025?
You may continue submitting jobs to the HPC system until the maintenance period begins. However, if the system determines your job won't finish in time, it will not start until the system is back online. The upcoming maintenance will include upgrades to the HPC image (upgrading to Rocky 8.10), the HPC scheduler (Slurm 24.11), the NVIDIA driver and Open OnDemand. To complete these tasks, all compute nodes and login nodes - including the Open OnDemand and FastX portals ‚Äì will need to be taken offline. However, Research Project Storage and Research Standard Storage will remain accessible via SMB and NFS mounts. Additionally, the UVA Standard Security Storage Data Transfer Node (DTN) will stay operational throughout the maintenance period. 
If you have any questions about the upcoming HPC system maintenance, you may contact our user services team. 
IMPORTANT MAINTENANCE NOTES
System upgrades

Operating system: Rocky 8.10
Slurm: 24.11.3
NVIDIA driver: 570.124.06 (CUDA 12.8)
Open OnDemand: 4.0

Modules
The software stack is migrated from /sfs/applications to /sfs/gpfs/tardis/applications and the /apps symbolic link is updated accordingly. Unless the full path is required, users should simply use /apps.
Default version changes include:
- cuda/12.4.1 ‚Üí 12.8.0
- miniforge/24.3.0-py3.11 ‚Üí 24.11.3-py3.12
- namd/2.14 ‚Üí 3.0.1
- nvhpc/24.5 ‚Üí 25.3
- R/4.4.1 ‚Üí 4.5.0
(The original defaults are not removed.)
{{< table title=""replacement"" class=""table table-striped"" >}}
| Module | Remove | Replace with |
|---|---|---|
|afni       | 23.1.10 | 25.0.12 |
|bracken    | 2.9 | 3.1 |
|cellranger | 8.0.0 | 9.0.1 |
|code-server| 4.92.2 | 4.99.1 |
|cmake      | 3.23.3, 3.24.3 | 3.28.1, 4.0.0 |
|cuda       | 10.2.89, 11.8.0 | 12.8.0 |
|cudnn      | 8.2.4.15, 8.9.7 | 8.9.4.25, 9.8.0-CUDA-12.8.0 |
|cutadapt   | 3.4 | 4.9 |
|fastqc     | 0.11.5 | 0.12.1 |
|fmriprep   | 23.1.4 | 25.0.0 |
|fsl        | 6.0.7.6 | 6.0.7.17 |
|gcc        | 13.3.0 | 14.2.0 |
|globus_cli | 3.11.0 | 3.34.0 |
|go         | 1.21.4 | 1.23.6 |
|intel      | 2024.0 | 2025.0 |
|jcuda      | 11.4.1 | - |
|jupyterlab | 3.6.3-py3.11 | 4.4.1-py3.12 (see note below) |
|kraken2    | 2.1.3 | 2.1.5 |
|mathematica| 11.2 | 14.2 |
|matlab     | R2022b | R2024b |
|multiqc    | 1.14 | 1.27.1 |
|nextflow   | 23.04.1 | 24.10.5 |
|openmpi    | 4.1.4-nofabric[-withoutverbs] | 4.1.4 |
|openmpi    | 4.1.5-intel | 4.1.5 |
|perl       | 5.36.0 | 5.40.2 |
|pytorch    | 1.12.0, 2.0.1 | 2.4.0, 2.7.0 |
|ruby       | 3.1.2 | 3.4.3 |
|smrtlink   | 13.1.0.221970 | 25.2.0 |
|spaceranger| 3.1.1 | 3.1.3 |
|sumo       | 1.14.1 | 1.22.0 |
|texlive    | 2023 | 2025 |
|tmux       | 2.5 | 3.4 |
|tree       | 1.8.0 | 2.2.1 |
|trimgalore | 0.6.4 | 0.6.10 |
{{< /table >}}
Post-Maintenance Note

Due to issues with accessing subfolders in the new JupyterLab, we have reverted to the pre-maintenance version on Open OnDemand. However, the issue is not observed when Jupyter is launched manually from the command line, and so the new version is still kept as a module for those who wish to launch it manually (e.g. within a Desktop session).
"
rc-website-fork/content/post/2022-04-women-in-hpc.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-04-15T10:18:25-05:00""
title = ""Virginia Women in HPC - Panel Discussion, April 27""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
Please join us for a lively panel discussion of the high-performance computing infrastructure and support resources available to researchers in the Commonwealth of Virginia. Panelists from Virginia's top research universities will provide an overview of the user base at their home institutions and discuss strategies for helping researchers make better use of existing HPC resources. Attendees are encouraged to share their own experiences and engage with panelists during this interactive Q&A session.
*Topic: High-performance Computing Resources in the Commonwealth of Virginia
When: April 27, 2022 01:00 PM, Eastern Time (US and Canada) 


Featured panelists:


Matthew Brown (VT) 


Jayshree Sarma (GMU)


Jacalyn Huband (UVA)


Eric Walter (W&M)


Carol Parish (U. Richmond)


Mike Davis (VCU)


Virginia WHPC is committed to increasing diversity and inclusion by promoting and encouraging the participation of women in high-performance computing and related fields."
rc-website-fork/content/post/2019-summer-workshops.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-04-23T15:18:25-05:00""
title = ""Summer 2019 Education Series""
draft = false
tags = [""education"",""workshops"",""feature""]
categories = [""feature""]
+++

RC staff are teaching a series of free hands-on workshops this summer that are open to all UVA researchers. Space is limited, so register today! Topics include:

Programming in Python (June 3-June 5)
R (June 3-June 4)
MATLAB (June 5-June 7 and June 13)
Compiled Languages, C++ and Fortran (June 6-June 7)
Scientific Image Processing with Fiji/ImageJ (June 10)
Introduction to High-Performance Computing (June 10)
Software Design and Testing (June 11)
HPC Data Analytics (June 11)
Parallel Programming Using MPI (June 12-June 13)
Bioinformatics (June 12)
Scientific Visualization (June 14)
OpenMP and Accelerators (June 14)

Register through the new CADRE Academy portal. You can register for classes, follow tracks or workshops that are not yet scheduled, and find other
related workshops.
For a comprehensive list of all such educational opportunities across Grounds, visit the Research & Data Services Training Portal.

Learn More
Would you like to learn the basics of high-throughput and parallel computing? Do you want hands-on high performance training that you can apply to your research? If so, then we invite you to attend one or more sessions in ARCS' Summer Education Series. The sessions are free for all UVA faculty, staff, and students. Topics include how to program in Python, scientific visualization, modern Fortran, data analytics, MPI for distributed systems, OpenMP for multicore systems, and more! Lectures on relevant topics will be delivered during the morning sessions; afternoon sessions offer hands-on practice.


June 3-June 5: Programming in Python - Go from beginner to proficient in three days. Day 1: Basics of programming in Python. Day 2: Modules and packages. NumPy, SciPy, and Pandas. Day 3: Dictionaries and classes. Some computing experience is helpful but not required. Mornings are lectures and hands-on exercises, with afternoon labs focused on programming applications to cement your skills.


June 3-June 4: R - Day 1 (R for data science): R is one of the most ubiquitous statistical computing languages. In this session, you will be introduced to the language's fundamental data structures as well as how to load packages and use functions. Topics include: composing scripts, reading data into R, and basic data manipulation and visualization techniques using the ""tidyverse"" set of packages (dplyr,tidyr,ggplot2). Day 2 (R as a programming language): Need to improve your skills with manipulating data in R?  In R as a programming language, you will learn how to make R do more work for you. Topics include decision-making, repetition, functions, and optimizations (such as vectorization).


June 5-June 7: MATLAB - Day 1: MATLAB fundamentals (no prior MATLAB experience required). Day 2: MATLAB programming techniques. Provides hands-on experience using the features in the MATLAB language to write efficient, robust, and well-organized code. These concepts form the foundation for writing full applications, developing algorithms, and extending built-in MATLAB capabilities. Details of performance optimization, as well as tools for writing, debugging, and profiling code are covered. Day 3: MATLAB for data visualization. Focuses on importing and preparing data for data analytics applications. The workshop is intended for data analysts and data scientists who need to automate the processing, analysis, and visualization of data from multiple sources. 


June 6-June 7: Compiled Languages - The basics of C++ and Fortran will be taught in this accelerated two-day course. Prerequisite: experience programming in some language. Mornings will be hands-on and lecture, with afternoon programming labs.


June 10: Intro to High-Performance Computing - This all-day session is an in-depth introduction to using the Rivanna HPC cluster. Topics covered will include an introduction to the Unix command line, writing Slurm scripts, and high-throughput computing.


June 10: Scientific Image Processing with Fiji/ImageJ - This hands-on workshop is an introduction to using Fiji, an open-source and enhanced version of the popular ImageJ program used for scientific image processing. During the morning session, participants will be introduced to image processing filters, strategies for image background correction, as well as identification and analysis of image objects of interest using segmentation masks. During the afternoon session, participants will learn how to write scripts for automated execution of image processing pipelines and batch processing of multiple image files in Fiji. Example scripts will be provided using the Python and BeanShell languages. Participants should bring their own laptop with Fiji preinstalled. Instructions for installations of Fiji on Windows, Mac OSX, and Linux can be found on this website: https://fiji.sc/


June 11: HPC Data Analytics - An overview of data analytics using an HPC Cluster. Topics include data transfer and storage; introduction to machine learning, including tensorflow. 


June 11: Software Design - Learn to write code that is readable, reusable, and extensible.


June 12-June 13: Parallel Programming Using MPI - The Message-Passing Interface is the standard for distributed parallel programming. MPI programs can run on multiple nodes of a cluster. Day 1: Basics of parallel programming. Collective Communications. Day 2: Point-to-point communications.


June 12: Bioinformatics - Next-generation sequencing technology has evolved dramatically to enable investigation of genome/transcriptome/epigenome of any organism. Sessions in this track will introduce researchers to basic skills needed to analyze sequencing data using Rivanna, UVA's HPC cluster.


June 13: Accelerating and Parallelizing MATLAB Code - Covers a variety of techniques for making your MATLAB code run faster. You will identify and remove computational bottlenecks using techniques like preallocation and vectorization. On top of that, you will take advantage of multiple cores on your computer by parallelizing for-loops with Parallel Computing Toolbox, and scale up across multiple computers using MATLAB Parallel Server. 


June 14: Parallel Programming with OpenMP and Accelerators - OpenMP is a programming model based upon threads that run over multiple cores of one node. OpenMP can also be used to program general-purpose graphical processing units (GPGPUs). OpenACC, another compiler-based model for GPGPUs, will also be discussed.


June 14: Scientific Visualization - An exploration of tools used for visualization, including ParaView for scientific visualization, ITK-SNAP for medical visualization, and Python for remote sensor data visualization.

"
rc-website-fork/content/post/job-postings.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2025-02-05T01:18:25-05:00""
title = ""RC Job Openings""
draft = false
tags = [""uvarc"",""infrastructure"",""user-services""]
categories = [""feature""]
+++
Data Analytics Center Positions


Research Scientist (AI Specialist)


Research Scientist (Image Processing Specialist)

"
rc-website-fork/content/post/2024-women-in-hpc-20240903.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-08-20T10:18:25-05:00""
title = ""Virginia Women in HPC - Building and Maintaining Supportive Communities""
draft = false
tags = [""whpc"",""va-whpc"",""hpc""]
categories = [""feature""]
+++
When: Sep 03, 2023, 01:00 PM EST (US and Canada).
What: Join us for a first of its kind Women in High Performance Computing (WHPC) event, co-hosted by the Northeast, Purdue, and Virginia WHPC chapters to engage in discussions about current challenges and opportunities for fostering a more diverse and inclusive WHPC community. Our panelists will provide an overview of their chapters‚Äô community activities and offer insights into navigating hurdles and fostering professional growth within HPC. 
The format for this event will be a brief presentation by the hosts followed by theme-focused breakout rooms where participants will be invited to share their experience and personal perspective.
Please join us for this critical dialogue! 


Panelists:
* Heather Baier, Graduate Student ‚Äì Arts & Sciences, William & Mary
* Krista Valladares, MSM, Associate Director of Technology Strategy and Planning ‚Äì Harvard University
* Paula Sanematsu, Ph.D., Sr. Research Computing Facilitator ‚Äì FAS Research Computing, Harvard University
* Kaylea Nelson, Ph.D., Director, Arts & Sciences Research Computing ‚Äì Yale University 
* Laura Theademan, Director, Center Operations and Visualization ‚Äì Rosen Center for Advanced Computing, Purdue University
Discussion Topics:
* The role of mentors.
* Engaging allies.
* What topics/types of events are we covering (or want to cover) within our chapters?
* What gaps do you see in our programming or outreach efforts, and how can we fill them?
* What would you like to see from the WHPC chapters?
* Current challenges in gender diversity and inclusivity within HPC.
Code of conduct: We welcome and value the diverse perspectives and experiences of all participants, ensuring a respectful and inclusive environment for meaningful dialogue."
rc-website-fork/content/post/2024-february-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-01-29T00:00:00-05:00""
title = ""Rivanna Maintenance: February 6, 2024""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna, Research Project storage, and Research Standard storage will be down for maintenance on Tuesday, February 6 beginning at 6 a.m. {{< /alert-green >}}
You may continue to submit jobs to Rivanna until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Wednesday, February 7.
UVA‚Äôs Facilities Management (FM) group will be updating the data center power grid during the maintenance period. This work is expected to be completed by 6 a.m. on 2/7."
rc-website-fork/content/post/2024-women-in-hpc-20241105.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-10-24T00:00:00-05:00""
title = ""Virginia Women in HPC - Student Lightning Talks: November 12, 2024""
draft = false
tags = [""whpc"",""va-whpc"",""hpc""]
categories = [""feature""]
+++
On November 12, 2024 from 1-2 p.m. EST, Virginia Women in High Performance Computing (VA-WHPC) will be hosting 8 students as they present research lightning talks to members of the HPC community from across Virginia. See here for details.
Interested in giving a lightning talk? You‚Äôll get 3 minutes and 1-2 slides to tell us all about your research. Don‚Äôt miss this chance to practice your presentation skills and share your research with a diverse audience! Sign up here by the end of today, October 24th! A helpful guide on how to give a successful lightning talk is available here
This is a fantastic opportunity to explore a diverse range of topics in High-Performance Computing (HPC) and engage with the next generation of researchers. Don‚Äôt miss out on this insightful event!
"
rc-website-fork/content/post/2025-DAC-Fellowship.md,"+++
title = ""Data Analytics Center / Digital Humanities Center Fellowship""
description = """"
author = ""RC Staff""
date = ""2025-02-28T12:57:24-05:00""
tags = [""rc""]
categories = [""feature""]
draft = false
+++
The Scholars‚Äô Lab, the UVA Library Digital Humanities Center (DHC), and the Data Analytics Center (DAC) are delighted to announce a new fellowship opportunity.
The 2025 Data Analytics Center-Digital Humanities Center Fellowship is open for proposals from UVA faculty and graduate students. This is an experimental iteration encouraging the use of high-performance computing resources in the humanities, as well as identifying digital humanities research and teaching that could benefit from support from DHC and DAC. Possible projects might utilize AI, gaming platforms, imaging tools, geospatial technologies, use of new tools, and more.
The awarded team will consist of one faculty and one graduate student collaborating on humanities research in the University of Virginia. For more information, see the complete CFP. Applications are due April 1, 2025. We highly recommend all interested applicants (either individually or as a team) email us (lb-dac-dhc-fellowship@virginia.edu) to discuss ideas, budget possibilities, and proposed collaborations."
rc-website-fork/content/post/2024-rivanna-maintenance-dates.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-05-13T00:00:00-05:00""
title = ""Rivanna Maintenance Schedule for 2024""
url = ""/maintenance""
draft = false
tags = [""rivanna"", ""afton""]
categories = [""feature""]
+++
Rivanna/Afton will be taken down for maintenance in 2024 on the following days: 

Tuesday, February 6
Tuesday & Wednesday, May 28 & 29
Tuesday, July 2
Tuesday, October 15
Thursday, December 12 -> Postponed to Tuesday, January 7, 2025


Please plan accordingly. Questions about the 2024 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2022-08-accord-commmunity-meeting.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-08-01T10:18:25-05:00""
title = ""ACCORD Community Meeting - August 5, 2022""
draft = false
tags = [""accord"",""k8s"",""hipaa"",""security"",""nsf""]
categories = [""feature"",""accord""]
+++
When: Aug 5, 2022 11:00AM-3:00PM, Eastern Time (US and Canada)
UVA is proud to sponsor a community meeting to discuss ACCORD, how far we have come and where we are headed. The event is all virtual, and you 
may attend any or all of the topics.

Access the Event:


You are welcome to invite colleagues to attend the Community Meeting, especially researchers who are new to ACCORD.  

Agenda

11:00 ‚Äì 11:10 Welcome to ACCORD   
11:10 ‚Äì 12:00 Technical Overview of ACCORD  
12:00 ‚Äì 12:15 Break 
12:15 ‚Äì 12:45 Demo of ACCORD 
12:45 ‚Äì 1:30 Brown bag lunch with users sharing their experience 
1:30 ‚Äì 1:50 Breakout Room Topic Discussions Session 1
1:50 ‚Äì 1:55 Switch Breakout Rooms 
1:55 ‚Äì 2:15 Breakout Room Topic Discussions Session 2
2:15 ‚Äì 2:30 Break 
2:30 ‚Äì 3:00 Peek into the Future of ACCORD

The same topics will be covered in the two breakout sessions so that you may attend two of the topics.  
Breakout Room Topics:

Security Architecture  
RUDAs/MOUs  
Software Applications  


Send your questions to ACCORDSUP@virginia.edu. See you on August 5th!"
rc-website-fork/content/post/2020-june-r_updates.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-06-16T00:00:00-05:00""
title = ""R Updates: June 17, 2020""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
During the June maintenance, we made changes to R which will affect how your R programs run on Rivanna. A brief description of the changes is as follows:


The gcc-built versions of R were updated to goolf-built versions.
The locations of the R libraries were updated.
The versions of R have been streamlined to 3.4.4, 3.5.3, 3.6.3, and 4.0.0.

1. The gcc-built versions of R have been updated to goolf-built versions.
Instead of loading gcc before loading R, you will need to load goolf or gcc openmpi.  For example:  module load goolf R/4.0.0. 
Remember to update any Slurm scripts that have module load gcc R or module load gcc R/3.x.x.  
2. The locations of the R libraries have been updated.
We are changing the locations of the R libraries (i.e., the folders where local packages are installed).  This change will create separate folders for different compiler versions of R, which will prevent package corruption.
As a result, R will not see the packages that you had installed before the maintenance.  (The only exception would be gcc openmpi R/4.0.0, which already uses the new library location).  You will need to reinstall your R packages.  To help with this effort, we are providing a script that will scrape the list of packages installed in an older library and will attempt to install these packages in the new library. Details are provided at ""New Libraries"".
3. The versions of R have been streamlined to 3.4.4, 3.5.3, 3.6.3, and 4.0.0.
If you had hard-coded another version of R in your scripts (e.g., R/3.6.1), you will need to update your scripts to specify one of these newer versions.
To see what modules would need to be loaded prior to loading R, you can use the module spider command (e.g.,module spider R/3.6.3)."
rc-website-fork/content/post/scratch-policy.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-10-08T15:18:25-05:00""
title = ""/scratch Policy Enforcement""
draft = false
tags = [""scratch""]
categories = [""feature""]
+++
Beginning 10/14/2019 RC system engineers will begin actively clearing /scratch files that have not been accessed for 90 days. /scratch is intended as a temporary work directory (90 days maximum).
It is not backed up and needs to be purged periodically in order to maintain a stable HPC environment. We encourage users to back up their important data.
RC offers several low-cost storage options to researchers.
For more information about research computing storage options:

Visit our Storage Overview page.
Learn more about specific storage features of Rivanna HPC.
"
rc-website-fork/content/post/2025-04-RCExhibition-Awards.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2025-05-15T00:00:00-05:00""
title = ""2025 Exhibition Award Winners: Recognizing Computational Research at UVA""
draft = false
tags = [""projects"",""hpc""]
categories = [""feature""]
+++
{{% lead %}}
Research Computing, a team within UVA Information Technology Services, recently held its second annual Research Computing Exhibition on April 23. This event showcased how researchers at UVA are pushing the boundaries of innovation using Research Computing‚Äôs services and resources.
{{% /lead %}}
Featuring research presentations, faculty lightning talks, and a poster competition, the exhibition underscored the growing significance and impact of computational research across the University. 
More than 30 students and scholars presented posters highlighting research powered by UVA Research Computing‚Äôs resources. Their work spanned a wide range of fields ‚Äî including environmental science, astronomy, cancer genomics, digital health, and chemical engineering ‚Äî demonstrating the breadth of disciplines benefiting from advanced computational tools. Projects included analyzing forest dieback along the Atlantic Coast, predicting health outcomes for newborns in intensive care, and investigating the chemistry of stars. 
In addition to the posters, faculty from across UVA delivered lightning talks‚Äîbrief, high-level presentations on how they are integrating Research Computing tools into their research. Topics ranged from exploring the universe to advancing healthcare and molecular biology. 
The poster competition was judged by a cross-disciplinary panel of UVA faculty, who evaluated submissions in two categories: Biological & Health Science and Engineering & Physical Sciences. Evaluations were based upon three criteria: 1) the extent to which Research Computing resources contributed to the research, 2) how clearly the use of those resources was presented on the poster, and 3) the clarity and significance of the scientific findings. 
Below are this year's winners of the poster competition. The winners receive a travel voucher to attend a conference of their choice where they can present their work.  
Thank you to all the researchers for sharing your brilliant insights and compelling results with us!
Biological & Health Science

#1: Henry Yeung
Graduate Student, Department of Environmental Sciences
Poster: Extensive Ghost Forest Formation Across the US Atlantic Coast 
Abstract:

Rising sea levels and climate-driven stressors are transforming coastal forests, reducing their ability to sequester carbon, protect against storms, and provide other services (e.g. timber) to coastal communities. Yet, the true extent and drivers on coastal forest loss remain poorly quantified. By mapping over 10 million individual dead trees across the US Atlantic region, we reveal pervasive mortality even in areas previously deemed resistant to sea-level rise ‚Äì‚Äì an extent far greater than previous assessments had recognized. Previous studies only based their findings on a few well-studied sites with mass dieback, largely overlooking landscape-scale increase in tree mortality. By integrating deep learning and sub-meter imagery with the computing capacity of the Rivanna high-performance computing, we, for the first time, expose fine-scale vulnerability patterns that are invisible to traditional satellites across 1.2 million hectares of low-lying forests (< 5m), resolving untested assumptions on the roles of salinity versus flooding at regional scale. We find that these at-risk areas experience mortality fourfold higher than upland ecosystems, driven mainly by salinization. Our findings highlight an alarming yet underestimated scale of coastal ecosystem loss, offering scientists and policymakers urgently-needed insights and tangible pathways to preserve these vital habitats. 


#2: Navya Annapareddy
Graduate Student, UVA School of Data Science (PhD) 
Poster: A New NICU: High Performance Compute Enabled Digital Twins for Real Time Infant Monitoring 
Abstract:

1 in 10 infants are born prematurely globally and receive specialized care in environments like the Neonatal Intensive Care Unit. Preterm infants are at exponentially higher risk for developmental disorders, such as cerebral palsy and autism at rates tens to hundreds of the general population. The most common diagnostic for such disorders is the general motion assessment (GMAs) heuristic carried out manually by clinicians for preterm infants to assess neurodevelopmental (NVD) risk. As formal screening and diagnosis methods are limited by availability of trained clinicians and NICU size, we propose using computer assisted digital twins for automatic, even remote, risk assessment using a computer vision machine learning (CVML) model. We successfully develop the first clinical pose estimation framework for infants in clinical care settings meant for real-time streaming contexts. Our two-step sequential framework is comprised of two distinct models: (1) a CNN model for pose estimation of anatomic key points, and (2) a digital twin model prototype to reproduce the pose estimation CNN results in high fidelity. Each frame of a video is processed by the model pipeline and is only enabled by the vastly different capabilities of the HPC ecosystem, ranging from data storage, labeling, manipulation, training, and even validation and low latency deployment onto edge devices such as clinical tablets and computer devices. 


#3 Jisu Shin
Graduate Student, Biochemistry and Molecular Genetics, Biomedical Sciences Graduate Program 
Poster: Multi-modal Single-cell Sequencing Reveals Cellular Heterogeneity in LGL Leukemia 
Abstract:

Large granular lymphocytic leukemia (LGLL) is a rare lymphoproliferative disorder characterized by clonal expansion of cytotoxic lymphocytes. To investigate the cellular heterogeneity and molecular mechanisms of LGLL, we performed single-cell multi-omics analysis, integrating transcriptomic, proteomic, and TCR sequencing data from patients treated at UVA, a national referral center for this rare disease. Given the scale and complexity of the data, our analyses required UVA‚Äôs high-performance computing (HPC) resources, Rivanna and Afton. Parallelization and batch corrections in pre-processing steps significantly accelerated data analysis, making large-scale single-cell processing feasible. However, downstream analyses required careful step-by-step validation and could not be automated through job submissions. The ability to run interactive jobs with large memory and multiple cores was essential for ensuring accuracy and efficiency in data integration and interpretation, enabling insights into LGLL pathogenesis that would be computationally infeasible on standard systems. 
Engineering & Physical Sciences

#1: M√©lisse Bonfand-Caldeira
Post-Doctoral Fellow, Departments of Astronomy and Chemistry 
Poster: Unlocking the chemistry of stars with high-performance computing 
Abstract:

Complex organic molecules, the elemental building blocks of life, are known to form in space within giant clouds of interstellar gas and dust that eventually give rise to stars and planets. To understand how these molecules form and evolve, surviving the harsh conditions of space, until they eventually become incorporated into planetary systems, we harness the computing power of Rivanna/Afton to run thousands of numerical simulations, modeling the chemistry of various star-forming environments. We have developed an automated pipeline that then transforms astrochemical model outputs into synthetic data, mimicking real astronomical observations. By comparing observations of star-forming regions with a large grid of synthetic data, we are able to identify the models that best reproduce the observations, unveiling the region‚Äôs key physical properties and the chemical reactions that shape its molecular complexity. 


#2: Md Jakir Hossen
Graduate Student, Chemical Engineering 
Poster: Elucidating Biomolecular Surface Hydrophobicity 
Abstract:

Many biophysical processes are driven by the removal of water molecules, known as hydrophobic interactions, near complex biomolecular surfaces such as proteins, enzymes, and membranes. While the hydrophobicity of individual non-polar, polar, and charged amino acids is relatively simple to predict, collective water-mediated interactions on heterogeneous surfaces remain challenging due to non-additive effects. In this study, we designed self-assembled monolayers (SAMs) with varying spacings of hydroxyl, ammonium, and guanidinium groups‚Äîcommon in amino acid sequences‚Äîto investigate hydrophobic behavior in a non-polar environment. Using molecular dynamics (MD) simulations on Rivanna's high-performance computing resources, we analyzed the binding affinity of a model hydrophobic protein to these surfaces and employed indirect umbrella sampling (INDUS) simulations to compare their hydrophobicity. Our findings reveal the role of hydrogen bonding dynamics, binding affinity, and the free energy of water removal, emphasizing the impact of functional group spacing. These insights provide a foundation for studying more realistic peptide-functionalized surfaces, where such functional groups are prevalent in a non-polar background."
rc-website-fork/content/post/2020-dcos-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-05-31T10:18:25-05:00""
title = ""DCOS Maintenance Postponed""
draft = true
tags = [""dcos"",""containers"",""maintenance""]
categories = [""feature""]
+++
The DCOS cluster maintenance scheduled for 6/4/2020 has been postponed. A new date and time for this system outage will be announced in the near future."
rc-website-fork/content/post/2018-spring-workshops.md,"+++
images = [""""]
author = ""RC Staff""
description = """"
date = ""2018-01-24T15:18:25-05:00""
title = ""Spring 2018 Workshops""
draft = true
tags = [""education"",""workshops"",""feature""]
categories = [""feature""]
+++

School of Medicine Research Computing provides training opportunities covering a variety of data analysis, basic programming 
and computational topics. 
Workshops break roughly into the three main areas relevant to computationally-intensive research: code, data, and computing.
All of the classes are taught by RC experts and are freely available to UVa faculty, staff and students.

R / R package development
Python
Matlab
Biomedical Image Processing
Bioinformatics on HPC
Data manipulation
Data visualization
Databases
Machine Learning
Cloud Computing
Containers
Rivanna (HPC)
Ivy (Secure Computing)

View Workshops"
rc-website-fork/content/post/2022-knl.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-06-16T11:18:25-05:00""
title = ""KNL Nodes and Partition to be Removed from Rivanna on June 30, 2022""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
Rivanna has included Intel KNL (Knight's Landing) nodes for several
years. This was a unique architecture, not well suited for general
use, and the manufacturer has stopped producing or supporting this type
of hardware. As a result, the KNL nodes will be removed from Rivanna
on June 30, 2022.
Rivanna System Details"
rc-website-fork/content/post/2020-september22-matlab-seminar.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-09-09T00:00:00-05:00""
title = ""Deep Learning for Neuroscience - Live Seminar: September 22, 2020""
url = ""/maintenance""
draft = false
tags = [""rivanna"",""Matlab"",""education"",""workshops"",""feature""]
categories = [""feature""]
+++
MathWorks engineers will offer a free live webinar on September 22nd from 2:00 to 3:30 Eastern time.



Overview
Deep learning can achieve humanlike accuracy at tasks such as naming objects in a scene or recognizing optimal paths in an environment.  Sometimes it can even exceed human performance, recognizing non-obvious patterns in image or signal data.
In this new neuroscience seminar, we‚Äôll illustrate the fundamentals of deep learning in MATLAB.  Using an age-labeled BIDS dataset from the OpenNeuro repository, we‚Äôll train a deep network to accurately classify the age range of normalized human MRI brain images, not obviously discernible by human inspection.
Highlights
Along the way, participants will learn many aspects of the deep learning workflow:

Load and manage large sets of images
Import pre-trained models such as ResNet
Set up transfer learning via network modification
Get to network training quickly with apps for preprocessing and augmenting training image data
Configure network training parameters
Validation of convergence during deep model training
Interoperability with open source deep learning frameworks (i.e.,TensorFlow-Keras, Caffe, PyTorch, etc.,) using ONNX
Accelerate algorithms on NVIDIA¬Æ GPUs, cloud, and datacenter resources without specialized programming.
"
rc-website-fork/content/post/2020-december-maintenance-announcement.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-11-29T00:00:00-05:00""
title = ""Rivanna Maintenance: December 16-17, 2020""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Wednesday, December 16 & Thursday, December 17, beginning at 6 a.m. on December 16.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.

Rivanna is expected to return to service later in the day on December 17. The instructional queue will not be available until January 10.
IMPORTANT MAINTENANCE NOTES
Modules


The following software modules will be removed from Rivanna during the maintenance period:

gcc/8.3.0 (replaced by 9.2.0)



The following upgrades will take place during the maintenance period:


Changes to the default compiler and related toolchain versions:

gcc/7.1.0 -> 9.2.0
intel/18.0 -> 20.0
cuda/10.2.89 -> 11.0.228

Most dependent modules have been installed under the new default, except for R. R users must specify the previous goolf/iintelmpi version explicitly as goolf/7.1.0_3.1.4 or iintelmpi/18.0_18.0. Some dependent modules have been removed from the previous default compiler/toolchain version. If you need to use a module under the previous version, please contact our user services team at hpc-support@virginia.edu.
Users are reminded to recompile their code when switching to a different compiler/toolchain.


Upgrades to default versions of applications:

cellranger/3.1.0 -> 4.0.0
go/1.8.1 -> 1.13.4
julia/1.3.1 -> 1.5.3
ase/3.17.0-py3 -> 3.20.1
ffmpeg/4.3.1 (under gcc/9.2.0) - with x264 and x265 codecs

If you need to use a non-default version of an application, please specify the version when you load the module. Use module spider to find prerequisites.




New nodes and tools:

Two RTX 2080 Ti nodes (10 GPU devices each) in gpu partition - use --gres:rtx2080 in Slurm script
Visual Studio Code Server on Open OnDemand
nvhpc/20.9 - NVIDIA HPC SDK (CUDA 11.0)
awscli/2.1.10 - command line interface to Amazon Web Services
texlive/2020 - LaTeX
jq/1.6 - JSON processor
qiime2/2020.8 - microbiome bioinformatics platform with Empress and PICRUSt2 plugins
cellranger-atac/1.2.0
lightgbm/2.3.1 - CLI for gradient boosting framework



Docker Hub container registry
We are now using Docker Hub as our official container registry. Details are available here. Many of these container images are very new (e.g. pytorch:1.7.0, tensorflow:2.4.0) and have not been installed as modules on Rivanna, but you are welcome to use them by following the instructions on the Docker Hub repository page.
If you have any questions or concerns about maintenance day, please contact our user support team at hpc-support@virginia.edu prior to 12/16."
rc-website-fork/content/post/2023-women-in-hpc-20230919.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-09-11T10:18:25-05:00""
title = ""Virginia Women in HPC Events in September & October""
draft = false
tags = [""rivanna"",""whpc"",""va-whpc"",""hpc""]
categories = [""feature""]
+++
VA-WHPC September Event - Leadership Journeys
Time: Sep 19, 2023, 01:00 PM EST (US and Canada).
Join us for our next community event featuring Dr. Neena Imam as she shares her personal view of challenges and successes experienced throughout her inspiring leadership journey in research, HPC and AI computing. Come learn about career strategies, ask questions, and contribute to our discussion of how the playing field may be leveled to offer equitable IT & HPC leadership opportunities for women and minorities. 
Dr. Imam earned a PhD in Electrical Engineering and has been engaged in research and computing in a variety of roles. She was a Science Fellow to Senator Lamar Alexander, a Research Scientist and Deputy Director of Research Collaboration, Computing and Computational Sciences at Oak Ridge National Laboratory, and currently leads research engagement at NVIDIA for the Americas regions.
Featured speaker:
Neena Imam - Director of Researcher Engagement at NVIDIA
Attendees are invited to share their own experiences and engage with the speaker during this interactive Q&A session.
This virtual event is jointly hosted by Virginia Commonwealth University, George Mason University, Virginia Tech, William & Mary, University of Richmond, Virginia Institute of Marine Science, Old Dominion University, ACCESS, and the University of Virginia in cooperation with NVIDIA.


VA-WHPC October Event - Student Lightning Talks
Time: Oct 31, 2023, 01:00 PM EST (US and Canada).
VA-WHPC will be hosting 10-12 students as they give lightning talks on their research to members of the HPC community from across Virginia. Interested in giving a lightning talk? You‚Äôll get 3 minutes and 1-2 slides to tell us all about your research. Don‚Äôt miss this chance to practice your presentation skills and share your research with a diverse audience! 
Send us the title of your presentation before September 29.
A helpful guide on how to give a successful lightning talk is available here."
rc-website-fork/content/post/2023-december-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-10-23T00:00:00-05:00""
title = ""Rivanna Maintenance: December 18, 2023""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Monday, December 18, 2023 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
All systems are expected to return to service by 6 a.m. on Tuesday, December 19.
IMPORTANT MAINTENANCE NOTES
The operating system will be upgraded to Rocky 8.7 with system glibc 2.28 and GCC 8.5.0. Due to fundamental changes in system libraries, the entire software stack is rebuilt. Users should rebuild all self-compiled codes and R packages. Starting Nov 21, users who want early access to the development environment to rebuild/test codes against the new software stack can log in by running ssh udc-ba33-36 on the frontend. (Please be sure not to overwrite your existing codes for the production environment.) Contact us here if you need assistance.
Modules


Compilers and toolchains have been consolidated to the following:

GCC: gcc/11.4.0, goolf/11.4.0_4.1.4 
Intel: intel/2023.1 (default), intel/2024.0 (experimental), intel/18.0 (legacy)
NVIDIA: nvhpc/23.7, nvompi/23.7_4.1.5



Singularity has been renamed to Apptainer. Load the apptainer/1.2.2 module for containers. (The singularity command is provided as an alias.) All users can now build containers directly on Rivanna; see here for details.


There are many module version upgrades and deprecation of older versions. Run module spider NAME to check the available versions and the corresponding load command. Contact us here if you need a different version. Only the most important changes are listed below:


{{< table title=""Replacements"" class=""table table-striped"" >}}
|Name       |Default version|Other versions|Removed|
|---|---|---|---|
|OOD JupyterLab | 3.6.3  | - | 2.2.9 |
|OOD RStudio Server | 2023.06.2 | - | 1.0.143, 1.1.463, 1.3.1073, 2023.03.0 |
|anaconda   |2023.07-py3.11 | - | 2019.10-py2.7, 2020.11-py3.8|
|clang      |15.0.7  | - | 10.0.1 |
|cuda       |12.2.2  |10.2.89, 11.4.2| 10.1.168, 11.0.228 |
|gcc        |11.4.0  | - | 7.1.0, 9.2.0, 11.2.0 |
|go         |1.21.4  | - | 1.18.4, 1.19.4 |
|intel      |2023.1  | 18.0, 2024.0 | 20.0, 2022.11 |
|julia      |1.9.2   | - | 1.5.3, 1.6.0 |
|llvm       |15.0.7  | - | 4.0.0 |
|netcdf     |4.9.2   | - | 4.6.2, 4.7.3, 4.7.4 |
|nvhpc      |23.7    | - | 21.9 |
|perl       |5.36.0  | - | 5.24.0 |
|python     |3.11.4  | 2.7.18, 3.9.16 | 2.7.16, 3.6.6, 3.6.8, 3.7.7, 3.8.8 |
|pytorch    |2.0.1   | 1.12.0 | 1.8.1 |
|R          |4.3.1   | - | 3.5.3, 3.6.3, 4.0.3, 4.1.1, 4.2.1 |
|ruby       |3.1.2   | - | 2.3.4 |
|rust       |1.66.1  | - | 1.38.0, 1.41.0 |
|spark      |3.4.1   | - | 3.1.2 |
|tensorflow |2.13.0  | - | 2.7.0, 2.10.0 |
|texlive    |2023    | - | 2020  |
{{< /table >}}
Special reminders

C/C++/Fortran users who must build code with GCC 7 or older should containerize the application starting with the official GCC base image. Contact us if you need assistance.
Intel 18.0 modules are either migrated to the newer version (2023.1) or dropped. Intel users should rebuild code with intel/2023.1 if possible.
RStudio Server is now backed by a native module with R as a dependency. R packages installed via the R module will be detected automatically in RStudio Server, and vice versa. All R packages will need to be rebuilt.
Python 2.7-dependent modules are completely removed from the software stack. Users of legacy Python code can create a custom environment using the anaconda or mamba (recommended) module.
Code Server is backed by a native module instead of a container. This allows usage of compilers and interpreters on Rivanna. Python users please see instructions here.
Mamba is separated from anaconda into its own module.
Java module versions are standardized to 7, 8, 11, 12 (previously 1.7.0, etc.).

Rebuilding R Libraries
Due to changes in the operating system and compilers on Rivanna, your existing R libraries will not work. We have created a command that will help you to rebuild your library for the new version of R.
The command is updateRlib and requires two pieces of information:
i) How you run your R code
ii) The version of R that you have been using.
For example, if you have been using with RStudio 1.3.1073 - R 4.1.1, you can type:
updateRlib OOD 4.1.1
This command will capture your packages that were used in your R/4.1 library for Open OnDemand and rebuild to a new library.
The three options for how you run your code are: OOD, goolf, or intel. Rebuilt libraries will be installed in ~/R/goolf/4.3 for both module and OOD versions."
rc-website-fork/content/post/2022-rivanna-maintenance-dates.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-08-06T10:18:25-05:00""
title = ""Rivanna Maintenance Schedule for 2023""
draft = false
tags = [""rivanna"",""maintenance""]
categories = [""feature""]
+++
Rivanna will be taken down for maintenance in 2023 on the following days:

Tuesday, March 7
Tuesday, May 30
Tuesday, October 3
Monday, December 18

Please plan accordingly. Questions about the 2023 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2020-december-maintenance-update-2.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-12-18T00:00:00-10:00""
title = ""Rivanna Maintenance Update: 12/18/20, 02:30 pm EST""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
In order to expedite users' access to the system, Rivanna will be returned to service in 2 separate phases:

During phase 1, which is expected to be completed in fewer than 72 hours from now, the standard, GPU, largemem, and front-end nodes will be reactivated.
Rivanna's parallel nodes will be released in phase 2 which should be finished within the 72-hour timeframe.
"
rc-website-fork/content/post/2019-september-scratch-notes.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-09-15T15:18:25-05:00""
title = ""Changes to /scratch September 2019""
draft = true
tags = [""maintenance"",""storage"",""scratch""]
categories = [""feature""]
summary = ""As part of the September 2019 maintenance for Rivanna, the Research Computing team replaced the /scratch file system with new, faster hardware.""
+++
{{% callout %}}
As part of the September 2019 maintenance for Rivanna, the Research Computing team replaced the /scratch file system with newer hardware.
The systems engineers did transfer files that were no older than 90 days to the new system.  
If you are missing files, you will be able to retrieve them until the next maintenance (planned for December 2019).
As a reminder, /scratch is temporary storage and files older than 90 days are subject to purging.
{{% /callout %}}
Transferring your files
The path to your folder on the old scratch file system is /oldscratch/$USER. This folder has been changed to read-only.  You do not have permission to write or execute files from that location.
To transfer your files, we recommend using the rsync command within a shell (i.e., Terminal Window).
If you normally use Open on-Demand or JupyterLab, you can open a Terminal Window by


logging into Open on-Demand (https://ood.hpc.virginia.edu);


clicking on Files > Home Directory ;


clicking on >_ Open in Terminal


Transferring a single file
{{% callout %}}
To copy a file from /oldscratch to /scratch, you can type (for example):

rsync -av /oldscratch/$USER/somefolder/myFile /scratch/$USER/somefolder
{{% /callout %}}

Transferring a folder
{{% callout %}}
To copy a folder and its contents from /oldscratch to /scratch, you can type (for example):

rsync -av /oldscratch/$USER/somefolder/ /scratch/$USER/somefolder
{{% /callout %}}

Notice the trailing slash at the end of the first somefolder, and the lack of a slash at the end of the second somefolder.  The placement of the slash is important for how the transfer is done.
The slash at the end of the first folder refers to the contents of the folder (in this case, all files within somefolder).  Whereas, no slash at the end of the second folder instructs the computer to place the files directly in that folder. If you do include a slash at the end of the second folder, the computer will create a new folder under the existing folder. So, you would have /scratch/$USER/somefolder/somefolder.
Need Help?
If you have questions or need help with transferring your files, Research Computing will hold a ""Clinic on Scratch"" on Wednesday, September 18 in Brown Library, room 145 from 3 to 5 pm.  
More about scratch"
rc-website-fork/content/post/2022-globus-scratch-transfer.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-09-28T10:18:25-05:00""
title = ""Scratch Recovery with Globus""
draft = false
tags = [""rivanna"",""scratch"",""globus"",""data-transfer""]
categories = [""feature""]
+++
Globus is a simple, reliable, and fast way to access and move your research data between systems. Researchers can transfer data from their old scratch directories to new scratch via a single web interface - no software download or installation is necessary.

Old scratch (/oldscratch) directories are now read-only. You will not be able to write new files to /oldscratch or copy files from /scratch to /oldscratch.
{{% callout %}}
Users who need help using Globus to transfer their /scratch files are invited to attend one of the following online tutorial sessions:


Thursday, September 29 (2 to 4 p.m.)


Friday, September 30 (10 to 11 a.m.)


Monday, October 3 (10 to 11 a.m.)


Friday, October 7 (10 to 11 a.m.)


The old scratch system will be permanently retired on October 31.
{{% /callout %}}
Transferring Data
Selecting data from old scratch


Go to https://app.globus.org/file-manager.


Choose ""University of Virginia"" as your institution and log in using Netbadge.



Click the ""Collection"" field.


Search for and select ""UVA Main-DTN"".



Double-click the ""/oldscratch"" folder and then double-click the folder with your computing ID (only your own computing ID will be visible).



Select the files and folders you want to transfer.



Select destination for files on /scratch


Click ""Transfer or Sync to..."" in the menu to open up the second collection panel. You can also do this by clicking the two-panel icon in the Panels menu in the top right-hand corner of the app.



Click the ""Collection"" field in the newly opened panel.


Search for and select ""UVA Main-DTN"". It should now be in your ""Recent"" collections since you selected it previously.


Double-click the ""scratch"" folder and then double-click the folder with your computing ID.



If you have already created new scratch folders you can double-click them to select them as a destination for your old scratch files. If you want the files in the top level of your scratch folder then do not select anything.


Transfer


Click the highlighted ""Start"" button to begin the transfer. This should be the ""Start"" button on the /oldscratch Panel.



Your transfer will begin. You can monitor your transfer in the ""Activity"" tab of the Globus app. You will receive an email when the transfer is complete. 


{{% callout %}}
To transfer data from /oldscratch to local storage like your laptop or lab/departmental storage, you will need to install Globus on your workstation. Please see our documentation for Globus installation and data transfer between systems.
{{% /callout %}}"
rc-website-fork/content/post/2020-september15-matlab-seminar.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-09-09T00:00:00-05:00""
title = ""Automated Image Labeling and Iterative Learning - Live Seminar: September 15, 2020""
url = ""/maintenance""
draft = false
tags = [""rivanna"",""Matlab"",""education"",""workshops"",""feature""]
categories = [""feature""]
+++
MathWorks engineers will offer a free live webinar on September 15th from 2:00 to 3:00 Eastern time.



Overview
Automated image labeling helps to reduce the cost and time that it takes to label your dataset. MATLAB significantly reduces the time required to preprocess and label datasets with domain-specific apps for audio, video, images and text data. Whether you train your models in MATLAB or Python, we support your entire image processing and AI workflow, from acquisition to deployment.
In this session we show you how to use apps for labeling image and video data to build AI models. We explore preprocessing to facilitate feature extraction and present approaches to building models in an iterative fashion, validating predicted labels and incorporating on-the-fly models to label large datasets. We also discuss an approach to automating pixel-level labeling for semantic segmentation workflows.
Highlights

Preprocess and label datasets faster with domain-specific apps for audio, video, images and text data
Use interactive apps to label, crop and identify important features and automate the process of labeling
Create, visualize and edit deep learning networks with our easy-to-use Deep Network Designer app
Incorporate deep learning models without having to create complex network architectures from scratch
Accelerate development and training of deep learning networks with GPUs, clusters and cloud resources
"
rc-website-fork/content/post/featured-projects.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-06-23T15:18:25-05:00""
title = ""Featured Projects & Collaborations""
draft = false
tags = [""projects"",""feature""]
categories = [""feature""]
+++

UVA Research Computing strives to empower researchers to achieve more through the use of cutting-edge computational resources.
This has led to fruitful collaborations with researchers and staff across grounds, including these groups and departments:

Astronomy
Biochemistry and Molecular Genetics
Biomedical Engineering
Center for Applied Biomechanics
Center for Advanced Medical Analytics
Center for Behavioral Health and Technology
Center for Diabetes Technology
Center for Public Health Genomics
Economics
Emergency Medicine
Environmental Sciences
Infectious Diseases
Public Health Sciences
Materials Science & Engineering
Pediatrics‚ÄìNeonatology
Radiology and Medical Imaging
Surgery
UVA Sinklab

To browse a gallery of recent projects, visit our Projects page below.
Browse Projects"
rc-website-fork/content/post/2025-workshop-survey.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2025-02-03T00:00:00-05:00""
title = ""Future of Research Computing‚Äôs Training Initiatives: Workshops Survey""
draft = false
tags = [""rivanna"", ""afton"",""python"",""hpc""]
categories = [""education"",""workshops"",""feature""]
summary = ""Please fill out the survey below to help us shape the future of Research Computing‚Äôs training initiatives: Link to Survey""
+++
Please fill out the survey below to help us shape the future of Research Computing‚Äôs training initiatives. Your insights ensure the continued delivery of relevant and impactful learning opportunities for the research community.
Link to Survey : https://virginia.az1.qualtrics.com/jfe/form/SV_6PDQmrw5AbCrVR4"
rc-website-fork/content/post/2025-jan-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-12-17T00:00:00-05:00""
title = ""HPC Maintenance: Jan 7, 2025""
draft = false
tags = [""rivanna"", ""afton""]
categories = [""feature""]
+++
{{< alert-green >}}The HPC cluster will be down for maintenance on Tuesday, Jan 7, 2025 beginning at 6 am.{{< /alert-green >}}
All systems are expected to return to service by Wednesday, Jan 8 at 6 am.
IMPORTANT MAINTENANCE NOTES
How should I prepare and what to expect on Jan 7, 2025?
You may continue submitting jobs to the HPC system until the maintenance period begins. However, if the system determines your job won't finish in time, it will not start until the system is back online. The maintenance will involve upgrading the storage client, requiring all compute and login nodes, including the Open OnDemand and FastX portals, to be taken offline. Additionally, Research Project Storage and home directories will be inaccessible. Research Standard Storage will remain accessible via SMB and NFS mounts. The UVA Standard Security Storage Data Transfer Node (DTN) will remain operational throughout the maintenance period.
Reminder: New Service Unit pricing and consumption rates
Effective January 8, 2025, Research Computing will implement a new Service Unit (SU) and pricing schedule for HPC services. Standard and instructional SU allocations will remain free of charge. Jobs that start after the maintenance period will be charged based on the updated SU consumption rates determined by the type of hardware utilized. For details on the changes, see the pricing table  and SU consumption rates. Additionally, the default memory allocation per CPU core will decrease to 4GB, reflecting typical usage patterns and aligning with the updated SU model. This change provides more granular control, as SU charges will now account separately for CPU, memory, and specialty hardware like GPUs. If your job encounters an ""Out-of-Memory"" error, adjust your memory request accordingly. 
Reminder: ‚ÄúDedicated Computing‚Äù as a new service model
This model allows researchers to lease hardware managed by Research Computing (RC) as an alternative to purchasing their own equipment. It provides dedicated access to HPC resources with no wait times. See here. 
Removed old DNS names
The old Domain Name System (DNS) entries for logging into Rivanna/Afton HPC have been removed. Please refer to the table below for the updated login names.
|Old|New|
|---|---|
|rivanna.hpc.virginia.edu ->|login.hpc.virginia.edu|
|rivanna-desktop.hpc.virginia.edu ->|fastx.hpc.virginia.edu|
|rivanna-portal.hpc.virginia.edu ->| ood.hpc.virginia.edu|
Modules


Apptainer will be upgraded from 1.2.2 to 1.3.4. There is no change to the containers themselves, and users do not need to rebuild their own containers.


The following modules will be removed from Rivanna during the maintenance period.


{{< table title=""replacement"" class=""table table-striped"" >}}
| Module | Removed version | Replacement |
|---|---|---|
|amber    | 22.0    | 24-CUDA-12.2.2 |
|apptainer| 1.2.2   | 1.3.4 |
|blender  | 3.2.1, 3.4.1 | 3.6.17 |
|diamond  | 2.0.14  | 2.1.6 |
|freesurfer| 6.0.1  | 7.2.0 |
|gatk     | 4.3.0.0, 4.5.0.0 | 4.6.0.0 |
|irfinder | 1.3.1   | 2.0.1 |
|kraken2  | 2.1.2   | 2.1.3 |
|ncbi-vdb | 3.0.2   | 3.1.1 |
|orca     | 5.0.2   | 5.0.4, 6.0.0 |
|rapidsai | 23.10   | 24.06 |
|rust     | 1.66.1  | 1.79.0 |
|scons    | 4.2.0   | 4.5.2 |
|smrtlink | 12.0.0.177059 | 13.1.0.221970 |
|spaceranger| 2.0.1 | 3.1.1 |
|sratoolkit| 3.0.2, 3.0.3 | 3.1.1 |
{{< /table >}}"
rc-website-fork/content/post/2020-september22-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-09-15T00:00:00-05:00""
title = ""Rivanna Maintenance: Sept 22, 2020""
draft = false
tags = [""rivanna"",""maintenance"",""feature""]
categories = [""feature""]
+++
{{< alert-green >}}
Rivanna will be down for maintenance on Tuesday, September 22, beginning at 8:30 a.m. It is expected to return to service later in the day.
{{< /alert-green >}}
RC engineers will be installing new hardware that is needed to stabilize the /scratch filesystem.
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
If you have any questions or concerns about the maintenance period, please contact our user support team at hpc-support@virginia.edu."
rc-website-fork/content/post/2025-hpc-maintenance-dates.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2025-02-18T00:00:00-05:00""
title = ""HPC Maintenance Schedule for 2025""
url = ""/maintenance""
draft = false
tags = [""rivanna"",""afton""]
categories = [""feature""]
+++
Rivanna/Afton will be taken down for maintenance in 2025 on the following days: 

Winter: Tuesday, January 7
Spring: May 27, 2025
Summer: August 12, 2025
Fall: Oct 14, 2025

Please plan accordingly. Questions about the 2025 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2022-women-in-hpc-202208.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-09-19T10:18:25-05:00""
title = ""Virginia Women in HPC - Women in HPC & IT Leadership Roles""
draft = false
tags = [""rivanna"",""whpc"",""va-whpc"",""hpc""]
categories = [""feature""]
+++
Topic: Women in HPC & IT Leadership Roles.
When: October 12, 2022 01:00 PM, Eastern Time (US and Canada).
Join us for our Fall community meeting to hear from female leaders in the HPC & IT field sharing challenges and successes experienced throughout their careers. Don‚Äôt miss this fantastic opportunity to learn about career strategies, share your experience, and contribute to our discussion of how the playing field may be leveled to offer equitable HPC & IT leadership opportunities for women and minorities. Attendees are invited to share their own experiences and engage with panelists during this interactive Q&A session.
Featured Panelists:


Robin Bryan - AVP for Information Technology/CIO at James Madison University.


Corinne Picataggi - Chief Technology Officer at the College of William and Mary.


Moderator - Gladys Andino, Senior Computational Scientist at the University of Virginia.





Virginia WHPC is committed to increasing diversity and inclusion by promoting and encouraging the participation of women in high-performance computing and related fields. 
https://va-whpc.org/"
rc-website-fork/content/post/2020-march-rivanna-software.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-03-11T10:18:25-05:00""
title = ""Rivanna Software Updates: March 11, 2020""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}The Rivanna maintenance has been completed on March 11 and the system is back in service.{{< /alert-green >}}

The following software modules have been removed from Rivanna during the maintenance period. Please use the suggested newer versions:

gcc/5.4.0 & toolchains -> 7.1.0
All modules that depend on gcc/5.4.0 are now available under gcc/7.1.0.
The only exception is cushaw3/3.0.3. Please contact us if you need to use it.


pgi/19.7 & toolchains -> 19.10
All modules that depend on pgi/19.7 are now available under pgi/19.10.


anaconda/5.2.0-py2.7 -> 2019.10-py2.7
All modules that depend on anaconda/5.2.0-py2.7 are now available under anaconda/2019.10-py2.7.


tensorflow/1.6.0-py27, -py36 -> 1.12.0, 2.0.0, or 2.1.0
If you must use version 1.6.0, please pull the image from our repository on Singularity Library.


singularity/3.1.1 -> 3.5.2
boost/1.66.0 -> 1.68.0
julia/1.0.2, 1.0.3 -> 1.3.1
cushaw3/3.0.3 - no replacement

The following upgrades have taken place during the maintenance period:

JupyterLab v1.2.6
Python 3.7 Jupyter kernel - based on anaconda/2019.10-py3.7
The previous ""Python 3"" kernel (based on anaconda/5.2.0-py3.6) has been renamed as ""Python 3.6"".


anaconda/2019.10-py2.7, -py3.7
gcc/9.2.0 & toolchains - gcc/7.1.0 remains the default
singularity/3.5.2 - now default version
gurobi/9.0.1
tensorflow/2.1.0-py37 - Singularity container module & Jupyter kernel
julia/1.3.1 - module & Jupyter kernel
ansys/2020r1
samtools/1.10
rust/1.41.0
cmake/1.16.5

New tools:

LibreOffice - through FastX Web desktop environment
pytorch/1.4.0 - Singularity container module & Jupyter kernel
openfoam 7 (version 1909) - open-source CFD software
goolfc/6.5.0_3.1.4_10.1.168 - GOOLF toolchain (GCC + OpenMPI + OpenBLAS + ScaLAPACK + FFTW) with CUDA support
atom/1.43.0 - Atom text editor
rclone/1.51.0 - Rclone for cloud file syncing (supports Google Drive)
nodejs/12.14.1 - Node.js JavaScript runtime environment
ninja/1.10.0-py3.6 - Ninja build system
meson/0.53.1-py3.6 - Meson build system
gtk+/3.24.14 - GTK+ 3 libraries for GUI applications
fribidi/1.0.8 - Free Implementation of the Unicode Bidirectional Algorithm
atk/2.28.1 - Accessibility Toolkit
tree/1.8.0 - Tree structure of file system
gnupg/2.2.19 - GnuPG encrypt and sign data
"
rc-website-fork/content/post/afton-dedication.md,"Ôªø+++ 
images = [""""]
author = ""Staff""
description = """"
date = ""2024-10-15T00:00:00-05:00""
title = ""Afton Cluster Dedicated to Prof. John Hawley""
draft = false
tags = [""afton"",""hpc""]
categories = [""feature""]
+++

On September 16, 2024, RC dedicated the new Afton computing cluster to the memory of John F. Hawley (1958-2021), late Professor of Astronomy who was a leading researcher in computational astrophysics.  He also served in the Office of the Dean of the College and Graduate School of Arts and Sciences for nine years, first as Associate Dean for the Sciences and later as Senior Associate Dean for Academic Affairs.   The ceremony featured some remarks by Josh Baller, Associate Vice President for Research Computing, and Scott Ruffner, Director of Infrastructure for Research Computing, along with a recorded message from Provost Ian Baucom.

John received his PhD in Astronomy from UIUC in 1984 with a dissertation on numerical studies of accretion disks around black holes. He moved to the California Institute of Technology as a Bantrell Fellow postdoc in the summer of that year.  He was deeply involved with the national supercomputing centers from the earliest days.  He returned to Illinois in October 1985 to prepare some ‚Äúshowcase‚Äù applications for the January 1986 opening of the National Center for Supercomputing Applications, housed at the University of Illinois at Urbana-Champaign.  John served on advisory committees and review panels for the national centers throughout his career.
He moved to UVA in the fall of 1987 as an assistant professor in the Astronomy Department, where he continued his research on numerical simulations of black-hole accretion disks.  At UVA he met Steven Balbus, with whom he collaborated on an important series of papers from 1991 to 1992 that unraveled one of the fundamental mysteries of accretion-disk dynamics.  They were jointly awarded the Shaw Prize in Astronomy in 2013 for these discoveries. 
Even though he primarily used the national centers for his own research, John was devoted to promoting computational science from the beginning of his time at UVA. In April of 2001, he chaired a University-wide committee whose outcome was the acquisition of the University‚Äôs first ‚ÄúBeowulf‚Äù cluster in May of 2002. From 2006-2008, John worked with other faculty to repurpose part of what at the time was research computing software support within ITS into the first dedicated group for more advanced research computing support.  The new group was given the name UVACSE (University Alliance for Computational Science and Engineering) and Andrew Grimshaw, Professor of Computer Science, was its first director.  James Hilton, then VP for IT, provided budgetary support and made the group answer to a faculty-led Computational Science Advisory Council.  
John continued to serve on committees such as UCIT (University Committee for Information Technology), other advisory committees, and multiple task forces, working with other faculty on projects related to computational education and hardware acquisition.  This culminated in the purchase in 2014 of the University‚Äôs first high-end computational resource, Rivanna, a goal he had sought for years.   Ivy, the first shared resource for sensitive data processing, was also initiated through the work of these committees.
Even with his strong advocacy for hardware, John was an even stronger champion of developing a professional support group for research computing.  He often stated that while hardware was important, what really mattered was people.  The University has responded by building Research Computing into the acclaimed group it is today.
{{< rawhtml >}}
    




Left: Cray-1 at NCSA, October 1985. Right: Frontera at TACC, August 2019.

{{< /rawhtml >}}"
rc-website-fork/content/post/2020-december-maintenance-update-3.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-12-21T00:00:00-10:00""
title = ""Rivanna Maintenance: Phase 1 Completed""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
Phase 1 of the maintenance period is now complete. Rivanna‚Äôs standard, GPU, largemem, and front-end nodes have been returned to service. Pending jobs that were submitted before the maintenance began need to be resubmitted.
Rivanna‚Äôs parallel nodes will be released in phase 2 which we anticipate being finished by 5 p.m. on Tuesday, December 22."
rc-website-fork/content/post/2024-july-scratch-purge.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-07-18T00:00:00-05:00""
title = ""Reinstatement of file purging of personal /scratch files on Afton and Rivanna""
url = ""/maintenance""
draft = false
tags = [""scratch"", ""afton"", ""rivanna""]
categories = [""feature""]
+++
On Sep 1, 2024 RC system engineers will reinstate a file purging policy for personal /scratch folders on the Afton and Rivanna high-performance computing (HPC) systems. From Sep 1 forward, scratch files that have not been accessed for over 90 days will be permanently deleted on a daily rolling basis. This is not a new policy; it is a reactivation of an established policy that follows general HPC best practices.   
The  /scratch filesystem is intended as a temporary work directory. It is not backed up and old files must be removed periodically to maintain a stable HPC environment. 
{{% callout %}}
Key Points:

Purging of personal scratch files will start on Sep 1, 2024. Files that have not been accessed since Jun 3, 2024 will be deleted on that day. 
Directories that have been emptied as part of the file purging process will be removed as well. 
The /scratch filesystem is not backed up. Users should back up important scratch data to other storage options on a regular basis. Eligible PIs can request 10TB of Research Standard storage for their groups at no charge. Learn more about all storage options.  

{{% /callout %}}
{{% highlight %}}
Do you have additional questions? 
Please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m..
{{% /highlight %}}
Communications
{{% accordion-group title=""Emails"" id=""emailgroup"" %}}
{{% accordion-item title=""Reinstatement of purging policy for personal scratch files on Afton and Rivanna"" id=""email-3"" %}}
Dear HPC user,  
On Sep 1, 2024 RC system engineers will reinstate a file purging policy for personal /scratch folders on the Afton and Rivanna high-performance computing (HPC) systems. From Sep 1 forward, files that have not been accessed for over 90 days will be permanently deleted on a daily rolling basis. This is not a new policy; it is a reactivation of an established policy that follows general HPC best practices. 
The  /scratch filesystem is intended as a temporary work directory. It is not backed up and old files must be removed periodically to maintain a stable HPC environment.  
How should I prepare? 
We encourage you, if you are using scratch, to identify and move important files to more persistent storage solutions on a regular basis. The FAQ ‚ÄúHow can I find out what files will be purged?‚Äù provides instructions to identify the scratch files marked for deletion. RC offers several low-cost storage options to researchers, including 10TB of Research Standard storage for each eligible PI at no charge. For more detailed descriptions of our storage options, visit [https://www.rc.virginia.edu/userinfo/storage/].  
What to expect on Sep 1, 2024?
Starting on Sep 1, 2024, a script will be launched on the Afton and Rivanna systems to monitor on a daily basis if scratch files have been accessed within the past 90 days. Last access times are determined as the last time a file was opened (read) or modified (written to). Scratch files that have not been accessed since Jun 3 (90 days before Sep 1) will be removed from the filesystem. The 90-day file purging window will move forward on a daily basis.   
If you have any questions about the file purging policy or process, you may contact our user services team.   
With regards,‚ÄØ  
Karsten Siller
Director, Research Computing User Services
Information Technology Services
University of Virginia 
{{% /accordion-item %}}
{{% /accordion-group %}}
FAQ
{{% accordion-group title=""Group"" id=""faqgroup""%}}
{{% accordion-item title=""1. Why are files being deleted? "" id=""faq-1"" %}}
Scratch is intended as a temporary work directory, not for long-term storage. It is not backed up, and old files need to be purged periodically to maintain system stability and filesystem performance. This is generally an established best practice at HPC centers. 
{{% /accordion-item %}}
{{% accordion-item title=""2. How does the file purging work? "" id=""faq-2"" %}}
Starting Sep 1, 2024, the Afton and Rivanna systems execute a daily script that identifies the last access time for each scratch file. Each day the script will permanently delete those files identified with an access time older than 90 days. Directories that are left empty as a result of the file purging process will be removed as well.
{{% /accordion-item %}}
{{% accordion-item title=""3. How is a file‚Äôs last access time being determined? "" id=""faq-3"" %}}
A file‚Äôs last access corresponds to the date and time that file was last opened (read) or modified (written to). 
{{% /accordion-item %}}
{{% accordion-item title=""4. How can I find out what files will be purged? "" id=""faq-4"" %}}
Log in to Afton/Rivanna and from the command line, run:
 check-scratch-for-purge > outfile 
where outfile is the path of the file to which you wish to save the results. This will save to your outfile a list of files ordered from the oldest last accessed to the most recently accessed that will be purged.
Alternatively, you can access the Check Scratch For Purge tool available in the Utilities dropdown on Open OnDemand. From there, you can view a list of files being purged and save the list to /home, /scratch, or download it locally.
{{% /accordion-item %}}
{{% accordion-item title=""5. What should I do with files that I still need? "" id=""faq-5"" %}}
We encourage users to back up their important data. Data can be transferred to either your home storage (50G) or leased storage (if applicable). For more details, see ""What storage options does RC provide?""
Learn more about available data transfer tools. 
{{% /accordion-item %}}
{{% accordion-item title=""6. Can deleted files be restored if needed later? "" id=""faq-6"" %}}
No. Scratch is a high-performance filesystem without any snapshots or backups. Deleted files cannot be restored.
{{% /accordion-item %}}
{{% accordion-item title=""7. What storage options does RC provide? "" id=""faq-7"" %}}
RC offers several low-cost storage options to researchers. Home directory storage provides up to 50G with daily and weekly snapshots of data. Eligible PIs can request 10TB of Research Standard storage at no charge. PIs also have the option to purchase additional leased storage. 
Learn more about storage options and how to purchase storage.
{{% /accordion-item %}}
{{% accordion-item title=""8. How can I get help? "" id=""faq-8"" %}}
If you have any questions, please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m..
{{% /accordion-item %}}
{{% /accordion-group %}}"
rc-website-fork/content/post/2023-may-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-05-05T00:00:00-05:00""
title = ""Rivanna Maintenance: May 30, 2023""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on May 30, 2023 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
All systems are expected to return to service by 6 a.m. on Wednesday, May 31.
IMPORTANT MAINTENANCE NOTES
Five RTX3090 nodes (4 GPU devices each) have been added to the gpu partition - use --gres=gpu:rtx3090 in Slurm script.
Modules

The toolchains gompic gcccuda goolfc will be removed from Rivanna during the maintenance period, since we now have CUDA-aware toolchains based on gcc/11.2.0. If you need assistance with rebuilding your own code, please contact us here. The following affected modules have been migrated:

{{< table title=""Replacements"" class=""table table-striped"" >}}
| Module | Previous toolchain | Replacement |
|-----|-----|-----|
|gpunufft/2.1.0 | gcccuda/9.2.0_11.0.228      | same version under gcc/11.2.0     |
|gromacs/2021.2 | goolfc/9.2.0_3.1.6_11.0.228 | 2022.4 under goolf/11.2.0_4.1.4 |
|mumax3/3.10    | gcccuda/9.2.0_11.0.228      | same version under gcc/11.2.0     |
{{< /table >}}
The following modules will be removed from Rivanna during the maintenance period.
{{< table title=""Replacements"" class=""table table-striped"" >}}
| Module | Removed version | Replacement |
|-----|-----|-----|
|alphafold | 2.2.0 | 2.1.2, 2.2.2, 2.3.0 |
|awscli | 2.4.12 | 2.9.17 |
|cellprofiler | 3.1.8 | 4.2.5 |
|gurobi | 9.5.0  | 10.0.1 |
|imagemagick | 7.0.7-0 | 7.1.0-57 |
|pytorch | 1.10.0 | 1.8.0, 1.12.0 |
|rapidsai  | 21.10 | 23.02 |
|tensorflow* | 2.4.1, 2.8.0 | 2.7.0, 2.10.0 |
{{< /table >}}
*Archived containers can be found in /share/resources/containers/singularity/archive.


Upgraded modules:

clang/15.0.7

Default version changes:

nextflow/20.10.0 ‚Üí 23.04.1



New modules:

bazel/6.1.1
clara-parabricks/4.0.3
gpumd/3.7
kubectl/1.26.2
optix/6.5.0, 7.3.0


"
rc-website-fork/content/post/2024-february-exhibition.md,"+++
title = ""UVA Research Computing Exhibition: April 23, 2024""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
date = ""2024-02-01T17:57:24-05:00""
tags = [""rc""]
categories = [""feature""]
draft = false
+++

About This Event:
The Research Computing Exhibition will be held on Tuesday, April 23, 2024 in the Newcomb Hall Ballroom. The event will include:

A panel discussion made up of academic scientific computing experts and research computing faculty and staff.
  Judged poster session with prizes:
  
First Place: $3,000 travel voucher
    Second Place: $2,000 travel voucher
    Third Place: $1,000 travel voucher
  
Light refreshments


If you would like to participate in the poster session, please fill out the Intent to Participate Form by Friday, March 15. While all are welcome to present a poster during the exhibition, only UVA affiliated non-faculty submissions will be eligible for the award prizes.
Key dates for poster session participants:


March 15: Intent to Participate Form Due


April 5: Final PDF version of Poster Due


April 23: Research Computing Exhibition (finalists will be notified prior to the event)


Event Schedule




 


Time
Activity


10:00-11:15
 Poster Setup


11:30-11:45
 Opening Remarks, Kelly Doney, Vice President & Chief Information Officer, UVA


11:45-12:45
 Panel Discussion; ""Employment in HPC""


12:45-2:15
 Event attendees and participants circulate among posters. Participants will be assigned specific time slots they need to be next to their posters. Judges circulate and interview finalists about their posters.*Will break down into 3 timeslots*


2:15-2:45
 Judges cluster to decide 1st, 2nd, and 3rd place winners


2:45-3:00
Announcement of poster competition winners and farewell remarks


"
rc-website-fork/content/post/2020-june-rivanna-accounting.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-04-26T10:18:25-05:00""
title = ""RC Acquires New Accounting Management Software for Rivanna""
draft = false
tags = [""rivanna"",""accounting""]
categories = [""feature""]
+++
Research Computing will be activating a new accounting management package for Rivanna on June 17, 2020. The software was purchased from Adaptive Computing, which specializes in advanced management applications for high-performance systems. Rivanna users can expect to see more accurate reporting on their Service Unit (SU) balances and burn rates. Information on usage by individual members of an allocation group will also be available.
Commands such as allocations will remain but will reflect the new accounting. Users should be aware that the new accounting system implements ""liens"" on running jobs, and that the SUs requested for each job will be held in a reserved pool until the job completes. When the job completes the lien is released and the actual SUs consumed
are deducted from the allocation balance. That means that fewer SUs will be available while jobs are running. Details are explained in our FAQ section.
Learn more:
- HPC Overview"
rc-website-fork/content/post/2020-december-maintenance-update-4.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-12-23T00:00:00-10:00""
title = ""Rivanna Maintenance: Phase 2 Completed""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
Phase 2 of the maintenance period is complete. A majority of Rivanna's nodes, including its parallel nodes, can now be accessed by users."
rc-website-fork/content/post/2021-december-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2021-12-01T00:00:00-05:00""
title = ""Rivanna Maintenance: December 14, 2021""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna and the Globus data transfer nodes (DTNs) will be down for maintenance on Tuesday, December 14, 2021 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
Users will be unable to transfer data using Globus during the maintenance period. Rivanna and the Globus DTNs are expected to return to service by 6 a.m. on Wednesday, December 15.
IMPORTANT MAINTENANCE NOTES
New GPU
We are pleased to announce the addition of DGX A100 GPU to the gpu partition. To request an A100 GPU in Slurm scripts, use --gres=gpu:a100.
Attention PyTorch/TensorFlow users: We are removing all the non-default PyTorch and TensorFlow versions, together with the corresponding Jupyter kernels, as they are not compatible on the A100, and adding a new version that will replace the current default (1.8.1 -> 1.10.0 for PyTorch; 2.4.1 -> 2.7.0 for TensorFlow). For the sake of reproducibility/continuity of ongoing projects, the deprecated containers will be accessible from /share/resources/containers/singularity/archive and can be used to install your own Jupyter kernel. You may use them on other GPUs by excluding the A100 via the Slurm option -x udc-an28-[1,7].
Modules


The following software modules will be removed from Rivanna during the maintenance period:

pytorch/1.4.0-py37, 1.5.1 (see section above)
tensorflow/1.12.0-py27, 1.12.0-py36, 2.0.0-py36, 2.1.0-py37 (see section above)
cuda/9.2.148.1
cudnn/7.4.1.5
python/3.8.8 under gompic -> moved to goolfc
matlab/R2018b, 2019a, R2019b
mathematica/12.0, 12.1
epacts/3.3.0 - replaced by 3.3.2 under goolf/7.1.0_3.1.4
R/3.2.1, 3.4.4, 3.5.3, 4.0.0, 4.1.0

Attention R users: We will streamline the R modules to include the following versions: 3.6.3, 4.0.3, and 4.1.1. The default version will be 4.0.3. If you have hard-coded an older version of R in your scripts (e.g., R/3.5.3), you will need to update your scripts to specify one of the newer versions. If you need to switch to a newer version of R, your library containing the packages that you have installed will have to be updated. You can attempt this manually, or you can contact hpc-support@virginia.edu for help with automating the installation of your packages.


The following upgrades will take place during the maintenance period.


gcc/11.2.0 and libraries (openmpi/3.1.6, openblas/0.3.17, scalapack/2.1.0, fftw/3.3.10, boost/1.77.0)
The default version is still 9.2.0.


gromacs/2021.2 - with GPU support; please load goolfc first




Upgrades to default versions of applications:
    - R/3.6.3 -> 4.0.3
    - matlab/R2021a -> R2021b
    - mathematica/12.2 -> 12.3
    - nvhpc/20.9 -> 21.9
    - cuda/11.0.228 -> 11.4.2
    - cudnn/7.6.5.32 -> 8.2.4.15
    - pytorch/1.8.1 -> 1.10.0
    - tensorflow/2.4.1 -> 2.7.0
    - alphafold/2.0.0 -> 2.1.1; note changes to flags!
    - amptorch/20210308 -> 0.1
    - freebayes/0.9.9 -> 1.3.4
    - salmon/1.2.1 -> 1.5.1
    - rapids/0.19 -> 21.10

New modules:
spark/3.1.2
rosetta/3.13 - computational modeling and analysis of protein structures
namd/2.14 - Nanoscale Molecular Dynamics
vmd/1.9.4 - Visualization software for NAMD
cc3d/4.2.5 - CompuCell3D
deeplabcut/2.2 - animal pose estimation
mirdeep2/0.1.3
multiqc/1.11
pbwt/3.0
ocaml/3.12.1
pov-ray/3.7.0 - 3D graphics with raytracing
unrar/6.0.2


"
rc-website-fork/content/post/2019-december-mainenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-12-10T10:18:25-05:00""
title = ""Rivanna Maintenance: December 18, 2020""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
Rivanna will be taken down for routine maintenance on Wednesday, December 18, beginning at 6 a.m.
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
Rivanna is expected to return to service by 6 a.m. on Thursday, December 19."
rc-website-fork/content/post/2023-04-computational-scientist-position.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-04-10T10:18:25-05:00""
title = ""Join Our Team! Opening for Computational Scientist Position""
draft = true
tags = [""user-services"", ""uvarc"", ""job-openings""]
categories = [""feature""]
+++
To see the full job posting, please visit the UVA job board.
UVA Research Computing (RC) within Information Technology Services (ITS) is seeking a Research Computing Associate/Computational Scientist to join their team. Our team strives to create innovative computational and data analysis solutions for researchers. We build and maintain the University's high performance computing platforms while educating the next generation of researchers on the power of advanced computing.
ITS at UVA is a phenomenal place to lead, grow, and deliver impact. It's an organization that values results and teamwork. We strive to create a welcoming, supportive workplace in an agile group where everyone feels empowered to be their authentic selves and share ideas. We embrace a commitment to diversity, equity, and inclusion. ITS values work-life balance and provides flexible work location options where possible. Please see additional information about joining our team.
This is an entry-level position. As a member of the RC User Services team, you will further our organization‚Äôs mission by teaching research computing best practices, directly working with university faculty and researchers, and engage with them to produce new computing solutions within their fields of research.
As an ideal candidate, you have an outgoing personality with a passion for providing high-quality services, enjoy supporting and educating academic researchers, have some experience with academic computing, and are able to apply those skills to UVA‚Äôs research computing environment. We seek an energetic self-starter and quick learner who enjoys technical problem-solving and working on multiple projects simultaneously, has strong communication skills, and is a team player.
Funding is available for a limited amount of travel for professional development.
This position is a hybrid role requiring occasional travel to UVA campus (Grounds) for office work, educational outreach and to meet with researchers. 
Responsibilities:

Provide technical support for use of RC‚Äôs computing and storage services.
Create internal and user-facing technical documentation.
In coordination with senior staff, develop and deliver educational and consulting offerings in their field of expertise.
Support senior staff with organizing outreach events to strengthen the research computing community at UVA.
Engage with and enable researchers to use RC‚Äôs computing resources for processing of research data, including highly sensitive data.

Requirements: 

Bachelor‚Äôs degree in engineering, mathematics, science, or a related area.
Familiarity with data analysis in a Linux environment.
At least one year of relevant experience. Relevant experience may be considered in lieu of a degree

Preferred Education and Experience: 

Master of Science degree in engineering, mathematics, natural or life science, data science or research experience in related area is preferred. At least 2 years of experience with R, Python, or Matlab programming.
Teaching or user training experience in any computational research domain is a plus.
Experience with high-performance computing and basic shell scripting.
Familiarity with using database management systems (e.g. SQL, PostgreSQL, MongoDB, etc.) is a plus. 
Familiarity with software container technology such as Docker and an understanding of the lifecycle of containers.
Experience with version control software such as Git/GitHub.
Experience with Python or R Shiny app development is a plus.

Benefits Include: The choice between 3 different health plans; vision and dental insurance; life insurance; benefits savings accounts; starting with 22 days of paid time off a year in addition to 12 or more paid holidays; 8 weeks of paid parental leave; short term disability; up to $4,360 after your first year for combined use of tuition toward a degree-seeking program or up to $2,000 for professional development including classes, certification training and conferences; and more!
The selected applicant will be required to complete a background check prior to their first day of employment per university policy.
TO APPLY:
Complete an application online and attach:

Cover Letter
Resume

PROCESS FOR INTERNAL UVA APPLICANTS: Please apply through your Workday Home page, search ‚ÄúFind Jobs‚Äù, and search for R0046849
PROCESS FOR EXTERNAL APPLICANTS: Please visit UVA job board:
https://uva.wd1.myworkdayjobs.com/UVAJobs and search for R0046849
The position will remain open until filled.
*** Please note that you MUST upload ALL documents into the CV/Resume box. Applications that do not contain all of the required documents will not receive full consideration. ***
For questions about the application process, please contact Bill Crane Senior IT Recruiter at xer5ff@virginia.edu
For more information about UVA and the Charlottesville community please see  www.virginia.edu/life/charlottesville  and https://embarkcva.com/
Physical Demands:
This is primarily a sedentary job involving extensive use of desktop computers. The job does occasionally require traveling some distance to attend meetings, and programs.
COVID Vaccination Requirement and Guidelines
Please visit the UVA COVID-19 Job Requirements and Guidelines webpage prior to applying for current information regarding vaccination requirements‚ÄØand‚ÄØguidelines for employment at UVA.
The University of Virginia, including the UVA Health System which represents the UVA Medical Center, Schools of Medicine and Nursing, UVA Physician‚Äôs Group and the Claude Moore Health Sciences Library, are fundamentally committed to the diversity of our faculty and staff.  We believe diversity is excellence expressing itself through every person's perspectives and lived experiences.  We are equal opportunity and affirmative action employers. All qualified applicants will receive consideration for employment without regard to age, color, disability, gender identity or expression, marital status, national or ethnic origin, political affiliation, race, religion, sex (including pregnancy), sexual orientation, veteran status, and family medical or genetic information."
rc-website-fork/content/post/alphafold.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2021-07-27T00:00:00-05:00""
title = ""Breaking News: AlphaFold is now available on Rivanna!""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
We are pleased to announce that AlphaFold is now available on Rivanna! Here are some of our user's first few protein structure prediction calculations on Rivanna.

Simple ZFc protein: Similar results for AlphaFold (brown) and I-TASSER (blue). (This figure was created with the RCSB Pairwise Structure Alignment tool.)

146PduD-linker-ZFc protein: AlphaFold's (left) superior ability to predict secondary structure, a Œ≤-sheet in its green-yellow region, whereas I-TASSER (right) is not sufficiently refined to feature any Œ≤-strands. (This figure was created with NGL Viewer.)
(Credits: David Bass and Prof. Keith Kozminski, Department of Biology)
FAQ
What is AlphaFold?
AlphaFold is an AI for protein structure prediction developed by Google DeepMind. Watch DeepMind's video or read the Nature article.
How do I use AlphaFold on Rivanna?
See here for installation and usage details. If you are a PI interested in using AlphaFold on Rivanna but do not have an account, please request an allocation."
rc-website-fork/content/post/2019-september-maintenance-notes.md,"+++
author = ""Staff""
date = ""2019-09-17T05:18:25-05:00""
title = ""Important Notes from the 17 September Rivanna Maintenance""
draft = true
tags = [""rivanna"",""maintenance"",""feature""]
categories = [""feature""]
summary = ""Learn about recent changes implemented during the Sept. 17, 2019 maintenance.""
+++
Rivanna was down for maintenance on Tuesday, September 17.  The items below summarize the changes that may impact the users of Rivanna.

I.  Changes to scratch
System engineers have installed a new /scratch file system, and have transferred to the new system any files/data that were less than 90 days old on the former scratch system.
II.  Updates to software modules
New and updated modules:
The following software modules either replace older versions or are new to Rivanna: 
 - pgi/19.7

 - openmpi/3.1.4 (for all GCC and PGI compilers)

 - cuda/10.1.168

For openmpi, be sure to remove any reference to 2.1.5 in your scripts.
Removed modules:
The following software modules were removed from Rivanna during the maintenance period:
- cellranger/2.1.1 (replaced with cellranger/3.1.0)

- exonerate/2.2.0 (replaced with exonerate/2.4.0)

- fenics/20180

- fluent/18.2 (is now part of the ansys/18.2 module)

- fiji/1.51

- miniconda/4.3.21-py3.6 (replaced with anaconda/5.2.0-py3.6

- openmpi/2.1.5 (replaced with openmpi/3.1.4)

- pgi/17.5 &  pgi/18.10 (replaced with pgi/19.7)

- povray/3.7.0.7

- rstudio/0.98.1103

III. Other important changes
{{% callout %}}
The loading of some software modules now requires preloading of a dependency, such as a compiler or version of mpi.
{{% /callout %}}
Run the command module spider <YOUR_MODULE> to view module load instructions for a particular application module.
For example,  module spider abinit/8.2.2  states that
 You will need to load all module(s) on any one of the lines below before the ""abinit/8.2.2"" module is available to load.
 intel/18.0  intelmpi/18.0

This statement tells you that both intel and intelmpi must be loaded in order to load abinit.
{{% callout %}}
The operating system was updated, and (as usual) users who compile their own code may need to recompile.

This also applies to anyone who installed R packages which are dependent on openMPI. Those packages will need to be reinstalled.

{{% /callout %}}
{{% callout %}}
Libraries and applications built with the Intel 18.0 compiler and IntelMPI libraries have been re-compiled to enable execution on compute nodes with Knights Landing Many-Core processors in the knl queue.
{{% /callout %}}
If you have any questions or concerns about these changes, please contact our user support team at hpc-support@virginia.edu.

{{< button button-class=""primary"" button-text=""About HPC"" button-url=""/userinfo/hpc"" >}}"
rc-website-fork/content/post/2022-may-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-05-02T00:00:00-05:00""
title = ""Rivanna Maintenance: May 17, 2022""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on May 17, 2022 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Users will not be able to access the Globus data transfer node (UVA Main-DTN) or Research Project storage during the maintenance period. All systems are expected to return to service by 6 a.m. on Wednesday, May 18.
IMPORTANT MAINTENANCE NOTES

The operating system will be upgraded from CentOS 7.8 to 7.9. This should have no impact on the software built on Rivanna, whether it be modules or your own compiled codes. If you need assistance to rebuild your code, please contact hpc-support@virginia.edu.
Slurm will be upgraded to 21.08.8, which requires us to rebuild the OpenMPI module. If your code was built with OpenMPI and it no longer works after maintenance, you may need to rebuild it.
NVIDIA Driver will be upgraded to 470.103.01. You should not need to rebuild CUDA programs.

Modules

The following software modules will be removed from Rivanna during the maintenance period:

| Module | Removed version | Replacement |
|---|---|---|
|gcc       |6.5.0 | 7.1.0, 9.2.0 |
|mvapich2  |2.3.1, 2.3.3 | Please use gcc openmpi or intel intelmpi. |
|nvhpc     |20.9 | 21.9 |
|alphafold |2.0.0, 2.1.1 | 2.1.2, 2.2.0 |
|awscli    |2.1.10 | 2.4.12 |
|cellranger|2.2.0, 3.0.2, 3.1.0 | 4.0.0, 5.0.0, 6.0.1 |
|cmake     | 3.5.2, 3.12.3 | 3.6.1, 3.16.5 |
|gatk      |3.8.1.0, 4.0.0.0, 4.1.6.0 | 4.2.3.0 |
|metamorpheus|0.0.311-dev, 0.0.317 | 0.0.320 |
|mpi4py    |3.0.0-py2.7, 3.0.3 | Load any MPI toolchain (e.g. gcc openmpi) plus anaconda and run pip install --user mpi4py; see here |
|picard    |2.1.1, 2.18.5, 2.20.6 | 2.23.4 |
|rapidsai*  |0.19 | 21.10 |
*Archived containers can be found in /share/resources/containers/singularity/archive.

Upgrades:
Addition of Matplotlib widget ipympl/0.8.7 to JupyterLab
tensorflow/2.8.0
swig/4.0.2



Default version changes:
    - alphafold/2.1.1 ‚Üí 2.2.0
    - cellprofiler/3.1.8 ‚Üí 4.2.1
    - cuda/11.0.228 ‚Üí 11.4.2
    - diamond/0.9.13 ‚Üí 2.0.14
    - igvtools/2.8.9 ‚Üí 2.12.0
    - matlab/R2021b ‚Üí R2022a
    - totalview/2019.0.4_linux_x86-64 ‚Üí 2021.4.10
    - zlib/1.2.11 ‚Üí 1.2.12 - hidden module; load zlib/.1.2.12

New modules:
nvompic/21.9_3.1.6_11.4.2 toolchain (nvhpc/21.9 + openmpi/3.1.6 + cuda/11.4.2)
libraries: scalapack, fftw, hdf5
berkeleygw/3.0.1
quantumespresso/7.0
yambo/5.0.4


pandoc/2.17
trinity/2.13.2
cufflinks/2.2.1
redis-cli/6.2.6


"
rc-website-fork/content/post/2025-april-ssz-h200-announcement.md,"Ôªø+++ 
images = [""""]
author = ""Staff""
description = """"
date = ""2025-05-05T00:00:00-05:00""
title = ""New NVIDIA H200 GPU Node Added to Afton""
draft = false
tags = [""afton"",""hpc""]
categories = [""feature""]
+++

We are excited to announce the expansion of UVA‚Äôs AI computing capabilities. On April 3, we added a NVIDIA HGX H200 GPU node to the Afton high-performance computing (HPC) cluster, the first of many planned for the coming year. Each node provides 2TB of node CPU memory and 8-way connected Tensor Core GPUs with 141GB of VRAM memory per device. These devices offer higher performance than the current Afton GPU nodes, opening new possibilities for the most challenging deep learning and large language model computations.
How can I get access to the new hardware?
On April 3, the new GPU nodes became available as a beta-release. During this phase the new nodes were accessible for all users with active Afton HPC allocations. Please follow these instructions to use the HGX H200 nodes in your jobs. Jobs running on the new hardware will consume service units based on charge rates reflective of the actual hardware and service cost. Visit our webpage for a complete list of service unit charge rates here.
On May 1, the new HGX H200 node was released into full production. Access and service unit charge rates remain in effect as posted. Additional hardware upgrades or configuration updates, if needed, will be handled as part of Afton‚Äôs pre-announced regular maintenance cycles.   
Where can I learn more?
More detailed descriptions of the new hardware‚Äôs capabilities and how to use it are available on our website. In addition, you may reach out to our User Services team during virtual office hours or by submitting a support request."
rc-website-fork/content/post/2019-jira-downtime.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-12-09T11:18:25-05:00""
title = ""JIRA Downtime: December 13, 2020""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
The JIRA ticketing system will be taken offline on Friday, December 13 from 6 p.m. to 9 p.m. while our system engineers continue the process of migrating the ticketing system from a local environment to the cloud. Please avoid submitting requests during this period if possible. Although moving to a cloud-based ticketing system will improve the speed and efficiency of our customer service in the long run, in the short-term it may cause disruptions for some users.
If you are unable to log in to JIRA after the migration is completed, you will need to change your password using your UVA e-mail address. Please note: Eservices passwords will not work in JIRA cloud."
rc-website-fork/content/post/2021-october-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2021-10-05T00:00:00-05:00""
title = ""Maintenance Day: October 12, 2021""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}The Globus data transfer nodes (DTNs) and Rivanna's parallel nodes will be down for maintenance on Tuesday, October 12, 2021, between 8 a.m. and 4 p.m.{{< /alert-green >}}
Users will be unable to transfer data using Globus or run parallel jobs on Rivanna during this period. 
All other systems and services‚Äîincluding storage‚Äîare expected to continue operating normally.
IMPORTANT MAINTENANCE NOTES


The following Rivanna software changes will be implemented during the maintenance period:

IDL/8.4- replaced by 8.8 (8.7.2 still available)
ANSYS default version changing from 2021r1 to 2021r2


"
rc-website-fork/content/post/2020-december-maintenance-update-1.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-12-18T00:00:00-05:00""
title = ""Rivanna Maintenance Extended""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
The maintenance period has been extended for another 72 hours minimum. We apologize for the inconvenience. Rivanna was expected to return to service on December 17, but ongoing electrical work in the data center and other obstacles have delayed our engineering team's progress.
We will send out an e-mail announcement as soon as the maintenance period ends. Updates will also be posted on our website."
rc-website-fork/content/post/what-is-research-computing.md,"+++
images = [""""]
author = ""Staff""
description = """"
title = ""What is Research Computing?""
date = ""2017-04-19T10:26:34-05:00""
draft = false
tags = [
  ""rc"",
  ""hpc"",
  ""rivanna"",
  ""ivy"",
  ""cloud"",
  ""xsede"",
  ""feature""
]
categories = [
  ""feature"",
  ""about""
]
+++
UVA Research Computing (RC) is a new program that aims to support computational biomedical research by providing advanced cyberinfrastructure and expertise in data analysis at scale. Our mission is to foster a culture of computational thinking and promote interdisciplinary collaboration in various data-driven research domains. We offer services related to high performance computing, cloud architecture, scientific programming and big data solutions. We also aim to promote computationally intensive research at UVA through collaborative efforts such as UVA's own CADRE (Computation And Data Resource Exchange) and XSEDE (Extreme Science and Engineering Discovery Environment).
One of our driving philosophies is that researchers already have medical and scientific expertise, and should not have to become computing experts on top of that. To that end, our approach is service- and project-driven. 
Rather than approach your research with a specific technology or platform in mind beforehand, we want to understand your research goals and how computational support will help you get there.

Ways to get started or learn more:


    Our Services
  

User Support

Bioinformatics & Genomics
Cloud Solutions
Data Analysis
High Performance Computing
Image Processing

Collaboration
Citations

"
rc-website-fork/content/post/2023-women-in-hpc-20230404.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-03-13T10:18:25-05:00""
title = ""Virginia Women in HPC - Student Lightning Talks""
draft = false
tags = [""rivanna"",""whpc"",""va-whpc"",""hpc""]
categories = [""feature""]
+++
What: Join us in welcoming 11 undergraduate and graduate students from across Virginia to talk about their research. The talks will be lightning style format allowing 3 minutes for students to present and 1-2 questions and 1-2 questions from the audience. Don‚Äôt miss out on this fantastic opportunity to hear about a variety of research topics within HPC!
Event Time: April 4, 2023, 01:00 PM EST (US and Canada).


Featured Speakers:


Lakshmi Miller - Graduate Student
    Aerospace and Ocean Engineering, Virginia Tech
    ‚ÄúCFD Informed Maneuvering of AUVs‚Äù


Rashmi Chawla - Graduate Student
    Aerospace Engineering, Virginia Tech
    ‚ÄúMultiphysics Modeling of UHTCs using Material Point Method‚Äù


Naina Pisharoti - Graduate Student
    Aerospace Engineering, Virginia Tech
    ‚ÄúHigh-fidelity Computational Analysis of UAV Propellers‚Äù


Liza Harold - Undergraduate Student
    Biomedical Engineering, University of Virginia
    ‚ÄúInvestigating the Role of Stereocomplexation in Peptide Assembly via Molecular Dynamics Simulations‚Äù


Clare Cocker - Undergraduate Student
    Chemical Engineering, University of Virginia
    ‚ÄúRole of Amino Acid sSereochemistry in the Assembly of Peptide Hydrogels for Tissue engineering‚Äù


Marion LoPresti - Undergraduate Student
    Biochemistry, Virginia Tech
    ‚ÄúUtilizing HPC to Explore the Dynamics of the Druggable Dengue Virus Protease‚Äù


Nhi Huynh - Graduate Student
    Engineering and Technology, Old Dominion University
    ‚ÄúRSM: To Increasing the Capacity of the Deep GCN (Graph Convolution Neural) Image‚Äù


Sarah Patterson - Undergraduate Student
    Developmental Biology, William & Mary
    ‚ÄúSingle Cell RNA Sequencing in Xenopus Laevis Embryology‚Äù


Cynthia Sias - Graduate Student
    Plant and Environmental Sciences, Virginia Tech
    ‚ÄúEvaluating the Effect of Cover Crop Termination Management on Palmer Amaranth (Amaranthus palmeri) Suppression in Soybean‚Äù


Afrina Tabassum - Graduate Student
    Computer Science, Virginia Tech
    ‚ÄúMultimodal Learning: Representation and Generation‚Äù


Mahshid Ahmadian - Graduate Student
    Systems Modeling and Analysis, Virginia Commonwealth University
    ‚ÄúModeling Salmon Shark‚Äôs Location in the Pacific Ocean Using Stochastic Process approach‚Äù


Moderators:

Mark Gardner - Network Research Manager, Advanced Research Computing, Virginia Tech
Heather Baier - Ph.D. Student in Computational Geography, William & Mary


This virtual event is jointly hosted by Virginia Commonwealth University, George Mason University, Virginia Tech, William & Mary,
University of Richmond, Virginia Institute of Marine Science, Old Dominion University, and the University of Virginia, 
{{% vawhpc %}}"
rc-website-fork/content/post/2024-oct-anaconda-transition.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-10-16T00:00:00-05:00""
title = ""Transition from Anaconda to Miniforge: October 15, 2024""
url = ""/maintenance""
draft = false
tags = [""anaconda""]
categories = [""feature""]
+++

Due to the new licensing restrictions by Anaconda  on research usage, the licensed Anaconda distribution was removed from the system on October 15, 2024. The current anaconda/2023.07-py3.11 module will redirect to the miniforge/24.3.0-py3.11 module, switching to conda-forge as the default package installation channel with fewer preinstalled packages. Existing environments will not be affected. However, using Anaconda default channels for research without a personal license will violate the Anaconda license. For instructional use, package installation from licensed channels is still allowed

Maintenance: Oct 15, 2024
{{< alert-green >}}The UVA high-performance computing (HPC) system will be down for maintenance on Tuesday, Oct 15, 2024, beginning at 6 a.m. The HPC systems are expected to return to full service by 6 a.m. on Wednesday, Oct 16.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until the HPC systems are returned to service.
Questions: Please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m..
{{% callout %}}
What to expect after the Oct 15 maintenance?
The licensed Anaconda distribution and base environment provided by the anaconda module will be removed from our systems on Oct 15, 2024. The anaconda module will redirect to the new miniforge/24.3.0-py3.11 module, effectively switching to conda-forge as the only default package installation channel and with a reduced number of preinstalled packages in the base environment.  
The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels as shown in the example above. However, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license.   
We understand that these changes may cause inconvenience, but these changes are mandated by the Anaconda licensing condition which we cannot control. If you have any questions or concerns, please feel free to reach out. 
What to expect after the December 12?
We will be removing the dummy module anaconda/2023.07-py3.11 from our system on December 12. The Conda package manager, along with a base environment, will only be available by loading the miniforge/24.3.0-py3.11 module. Please refer to the FAQ for further instructions on how to use the Miniforge module.""
{{% /callout %}}
FAQ
{{% accordion-group title=""Group"" id=""faqgroup""%}}
{{% accordion-item title=""1. How miniforge is different from Anaconda?"" id=""faq-1"" %}}
Miniforge and Anaconda are both popular tools for managing Python environments and packages, but they differ in a few key ways:
1. Size and Preinstalled Packages:

Anaconda base environment came with a large number of preinstalled data science libraries. However, miniforge only includes the essential Conda and Mamba package managers along with commonly used packages such as numpy, pandas, matplotlib, etc., from the conda-forge channel.

2. Default Package Channels:

Anaconda: Uses Anaconda‚Äôs proprietary channels (Anaconda repository) for package installations by default. These packages may have specific licensing restrictions.
Miniforge: Uses conda-forge as its default channel, an open-source community-driven repository, ensuring more transparency and flexibility without proprietary limitations.

3. Licensing:

Anaconda: The default Anaconda distribution has licensing restrictions for commercial and research use, requiring a paid license for certain types of usage.
Miniforge: Has no such restrictions since it uses conda-forge, which provides fully open-source packages.

{{% /accordion-item %}}
{{% accordion-item title=""2. Can I still use my conda environements?"" id=""faq-2"" %}}
The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels as shown in the example below.
conda install -n path-to-my-conda-enf seaborn -c anaconda
However, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license. Therefore, for research use, it is expected to replace packages installed through the anaconda restricted channels with packages from non-proprietary channels such as conda-forge.   
{{% /accordion-item %}}
{{% accordion-item title=""3. How to use miniforge to create conda envs?"" id=""faq-3"" %}}
The process and commands for creating conda environments using Miniforge are exactly the same. The only difference is that you need to load the Miniforge module instead of the Anaconda module on our system.Basically,
module load miniforge
conda create -n your_env_name_goes_here (default Python version: use conda info to find out)
{{% /accordion-item %}}
{{% accordion-item title=""4. Why are we switching from Anaconda to Miniforge?"" id=""faq-4"" %}}
Miniforge avoids violating Anaconda's Terms of Service because it pulls packages from the conda-forge channel by default. Conda-forge is a community led collection of recipes, build infrastructure and distributions for the conda package manager and is free to use.
{{% /accordion-item %}}
{{% accordion-item title=""5. Will I lose access to any packages that I had with Anaconda?"" id=""faq-5"" %}}
Existing environments will not be removed, however, any packages installed in your conda environments via the Anaconda default (proprietary) channel will need to be reinstalled through a different channel or tool such as conda-forge or pip.
{{% /accordion-item %}}
{{% accordion-item title=""6. How do I install Anaconda packages if I need them (e.g., licensed or proprietary ones)?"" id=""faq-6"" %}}
The Miniforge module includes the conda package management system. You can use conda install <package-name> as you may have done previously using the Anaconda module. Miniforge uses the conda-forge channel by default.
If you are using Anaconda for instructional use or have your own license, you can install packages from the anaconda channel using conda install -c anaconda <package-name>
{{% /accordion-item %}}
{{% accordion-item title=""7. Can I still use pip to install non-Conda packages with Miniforge?"" id=""faq-7"" %}}
Yes. Miniforge supports installation of non-Conda packages with pip and uses the same syntax. 
{{% /accordion-item %}}
{{% accordion-item title=""8. Will my existing Conda environments work with Miniforge?"" id=""faq-8"" %}}
Yes. Miniforge supports installation of non-Conda packages with pip and uses the same syntax. 
{{% /accordion-item %}}
{{% accordion-item title=""9. How do I update packages and environments in Miniforge?"" id=""faq-9"" %}}
Once the environment is activated you an update packages and environments in the same fashion as you would with Anaconda. To activate all packages run:
conda update --all
To activate a single package you would run:
conda update <name>
{{% /accordion-item %}}
{{% accordion-item title=""10. Will my scripts that depend on specific Anaconda packages break when switching to Miniforge?"" id=""faq-10"" %}}
Scripts using Anaconda packages that are also available within Miniforge may not break. However, any Anaconda packages that are not used soley for instructional purposes would be violating the Anaconda license terms, so they would need to be reinstalled through a different channel or tool such as conda-forge or pip.
{{% /accordion-item %}}
{{% accordion-item title=""11. What channels are available by default in Miniforge?"" id=""faq-11"" %}}
The conda-forge channel is set as the default (and only) channel for Miniforge.
{{% /accordion-item %}}
{{% accordion-item title=""12. Can I still use the Anaconda repository with Miniforge?"" id=""faq-12"" %}}
The Anaconda repository can only be used with miniforge provided that the packages are used strictly for instructional purposes.
{{% /accordion-item %}}
{{% accordion-item title=""13. How do I migrate my existing Anaconda environments to Miniforge?"" id=""faq-13"" %}}
First, you'll need to load the Miniforge module and activate the environment
module load miniforge
source activate <env_name>
Next, you'll need to export the existing environment to a yaml which will be used for rebuilding, then remove the existing environment. You'll need to deactivate the environment prior to removing:
conda env export -f <env_name>.yml
conda deactivate
conda env remove --name <env_name>
{{% callout %}}
Note: Use an editor to remove the proprietary channels (e.g. defaults or anaconda) from the yaml file.
{{% /callout %}}
You might encounter issues with resolving dependencies in which case you might need to leave out the versions of the packages in the yaml file and automatically install the most recent compatible versions. You can use the command conda env export | cut -f 1 -d ""="" | grep -v ""prefix"" > <env_name>.yml in place of the above command to remove the versioning information of packages while exporting the environment into a yaml file.
You can then recreate the environment with Miniforge using the following:
conda env create -f <env_name>.yml
{{% /accordion-item %}}
{{% accordion-item title=""14. How do I get help if I encounter problems during the transition?"" id=""faq-14"" %}}
You can either submit a support request on our website  or you can attend one of our office hour sessions. We meet virtually over Zoom on Tuesdays (3-5PM) and Thursdays (10AM-12PM) via Zoom. Links to the sessions can be found at the bottom of this page on our website. 
{{% /accordion-item %}}
{{% /accordion-group %}}
Announcements
{{% accordion-group title=""Comms"" id=""commsgroup"" %}}
{{% accordion-item title=""Aug 27, 2024 - Change to Anaconda Module on Our System"" id=""comm-1"" %}}
Dear PI, 
This message is important if you intend to use the anaconda/2023.07-py3.11 for your fall classes. Due to the recent licensing restrictions by Anaconda  on research usage, we will be removing the licensed Anaconda distribution from our system on October 15, 2024.  
As an alternative we have installed the miniforge/24.3.0-py3.11 module, which includes the essential Conda and Mamba package managers along with commonly used packages such as numpy, pandas, matplotlib, etc., from the conda-forge channel. 
How should I prepare? 
Between now and October 15, you may load the existing anaconda module or the newly installed miniforge/24.3.0-py3.11 module. Both modules provide the same conda commands to manage and use your conda environments.  
Important: By default, the miniforge distribution will only provide packages from the conda-forge channel. Therefore, if you require packages from channels that are covered by the Anaconda repository Terms of Service (main/anaconda, r, msys2) you may specify this in your installation command but only for environments that are restricted to educational use, i.e., instructional work in your classes. For example, to install the seaborn package from the Anaconda default channels, you would use: 
conda install ‚Äìn path-to-my-conda-env seaborn ‚Äìc anaconda 
What to expect on October 15, 2024? 
The licensed Anaconda distribution and base environment provided by the anaconda module will be removed from our systems on Oct 15, 2024. The anaconda module will redirect to the new miniforge/24.3.0-py3.11 module, effectively switching to conda-forge as the only default package installation channel and with a reduced number of preinstalled packages in the base environment.  
The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels as shown in the example above. However, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license.   
We understand that these changes may cause inconvenience, but these changes are mandated by the Anaconda licensing condition which we cannot control. If you have any questions or concerns, please feel free to reach out. 
At your service, RC staff 
Research Computing
E hpc-support@virginia.edu
P 434.243.1107
University of Virginia
P.O. Box 400231
Charlottesville 22902
{{% /accordion-item %}}
{{% /accordion-group %}}"
rc-website-fork/content/post/2021-remaining-maintenance-days-2021.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2021-06-23T11:18:25-05:00""
title = ""Remaining maintenance days in 2021""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
Rivanna will be down for maintenance on Tuesday, October 12 and Tuesday, December 14. Please plan accordingly. We do not anticipate any additional maintenance days in 2021."
rc-website-fork/content/post/2025-Call-for-Posters.md,"+++
title = ""RC Exhibition Call for Posters: March 31, 2025""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
date = ""2025-03-16T12:57:24-05:00""
tags = [""rc""]
categories = [""feature""]
draft = false
+++

Important Dates
New Submission Date: March 31, 2025
UVA RC Exhibition: April 23, 2025:   
`
Everyone is welcome to attend the Exhibition on April 23rd, at the Newcomb Hall Ballroom, 10:00am - 2:00pm. 
Our Exhibition includes a judged poster competition with travel awards presented to the winning finalists. If you have used Research Computing resources to support your research, we invite you to submit a poster! Posters should reflect how you have used these resources to support your research. 
Three finalists will be recognized with a travel award that can be used for professional travel such as conferences, meetings of professional associations, or other travel related to your research.  
First Place: $3,000 travel voucher 
Second Place: $2,000 travel voucher 
Third Place: $1,000 travel voucher 
Although all researchers using Research Computing resources are invited to participate, the lead author must be someone other than a faculty member to be eligible for an award. Resources used may include systems, such as Rivanna, Afton, or Ivy, as well as collaboration with Research Computing staff.  
Submit your poster PDF on the Poster Competition Submission Form on or before March 31st in one of the following categories: Biological and Health Sciences, Physical and Health Sciences, Social Sciences and Humanities, or Other. Participants must print their poster for the event day.  
Judging Information 
The judges will review the PDFs prior to the event to select the finalists for the awards. During the event, judges will visit your poster and ask questions related to your research. Prizes will be awarded at the end of the Exhibition.  
To select our finalists, judges will consider the following criteria: 
How much did using RC resources contribute to the outcome of the research presented? 
How well is the use of RC resources communicated on the poster? 
How well is the importance of this scientific investigation communicated? 
We look forward to your participation in this event! Please contact rc-events@virginia.edu with questions."
rc-website-fork/content/post/2024-oct-ivy-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-10-01T00:00:00-05:00""
title = ""High-Security Standard Storage Maintenance: Oct 15, 2024""
draft = false
tags = [""rivanna"", ""afton""]
categories = [""feature""]
+++
{{< alert-green >}}The Ivy Virtual Machines (VMs) and high security zone HPC system will be down for storage maintenance on Tuesday, Oct 15, 2024, beginning at 6 a.m.  
The system is expected to return to full service by 6 a.m. on Wednesday, Oct 16..{{< /alert-green >}}
IMPORTANT MAINTENANCE NOTES
During the maintenance all VMs will be down as well as the UVA Ivy Data Transfer Node (DTN) and Globus services. The High-Security HPC cluster will also be unavailable for all job scheduling and viewing. 
If you have any questions about the upcoming Ivy system maintenance, you may contact our user services team. 
Ivy Central Storage transition to HSZ Research Standard
To transition from old storage hardware, we have retired the Ivy Central Storage and replaced it with the new High Security Zone Research Standard storage. There will be new filesystem locations for Ivy Linux VMs and updated shortcuts on Windows VMs. 
Filesystem locations can be found on the RC website Research Data Storage Page."
rc-website-fork/content/post/2019-sept17-notes.md,"+++
author = ""Staff""
date = ""2019-09-17T05:18:25-05:00""
title = ""Important Notes from the 17 September Rivanna Maintenance""
draft = false
tags = [""rivanna"",""maintenance"",""feature""]
categories = [""feature""]
summary = ""Learn about recent changes implemented during the Sept. 17, 2019 maintenance.""
+++
Rivanna was down for maintenance on Tuesday, September 17.  The items below summarize the changes that may impact the users of Rivanna.

I.  Changes to scratch
System engineers have installed a new /scratch file system, and have transferred to the new system any files/data that were less than 90 days old on the former scratch system.
II.  Updates to software modules
New and updated modules:
The following software modules either replace older versions or are new to Rivanna: 
 - pgi/19.7
 - openmpi/3.1.4 (for all GCC and PGI compilers)
 - cuda/10.1.168

For openmpi, be sure to remove any reference to 2.1.5 in your scripts.
Removed modules:
The following software modules were removed from Rivanna during the maintenance period:
- cellranger/2.1.1 (replaced with cellranger/3.1.0)
- exonerate/2.2.0 (replaced with exonerate/2.4.0)
- fenics/20180
- fluent/18.2 (is now part of the ansys/18.2 module)
- fiji/1.51
- miniconda/4.3.21-py3.6 (replaced with anaconda/5.2.0-py3.6
- openmpi/2.1.5 (replaced with openmpi/3.1.4)
- pgi/17.5 &  pgi/18.10 (replaced with pgi/19.7)
- povray/3.7.0.7
- rstudio/0.98.1103

III. Other important changes
{{% callout %}}
The loading of some software modules now requires preloading of a dependency, such as a compiler or version of mpi.
{{% /callout %}}
Run the command module spider <YOUR_MODULE> to view module load instructions for a particular application module.
For example,  module spider abinit/8.2.2  states that
 You will need to load all module(s) on any one of the lines below before the ""abinit/8.2.2"" module is available to load.
 intel/18.0  intelmpi/18.0

This statement tells you that both intel and intelmpi must be loaded in order to load abinit.
{{% callout %}}
Recompiling & Reinstalling
The operating system was updated, and (as usual) users who compile their own code may need to recompile.

This also applies to anyone who installed R packages which are dependent on openMPI. Those packages will need to be reinstalled.

Libraries and applications built with the Intel 18.0 compiler and IntelMPI libraries have been re-compiled to enable execution on compute nodes with Knights Landing Many-Core processors in the knl queue.
{{% /callout %}}
If you have any questions or concerns about these changes, please contact our user support team.

{{< button button-class=""primary"" button-text=""About HPC"" button-url=""/userinfo/hpc"" >}}
{{< support-button >}}"
rc-website-fork/content/post/2023-july-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-07-10T00:00:00-05:00""
title = ""Rivanna Maintenance: July 18, 2023""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on July 18, 2023 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
All systems are expected to return to service by 6 a.m. on Wednesday, July 19.
IMPORTANT MAINTENANCE NOTES
New scratch
RC engineers will be installing a new /scratch storage filesystem that can be accessed at /scratch/$USER after the end of maintenance.
Modified queue limits will be implemented to provide maximum read/write performance of the new /scratch filesystem. Users are encouraged to consult our updated documentation and adjust their job scripts accordingly.
The current /scratch filesystem will be permanently retired on October 17, 2023 and all the data it contains will be deleted.  We have prepared a sample script for users who wish to transfer files to the new scratch system.  Users should clean up their current /scratch directory in preparation, to minimize the load.  A sample script is posted below.
Example script to copy files
{{< pull-code file=""/static/scripts/demo-copy-scratch.slurm"" lang=""bash"" >}}
The script will also be available through the Open OnDemand Job Composer:

Go to Open OnDemand Job Composer
Click: New Job -> From Template
Select demo-copy-scratch
In the right panel, click ""Create New Job""
This will take you to the ""Jobs"" page. In the ""Submit Script"" panel at the bottom right, click ""Open Editor""
Enter your own allocation. You may edit the script as needed. Click ""Save"" when done.
Going back to the ""Jobs"" page, select demo-copy-scratch and click the green ""Submit"" button.

As we expect a high volume of data migration, please refrain from doing so directly on the login nodes but instead submit it as a job via the provided Slurm script as described above.
The new scratch is subject to the same 10 TB quota and 90-day purge policy. There is no restriction on the number of files. A friendly reminder that scratch is intended as a temporary work directory, not long-term storage space. It is not backed up and old files need to be purged periodically for system stability. RC offers a number of low-cost storage options to researchers. For more information, visit our storage page.
Modules
The following software modules will be removed from Rivanna during the maintenance period:
| Module | Removed version | Replacement |
|---|---|---|
|abinit |8.2.2 (intel/18.0), 8.10.3 (intel/20.0) | 8.10.3 (intel/2022.11) |
|maven | 3.3.9 | 3.9.0 |
|postgresql | 11.3 | 14.5 |"
rc-website-fork/content/post/lolaweb.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2018-06-06T15:18:25-05:00""
title = ""LOLAweb""
draft = false
tags = [""projects"",""lolaweb""]
categories = [""post""]
+++
From his lab in the Center for Public Health Genomics at UVa, Nathan Sheffield seeks to develop a deeper understanding of functional genomics. Dr. Sheffield and his collaborators study epigenetic mechanisms, including DNA methylation, which can involve analyzing enrichment of genomic region set data. By identifying patterns of enriched genomic regions, one can differentiate between normal and diseased gene regulation. Dr. Sheffield builds on this research focus as well as a history of open-source software development with the publication of LOLAweb: A containerized web server for interactive genomic locus overlap enrichment analysis as part of a special web server issue of Nucleic Acids Research. 

The LOLAweb methodology for locus overlap analysis is derived from the LOLA R package, which was written and developed by Dr. Sheffield. Since its publication in 2016, LOLA has been cited in several dozen publications, and is currently in the top 20% of packages downloaded from the Bioconductor repository. These citations and downloads demonstrate the variety of scientific use-cases for quantifying overlap and enrichment for regions of interest. 
According to Dr. Sheffield, restricting the locus overlap analysis methods to a purely programmatic interface may prove to be a barrier for some biologists. However, by teaming with RC to create LOLAweb, the functionality from LOLA will become more accessible to the research community, thus allowing more researchers to develop new hypotheses. LOLAweb closely mirrors LOLA, providing users graphical drop-downs and inputs to upload their data. The application also features data visualizations that are not available in the original package. RC has also helped design LOLAweb with a flexible deployment, which includes a feature to cache results so that users can bookmark or share output with colleagues. 
The collaboration between Dr. Sheffield and RC staff is not the first, nor will it be the last. In 2017, RC staff contributed to the simpleCache R package and co-authored a resulting publication in the Journal of Open Source Software.  RC is also currently working with Sheffield Lab members to optimize containerized data analysis pipelines and develop new software packages. 

Read more: LOLAweb: A containerized web server for interactive genomic locus overlap enrichment analysis.
Visit LOLAweb: http://lolaweb.databio.org/
"
rc-website-fork/content/post/2026-hpc-maintenance-dates.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2026-01-01T00:00:00-05:00""
title = ""HPC Maintenance Schedule for 2026""
url = ""/maintenance""
draft = false
tags = [""rivanna"",""afton""]
categories = [""feature""]
+++
Rivanna/Afton will be taken down for maintenance in 2026 on the following days: 

[Winter: Tuesday, January 6, 2026]
Spring: 
Summer:
Fall: 

Please plan accordingly. Questions about the 2026 maintenance schedule should be directed to our user services team."
rc-website-fork/content/post/2025-DAC-Awards.md,"+++
title = ""New DAC Small Award Proposal Deadlines""
description = """"
author = ""RC Staff""
date = ""2025-05-30T12:57:24-05:00""
tags = [""dac""]
categories = [""feature""]
draft = false
+++
The Data Analytics Center (DAC) is accepting proposals for the Small Analytics Resource Award. 
If you have a computationally-intensive project and would like a collaborator, consider applying for a Small Analytics Resource Award. The DAC team is ready to support your research in AI, Bioinformatics, Image Processing, Data Analysis, Data Management, and more.
The next proposal deadline is 30 June 2025.
For details on how to apply for the Small Award, visit the Awards website"
rc-website-fork/content/post/2019-fall-workshops.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2019-09-02T15:18:25-05:00""
title = ""Fall 2019 Workshop Series""
draft = false
tags = [""education"",""workshops"",""feature""]
categories = [""feature""]
+++

RC staff are teaching a series of free hands-on workshops this fall that are open to all UVA researchers. Space is limited, so register today! Topics include:

Image Processing with Fiji/ImageJ (Sept 11)
MATLAB Fundamentals (Sept 12)
Programming in MATLAB (Sept 19)
Optimizing R Code (Sept 24)
Parallel Computing in MATLAB (Sept 26)
Introduction to Parallel R (Oct 1)
Machine Learning with MATLAB (Oct 3)
Automation of Image Processing with Fiji/ImageJ (Oct 9)
Deep Learning with MATLAB (Oct 17)
Moving R Programs to Rivanna (Oct 17)

Browse Workshops"
rc-website-fork/content/post/2020-march-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-02-28T10:18:25-05:00""
title = ""Rivanna Maintenance: March 11, 2020""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Wednesday, March 11, beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
Rivanna is expected to return to service later in the day.
The following software modules will be removed from Rivanna during the maintenance period (please use the suggested newer versions):

gcc/5.4.0 & toolchains -> 7.1.0
All modules that depend on gcc/5.4.0 will be available under gcc/7.1.0.
The only exception is cushaw3/3.0.3. Please contact us if you need to use it.


pgi/19.7 & toolchains -> 19.10
All modules that depend on pgi/19.7 will be available under pgi/19.10.


anaconda/5.2.0-py2.7 -> 2019.10-py2.7
All modules that depend on anaconda/5.2.0-py2.7 will be available under anaconda/2019.10-py2.7.


tensorflow/1.6.0-py27, -py36 -> 1.12.0, 2.0.0, or 2.1.0
If you must use version 1.6.0, please pull the image from our repository on Singularity Library.


singularity/3.1.1 -> 3.5.2
boost/1.66.0 -> 1.68.0
julia/1.0.2, 1.0.3 -> 1.3.1
cushaw3/3.0.3 - no replacement

The following upgrades will take place during the maintenance period:

JupyterLab v1.2.6
Python 3.7 Jupyter kernel - based on anaconda/2019.10-py3.7
The previous ""Python 3"" kernel (based on anaconda/5.2.0-py3.6) has been renamed as ""Python 3.6"".


anaconda/2019.10-py2.7, -py3.7
gcc/9.2.0 & toolchains
singularity/3.5.2 - now default version
gurobi/9.0.1
tensorflow/2.1.0-py37 - Singularity container module & Jupyter kernel
julia/1.3.1 - module & Jupyter kernel
ansys/2020r1
samtools/1.10
rust/1.41.0
cmake/1.16.5

New tools:

LibreOffice - through FastX Web desktop environment
pytorch/1.4.0 - Singularity container module & Jupyter kernel
openfoam 7 (version 1909) - open-source CFD software
goolfc/6.5.0_3.1.4_10.1.168 - GOOLF toolchain (GCC + OpenMPI + OpenBLAS + ScaLAPACK + FFTW) with CUDA support
atom/1.43.0 - Atom text editor
rclone/1.51.0 - Rclone for cloud file syncing (supports Google Drive)
nodejs/12.14.1 - Node.js JavaScript runtime environment
ninja/1.10.0-py3.6 - Ninja build system
meson/0.53.1-py3.6 - Meson build system
gtk+/3.24.14 - GTK+ 3 libraries for GUI applications
fribidi/1.0.8 - Free Implementation of the Unicode Bidirectional Algorithm
atk/2.28.1 - Accessibility Toolkit
tree/1.8.0 - Tree structure of file system
gnupg/2.2.19 - GnuPG encrypt and sign data
"
rc-website-fork/content/post/2021-its-home-dirs.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2021-06-16T11:18:25-05:00""
title = ""ITS home1 directories unavailable on Rivanna""
draft = false
tags = [""rivanna"",""storage""]
categories = [""feature""]
+++
ITS has completed phase 1 of its network reconfiguration and ITS NAS storage volumes have been remounted on Rivanna. RC managed Research Standard and Research Project storage are also fully available. ITS home1 directories, including /nv/t* mounts, remain unavailable on Rivanna. Users will still be able to mount ITS home1 storage to their local workstations and transfer their data via Globus and the UVA Main DTN. We regret the inconvenience."
rc-website-fork/content/post/2024-oct-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-10-01T00:00:00-05:00""
title = ""HPC Maintenance: Oct 15, 2024""
draft = false
tags = [""rivanna"", ""afton""]
categories = [""feature""]
+++
{{< alert-green >}}The HPC cluster will be down for maintenance on Tuesday, Oct 15, 2024 beginning at 6 am.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until the cluster is returned to service.
All systems are expected to return to service by Wednesday, Oct 16 at 6 am.
IMPORTANT MAINTENANCE NOTES
Expansion of /home
To transition away from the Qumulo filesystem, we will migrate all /home directories to the GPFS filesystem and automatically increase each user‚Äôs /home directory limit to 200GB. 
Transition from Anaconda to Miniforge module
Due to the recent licensing restrictions by Anaconda  on research usage, we will be removing the licensed Anaconda distribution from our system on October 15, 2024. The anaconda module will redirect to the new miniforge/24.3.0-py3.11 module, with a reduced number of preinstalled packages in the base environment, but includes the essential Conda and Mamba package managers along with commonly used packages such as numpy, pandas, matplotlib, etc., from the conda-forge channel. 
By default, the miniforge distribution will only provide packages from the conda-forge channel. Therefore, if you require packages from channels that are covered by the Anaconda repository Terms of Service (main/anaconda, r, msys2) you may specify this in your installation command but only for environments that are restricted to educational use, i.e., instructional work in your classes.
The use of your existing environments should not be affected by this change. For instructional use you may continue to install python packages from the licensed Anaconda default channels however, any use of such environment for research purposes is a violation of the Anaconda license unless you obtained your own license. More information on preparing for this change is available here.
We understand that these changes may cause inconvenience, but these changes are mandated by the Anaconda licensing condition which we cannot control. If you have any questions or concerns, please feel free to reach out. 
Modules

The following modules will be removed during the maintenance period.

{{< table title=""replacement"" class=""table table-striped"" >}}
| Module | Removed version | Replacement |
|---|---|---|
|anaconda   | 2023.07-py3.11 | miniforge/24.3.0-py3.11 |
|anvio      | 6.2            | 8 |
|code-server| 4.16.1         | 4.92.2 |
|deeplabcut | 2.2.1.1-anipose| 3.0.0rc4 |
|deeptools  | 3.5.1          | 3.5.5 |
|maestro    | 1.3.0          | 1.5.1 |
|mamba      | 22.11.1-4      | miniforge/24.3.0-py3.11 |
|matlab     | R2020a         | R2024a |
|matlab     | R2020b         | R2024a |
|matlab     | R2021a         | R2024a |
|matlab     | R2021b         | R2024a |
|matlab     | R2022a         | R2024a |
|matlab     | R2022b         | R2024a |
{{< /table >}}"
rc-website-fork/content/post/2024-july-afton-release.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-06-17T00:00:00-05:00""
title = ""Production Release of the Afton HPC System: July 2, 2024""
url = ""/maintenance""
draft = false
tags = [""afton""]
categories = [""feature""]
+++

Our new supercomputer, ‚ÄúAfton,‚Äù is now available for general use. This represents the first major expansion of RC‚Äôs computing resources since Rivanna's last hardware refresh in 2019. Afton represents a substantial increase in the High-Performance Computing (HPC) capabilities available at UVA, more than doubling the available compute capacity. Each of the 300 compute nodes in the new system has 96 compute cores, an increase from a maximum of 48 cores per node in Rivanna. The increase in core count is augmented by a significant increase in memory per node. Each Afton node boasts a minimum of 750GB of memory, with some supporting up to 1.5TB. The large amount of memory per node allows researchers to efficiently work with the ever-expanding datasets we are seeing across diverse research disciplines.

Maintenance: July 2, 2024
{{< alert-green >}}The HPC system in the standard security zone, including Rivanna, will be down on Tuesday, July 2, 2024 beginning at 6 a.m. During the downtime RC engineers will implement final configuration changes in preparation of the full production release of the new Afton HPC system.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until the HPC systems are returned to service.
The Rivanna and Afton production systems are expected to return to service by Wednesday, July 3 at 6 a.m.
Questions: Please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m..
{{% callout %}}
What to expect after the maintenance?


New hardware: On May 28, a total of 300 compute nodes, 96 cores each, based on the AMD EPYC 9454 architecture have been added to UVA's HPC environment as the new Afton system. The new Afton hardware provides additional capacity for serial, parallel and GPU computing side-by-side with the existing Rivanna system.


Configuration: The hardware partition definitions will be reconfigured to optimize effective use of the new Afton and existing Rivanna systems. (The Weka scratch filesystem will be mounted in non-dedicated mode, which means all cores will be available. Previously 3 cores per node were dedicated to Weka.)


Access: The Rivanna and Afton systems are accessible via the existing and shared Open OnDemand, FastX and ssh access points.


Software, Code, and Job Submissions: The shared software stack and modules have been tested during the pre-release phase. In most cases users can utilize the system without any changes to their job submission scripts. In some instances users may need to update their Slurm job scripts or recompile their own code. The RC team is available to help with the transition.


Policy: A new charge rate policy will be implemented during Fall 2024 (tentative) to reflect more closely the actual hardware cost.


{{% /callout %}}
FAQ
{{% accordion-group title=""Group"" id=""faqgroup""%}}
{{% accordion-item title=""1. Is Afton replacing the older Rivanna system?"" id=""faq-1"" %}}
No, the new Afton system exists side-by-side with the existing Rivanna system. Both systems are accessible through shared login nodes, see ""How do I log in to the Afton system?"".
{{% /accordion-item %}}
{{% accordion-item title=""2. What compute capabilities does the new Afton hardware offer?"" id=""faq-2"" %}}
On May 28, a total of 300 compute nodes with 96 cores each, based on the AMD EPYC 9454 architecture, have been added to UVA‚Äôs HPC environment. The added nodes expand UVA's HPC capabilities in the following areas:


A complete hardware refresh of the parallel partition with 96-core nodes that roughly doubles its capacity (based on aggregated cpu core count).


Expanded capacity of the standard partition for single node jobs and high-throughput computing with up to 96 cores and 1.5TB of memory per node.


Addition of new nodes with NVIDIA A40 general purpose graphics processing units (GPUs) to accommodate more ML/DL computing in the gpu partition.


{{% /accordion-item %}}
{{% accordion-item title=""3. How can I get an account to access the Afton system? Can I use my Rivanna allocation?"" id=""faq-3"" %}}
The service unit allocations are shared for Rivanna and Afton. If you already have an active Rivanna allocation, no action is required. If you'd like to start using Afton or Rivanna, please follow the instructions here.
{{% callout %}}
Please note: {{% pi-eligibility %}}
{{% /callout %}}
{{% /accordion-item %}}
{{% accordion-item title=""4. How do I log in to the Afton system?"" id=""faq-4"" %}}
The login access points are shared for the Afton and Rivanna systems.


Option 1: Web access via Open OnDemand: https://ood.hpc.virginia.edu


Option 2: Remote Desktop via FastX: https://fastx.hpc.virginia.edu


Option 3: Secure Shell (ssh) session: @login.hpc.virginia.edu


See here for details. You must be a member of an active HPC allocation before you can log in.
{{% /accordion-item %}}
{{% accordion-item title=""5. Can I still use Rivanna?"" id=""faq-5"" %}}
Yes, login access points are shared for the Rivanna and Afton systems. We added new hardware feature tags that allow you to specifically request Rivanna resources for your compute jobs once logged in. 
See ""What are the changes to the hardware partitions?"" and ""What are hardware features? What are the hardware feature defaults for each partition?"".
{{% /accordion-item %}}
{{% accordion-item title=""6. What are the changes to the hardware partitions?"" id=""faq-6"" %}}
The following partition changes are taking place on July 2:

The pre-release afton partition will be removed. The nodes will be placed in other partitions. 
The nodes making up the parallel partition will be completely replaced with 200 Afton nodes. The original nodes will be placed into standard.
The largemem partition will be removed. All 750GB nodes will be placed in the standard partition. 
All RTX3090 nodes from the gpu partition will be placed in the interactive partition.

New partition configuration:
{{< table title=""partition-config"" class=""table table-striped"" >}}
| Partition | Rivanna Nodes | Afton Nodes | Use Cases | 
| --- | --- | --- | --- |
| standard | yes | yes | For jobs on a single compute node, including those with large memory requirements. |
| parallel | no| yes | For large parallel multi-node jobs. |
| gpu | yes | yes | For jobs using general purpose graphical processing units, e.g. for machine learning/deep learning. |
| interactive | yes | yes | For quick interactive sessions, code development, and instructional use. It includes a small number of lower-end GPU nodes. |
{{< /table >}}
{{% /accordion-item %}}
{{% accordion-item title=""7. What happened to the largemem, dev, and instructional partitions?"" id=""faq-7"" %}}
Nodes of the largemem partition have been moved to the standard partition. See ""What are the changes to the hardware partitions?""
The dev and instructional partitions were merged and replaced with a single interactive partition during the Afton pre-release on May 30.
{{% /accordion-item %}}
{{% accordion-item title=""8. What are hardware feature constraints? What are the default hardware feature constraints for each partition?"" id=""faq-8"" %}}
Features constraints and generic resources (GRES) allow you to request specific hardware within a given partition. Through feature constraints you can specify if a job should be scheduled on the new Afton hardware or the older Rivanna system.
Feature constraints are optional; you may submit jobs without feature constraints. If no feature constraint is specified, the Slurm scheduler will place your job on available partition hardware following a default priority order.
Note: Not all features are available in every partition. This table lists the available features for each partition, including the default if no feature is specified. 
{{< table title=""feature-constraints-and-gres"" class=""table table-striped"" >}}
| Partition | Available Features Constraints | GRES | Default Priority Order | Notes |
| --- | --- | --- | --- | --- | 
| standard | afton, rivanna | None | rivanna > afton | If not specified, the scheduler will attempt to place the job on Rivanna hardware first or Afton hardware as second alternative. |
| parallel | None | None | n/a | The entire partition is configured with new Afton nodes. No feature constraint is required.
| gpu | None | v100, a40, a6000, a100_40gb, a100_80gb | v100 > a6000 > a40 >  a100_40gb> a100_80gb  | If no GRES request is specified, the scheduler will attempt to place the job on a V100 node first and A100 80GB nodes (i.e. the BasePOD) hardware as last alternative. The A40 nodes were purchased along with the new Afton hardware. | 
| interactive | afton, rivanna | rtx2080, rtx3090 | rivanna > afton | If not specified, the scheduler will attempt to place the job on Rivanna hardware first or Afton hardware as second alternative. |
{{< /table >}}

See ""How do I use Afton for my Slurm job? Do I need to update my job scripts?"" and ""How can I use the new Afton hardware in Open OnDemand?"" for instructions on using these feature constraints in your job submission scripts or Open OnDemand.
{{% /accordion-item %}}
{{% accordion-item title=""9. How do I use Afton for my Slurm job? Do I need to update my job scripts?"" id=""faq-9"" %}}
Most users should be able to submit jobs without changing their Slurm job scripts, unless:
- invalid request due to partition changes (see ""What are the changes to the hardware partitions?"")
- *Example:* A job submitted to `largemem` will become invalid since the partition has been removed. One should submit to `standard` with `--mem=...` (up to 1462G) to specify the memory.



cost considerations (see How is compute time charged on the Rivanna and Afton systems?)

Example: Instead of running a light GPU job on an A100 in gpu, request an RTX2080 or RTX3090 in interactive via --gres=gpu.



a need for specific Rivanna vs Afton hardware for performance/reproducibility/benchmarking reasons (only relevant for standard and interactive) 

Example: To restrict a standard job to run on the new Afton hardware, provide a constraint (--constraint= or -C):
```



SBATCH -p standard
SBATCH --constraint=afton
and likewise for Rivanna hardware:
SBATCH -p standard
SBATCH --constraint=rivanna
```
{{% /accordion-item %}}
{{% accordion-item title=""10. How can I use the new Afton hardware in Open OnDemand?"" id=""faq-10"" %}}
When setting up an Interactive App session in Open Ondemand you may enter the --constraint=afton or --constraint=rivanna feature constraint in Optional: Slurm Option ( Reservation, Constraint ) field.
Available feature constraints are listed here: ""What are hardware features? What are the default hardware features for each partition?"".  
{{% /accordion-item %}}
{{% accordion-item title=""11. Do I need to recompile my code?"" id=""faq-11"" %}}
If you have already done this for the Afton pre-release testing then no. Otherwise please use the following flowchart.


Which compiler did you use to build your code?

Not Intel (e.g. GCC gcc, NVIDIA nvhpc) ‚Üí no
Intel ‚Üí continue



Do you intend to run your code on Afton hardware? (Please note the parallel partition will be completely replaced by Afton hardware.)

No ‚Üí no
Yes ‚Üí continue



Did you use the -x flag (e.g. -xavx)?

No ‚Üí no
Yes ‚Üí yes, rebuild with -march=skylake-avx512 instead of -x...



{{% /accordion-item %}}
{{% accordion-item title=""12. How is compute time charged on the Rivanna and Afton systems?"" id=""faq-12"" %}}
Starting Spring 2025, a new service unit (SU) charge rate policy will be implemented to reflect more closely the actual hardware cost. For all non-GPU jobs, the SU charge rate will be based on the amount and type of CPU cores (Intel on Rivanna, AMD on Afton) plus memory allocated. For GPU jobs (in gpu and interactive), the SU charge rate will be based on the number and type of GPU devices allocated. Detailed information about new SU charge rates can be found here. 
{{% /accordion-item %}}
{{% accordion-item title=""13. Why are there different charge rates for Rivanna, Afton, and GPU hardware?"" id=""faq-13"" %}}
Starting Fall 2024 (tentative), a new charge rate policy will be implemented to reflect more closely the actual hardware cost. For all non-GPU jobs, the charge rate will be based on the amount of CPU cores and memory allocated. For GPU jobs (in gpu and interactive), the charge rate will be based on the amount of GPU devices allocated.
Use of Afton hardware may allow jobs to complete faster but may consume more SUs overall due to a higher burn rate.
{{% /accordion-item %}}
{{% accordion-item title=""14. What is fairshare?"" id=""faq-14"" %}}
To ensure fair access to the HPC environment for all research groups, we utilize Slurm's job accounting and fairshare system. This system influences job placement priority, with a higher fairshare value typically resulting in a higher queue priority. However, the fairshare value decreases as more service units are consumed.
Crucially, fairshare values are linked to the Principal Investigator (PI) of the allocation being utilized. This connection prevents any single group from dominating the resources and maintains fairness across PI groups, especially those who have not utilized their fairshare allocation for an extended period.
Paid service units place fairshare values and job priority above those of users utilizing instructional or standard allocations. 
{{% /accordion-item %}}
{{% accordion-item title=""15. How do use of different hardware and service unit burn rates affect my fairshare?"" id=""faq-15"" %}}
The high performance new Afton hardware as well as the higher-end GPU hardware incur higher service unit (SU) burn rates. For example, allocation of 40 cores and 256GB of memory on a new Afton node consumes more service units per hour than the same cpu core and memory allocation on an older Rivanna node. Similarly, use of an NVIDIA A100 80GB GPU device incurs a higher SU charge per hour compared to a lower-end NVIDIA A6000 GPU device.
The more SUs have been consumed, the lower the fairshare value drops. This will impact the user's priority when submitting new jobs with the same allocation.
Use of Afton hardware may allow jobs to complete faster but may consume more SUs overall due to a higher burn rate. See ""How is compute time charged on the Rivanna and Afton systems?"".
{{% /accordion-item %}}
{{% accordion-item title=""16. How can I get help?"" id=""faq-16"" %}}
Please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m..
{{% /accordion-item %}}
{{% /accordion-group %}}
Afton Release Announcements
{{% accordion-group title=""Comms"" id=""commsgroup"" %}}
{{% accordion-item title=""May 30, 2024 - Afton is available in pre-release configuration"" id=""comm-3"" %}}
Effective May 30, the new Afton HPC hardware is now available in a pre-release configuration as part of the HPC environment. During this pre-release phase the number of available Afton nodes may fluctuate as the RC team completes final configurations. The full production release of the Afton cluster with stable service of all 300 nodes is planned for Tuesday, July 2. Learn more.
{{% /accordion-item %}}
{{% accordion-item title=""May 20, 2024 - Reminder: Rivanna maintenance and Afton pre-release"" id=""comm-1"" %}}
Dear Rivanna user: 
A friendly reminder that Rivanna and Research Project storage will be down for maintenance from Tuesday, May 28 at 6 a.m. through Thursday, May 30 6 a.m..  
How to prepare and what to expect during the maintenance? 
You may continue to submit jobs to Rivanna until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All Rivanna compute nodes and login nodes will be offline, including the Open OnDemand and FastX portals. Research Project storage will be unavailable. The UVA Standard Security Storage data transfer node (DTN) and Research Standard storage remain online throughout the maintenance period.  
Pre-release of the new Afton cluster after the maintenance 
All systems are expected to return to service by 6 a.m. on Thursday, May 30. The new Afton HPC hardware will become available in a pre-release configuration at that time, with the addition of 300 new compute nodes, 96 cores each, based on the AMD EPYC 9454 architecture. The new Afton hardware will provide additional capacity for serial, parallel and GPU computing side-by-side with the existing Rivanna system. During this pre-release phase the number of available Afton nodes may fluctuate as the RC team completes final configurations. The full production release of the Afton cluster with stable service of all 300 nodes is planned for Tuesday, July 2. 
A detailed description of the maintenance plan and instructions for using the new Afton resources is available on the RC website.   
If you have any questions about the Rivanna maintenance or Afton pre-release, you may contact our user services team. 
At your service, RC staff 
Research Computing
E hpc-support@virginia.edu
P 434.243.1107
University of Virginia
P.O. Box 400231
Charlottesville 22902
{{% /accordion-item %}}
{{% accordion-item title=""May 14, 2024 - Rivanna maintenance and Afton pre-release"" id=""comm-2"" %}}
Dear Rivanna user: 
A reminder that Rivanna and Research Project storage will be down for maintenance from Tuesday, May 28 at 6 a.m. through Thursday, May 30 6 a.m..  
How to prepare and what to expect during the maintenance? 
You may continue to submit jobs to Rivanna until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All Rivanna compute nodes and login nodes will be offline, including the Open OnDemand and FastX portals. Research Project storage will be unavailable. The UVA Standard Security Storage data transfer node (DTN) and Research Standard storage remain online throughout the maintenance period.  
Pre-release of the new Afton cluster after the maintenance 
All systems are expected to return to service by 6 a.m. on Thursday, May 30. The new Afton HPC hardware will become available in a pre-release configuration at that time, with the addition of 300 new compute nodes, 96 cores each, based on the AMD EPYC 9454 architecture. The new Afton hardware will provide additional capacity for serial, parallel and GPU computing side-by-side with the existing Rivanna system. During this pre-release phase the number of available Afton nodes may fluctuate as the RC team completes final configurations. The full production release of the Afton cluster with stable service of all 300 nodes is planned for Tuesday, July 2. 
A detailed description of the maintenance plan and instructions for using the new Afton resources is available on the RC website.   
If you have any questions about the Rivanna maintenance or Afton pre-release, you may contact our user services team. 
At your service, RC staff 
Research Computing
E hpc-support@virginia.edu
P 434.243.1107
University of Virginia
P.O. Box 400231
Charlottesville 22902
{{% /accordion-item %}}
{{% /accordion-group %}}"
rc-website-fork/content/post/2024-may-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-05-14T00:00:00-05:00""
title = ""Rivanna Maintenance: May 28, 2024""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Tuesday, May 28, 2024 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. While drive mapping and project storage will be unavailable, other storage will remain accessible through Globus.
All systems are expected to return to service by Thursday, May 30 at 6 a.m.
IMPORTANT MAINTENANCE NOTES
Hardware and partition changes

afton: We are pleased to announce the addition of 300 nodes, 96 cores each, based on the AMD EPYC 9454 architecture. For the time being, these new nodes are placed into their own partition afton, in which each user can request up to 6000 cores in aggregate. There is some chance that 100 nodes will be available initially, in which case the core limit will be adjusted to 2000. (The full release is scheduled for July, where the nodes will be integrated into the other existing partitions. More information to follow.)
interactive: The dev and instructional partitions will be combined into interactive, which will have a maximum time limit of 12 hours and a maximum core limit of 24. The SU charge rate will be 1. Two RTX 2080Ti nodes (10 GPU devices each) will be added from the gpu partition.
gpu: Four NVIDIA A40 (48 GB) nodes, 8 GPUs each, will be added to the gpu partition. The RTX 2080Ti nodes (2 x 10 GPU) will be moved into the interactive partition.

Please contact us if you have questions about the new partitions or if you experience issues with the new hardware.
System upgrades

Operating System: Rocky 8.7 ‚Üí 8.9. There is no need to rebuild your own code. (Intel users may need to do so but for a different reason; see below.)
Slurm: 21.08.8 ‚Üí 23.02.7. Job-related commands remain the same.
NVIDIA driver: 535.104.12 ‚Üí 550.54.14.

Modules

Attn Intel users: With the addition of the Afton nodes based on the AMD EPYC architecture, we have reorganized and rebuilt all modules under the intel toolchain. If you used -x (e.g. -xavx) to build your own code, you should rebuild it with -march=skylake-avx512 for it to run on both AMD and Intel hardware. Below we list all the modules that are upgraded or moved to a different toolchain. (Intel modules not listed can be loaded the same way as before.) The toolchain needs to be loaded before the module, e.g. module load gcc gmp. The gompi toolchain is equivalent to gcc openmpi. There is no impact on the existing modules built with GCC.

{{< table title=""intel"" class=""table table-striped"" >}}
| Module | New version | Toolchain|
|---|---|---|
|abinit/8.10.3, 9.8.3| 10.0.3 | intel |
|chemps2/1.8.12 | (removed)  | - | 
|cesm/2.1.3     | 2.2.2  | intel | 
|cp2k/2023.1    | 2024.1 | intel |
|gmp/6.2.0      | -      | gcc |
|kim-api/2.3.0  | -      | gcc |
|mpfr/4.2.0     | -      | gcc |
|ncview/2.1.7   | 2.1.10 | intel |
|neuron/8.2.2   | -      | gompi |
|p3dfft/2.7.9   | -      | gompi |
|pcmsolver/1.3.0| -      | gompi |
|pcre2/10.42    | -      | gcc |
|raxml/8.2.12   | -      | gompi |
|readosm/1.1.0a | -      | gcc |
|scotch/7.0.3   | -      | gompi |
|shapelib/1.5.0 | -      | gcc |
|viennarna/2.5.1| -      | gcc |
|voro++/0.4.6   | -      | gcc |
{{< /table >}}


Attn NVHPC users: The compiler toolchain nvhpc and nvompi will be upgraded to 24.1 and 24.1_4.1.6, respectively. The previous versions 23.7 and 23.7_4.1.4 will be removed. All modules under this toolchain will be rebuilt. There should be no need to rebuild your own code.


The following modules will be removed from Rivanna during the maintenance period.


{{< table title=""replacement"" class=""table table-striped"" >}}
| Module | Removed version | Replacement |
|---|---|---|
|aocc      |4.1.0   | 4.2.0 |
|cellranger|6.0.1, 7.2.0| 8.0.0 |
|fiji      |1.53t   | 2.14.0 |
|fsl       |6.0.5   | 6.0.7.6|
|gatk      |4.2.3.0 | 4.3.0.0 |
|gpumd     |3.7     | 3.9.1   |
|picard    |2.23.4  | 2.27.5 |
{{< /table >}}"
rc-website-fork/content/post/2025-RC-event.md,"+++
title = ""UVA Research Computing Exhibition: April 23, 2025""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
date = ""2025-03-18T10:57:24-05:00""
tags = [""rc""]
categories = [""feature""]
draft = false
+++
Join us for our annual UVA Research Computing Exhibition: 
Wednesday, April 23, 10am - 2pm 
in the 
Newcomb Hall Ballroom
Drop by anytime!
The UVA Research Computing Exhibition showcases the incredible research happening across UVA. Whether you are interested in sharing your research or discovering what others are working on, this exhibition offers the opportunity to: 


See and discuss research posters 


Hear Lightning Talks from UVA faculty 


Learn about resources for research 


Explore diverse research methods 


Find potential collaborators 


Connect with the broader research community 


Enjoy free food and beverages 


Exhibition Agenda
Attendees can drop by anytime. 
8:30-9:30am ‚Äî Participant Sign-In and Poster Setup 
Participants check in, set up their posters, and get ready to share their research. 
10am ‚Äî Group A Poster Session 
Meet participants, discuss their research, exchange ideas, and explore potential collaborations. 
11am ‚Äî Welcome and Lightning Talks 
Hear short, engaging talks from UVA faculty about their research and techniques. 
12:30pm ‚Äî Group B Poster Session 
Meet participants, discuss their research, exchange ideas, and explore potential collaborations. 
1:30pm ‚Äî Awards Ceremony 
Celebrate outstanding research and recognize the winners of the poster competition. 
2:00pm ‚Äî Event Wrap-Up"
rc-website-fork/content/post/2021-june-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2021-06-03T00:00:00-05:00""
title = ""Rivanna Maintenance: June 15, 2021""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Tuesday, June 15, 2021, beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.
Rivanna is expected to return to service by 6 a.m. on Wednesday, June 16.
IMPORTANT MAINTENANCE NOTES
Globus
Some Globus users may need to rebuild their shared connections after the maintenance period has ended. Users who require assistance with this task are invited to join us for office hours between 10 a.m. and 12 p.m. on Thursday, June 17. The Zoom link is available here.
Modules


The following software modules will be removed from Rivanna during the maintenance period:

singularity/2.6.1, 3.5.2, 3.6.1 - replaced by 3.7.1 (details)
anaconda/5.2.0-py3.6, 2019.10-py3.7 - replaced by 2020.11-py3.8 (details)
julia/0.6.0, 1.1.0, 1.3.1 - replaced by 1.5.3, 1.6.0
vscode/1.50.1, 1.53.2 - replaced by Code Server on Open OnDemand
cellprofiler/2.2.0 - replaced by 3.1.8
meme/4.10.2 - replaced by 5.1.0, 5.3.3
nextflow/0.26.0.4715 - replaced by 20.10.0
phono3py/1.19.1 - 1.22.3 included in phonopy/2.9.3
qiime2/2020.6 - replaced by 2020.8
salmon/0.11.2 - replaced by 1.2.1
samtools/0.1.20, 1.4.1, 1.7 - replaced by 1.9, 1.10, 1.12
lftp/4.8.4 - replaced by 4.9.2



The following upgrades will take place during the maintenance period. Upgrades to default versions of applications:

JupyterLab backed by Anaconda 2020.11 with Python 3.8.8
python/3.7.7 -> 3.8.8
pytorch/1.5.1 -> 1.8.1
tensorflow/2.1.0-py37 -> 2.4.1
cellranger-atac/1.2.0 -> 2.0.0
lammps/20200615 -> 20201029
meme/5.1.0 -> 5.3.3
samtools/1.10 -> 1.12

For anaconda/python-dependent modules, please see below.


New modules:

rapidsai/0.19 - NVIDIA data science libraries
pipenv/2020.11.15 - automatically create and manage a virtualenv



Changes to Singularity modules
All Singularity modules are now under 3.7.1. If you hardcoded older Singularity versions, e.g.
bash
module load singularity/2.6.1 # or 3.5.2, 3.6.1
please change it to
bash
module load singularity
The containers themselves have not been modified. We have not encountered backwards compatibility issues; please let us know if you do.
If you need to know the Singularity version that was used to create a container, run:
bash
singularity inspect /path/to/container
Changes to Anaconda/Python modules
Many of our Anaconda/Python modules have been upgraded to Python 3.8.8 in light of security vulnerabilities. If you need assistance with migrating python packages from one version to another, please visit here. Note that conda environments created by one anaconda module version can be activated by another.
The following table shows the detailed version changes for all affected modules. Please note:
- The Python version is upgraded to 3.8.8 unless otherwise stated.
- The new version replaces the current default. If the new version is -, that means the module version remains the same.
- In some cases, the module load command is different. Check module spider <module>/<version> if you cannot load a module.
- If you must use a particular module with an older Python version, please create your own conda environment.
| Module | Version | Python| NEW version  | NEW Python | Removed versions |
|---|---|---|---|---|---|
|anaconda     | 2020.11-py3.8 | 3.8.5 | -            |  | 5.2.0-py3.6, 2019.10-py3.7 |
| ase         | 3.20.1        | 3.7.9 | -            |  | 3.17.0-py3 |
| bart        | 2.0           | 3.7.8 | -            |  | 1.0.1 |
|bioconda     | py3.8         | 3.8.5 | -            |  | py3.6, py3.7 |
|biopython    | 1.70-py2      | 2.7.17| 1.78-py3     |  | - |
|cudatoolkit  | 10.1.168-py3.6| 3.6.10| 11.0.3-py3.8 |  | 10.1.168-py3.6 |
| cutadapt    | 2.5           | 3.7.4 | 3.4          |  | 1.16, 2.5 |
| deeptools   | 3.3.1         | 3.6.6 | 3.5.1        |  | 2.5.3, 3.3.1 |
|gcloud-sdk   | 196.0.0       | 2.7.17| 334.0.0      |  | 196.0.0 |
|gdc-client   | 1.5.0         | 3.7.7 | 1.6.0        | 3.7.10 | 1.3.0, 1.5.0 |
|globus_cli   | 1.12.0        | 3.7.7 | 2.0.0        |  | 1.11.0, 1.12.0 |
|google-api   | 1.9.6         | 2.7.17| 2.0.2        |  | 1.9.6 |
|gpustat      | 0.6.0         | 3.7.7 | -            |  | - |
| hexrd       | 0.6.12        | 2.7.17| 0.8.4        |  | jb-0.3.x, jb-0.5.6, 0.6.12 |
| hoomd       | 2.9.4         | 3.7.7 | 2.9.6        |  | 2.9.4 |
| idr         | 2.0.2-py3     | 3.6.6 | -            |  | - |
| iqtree      | 2.0.3         | 3.7.9 | 2.1.2        |  | 2.0.3 |
| intervene   | 0.6.4         | 3.7.3 | 0.6.5        |  | 0.6.4 |
| kallisto    | 0.44.0        | 3.7.3 | 0.46.2       |  | 0.44.0 |
| marge       | 1.0           | 3.6.7 | -            |  | - |
| mayavi      | 4.5.0         | 2.7.15| 4.7.2        |  | 4.5.0 |
| meson       | 0.53.1        | 3.7.7 | 0.57.1-py3.8 |  | 0.53.1, 0.54.3 |
| mrtrix3     | rc3           | 2.7.17| 3.0.2        |  | rc3 |
|mrtrix3tissue| 5.2.8         | 2.7.17| 5.2.9        |  | 5.2.8 |
| mysqlclient | 1.4.6-py3.7   | 3.7.4 | 2.0.3-py3.8  |  | 1.4.4-py3.6, 1.4.6-py3.7 |
| ninja       | 1.10.0        | 3.7.7 | 1.10.2-py3.8 |  | 1.10.0 |
|openslide-python| 1.1.1-py3  | 3.6.6 | 1.1.2-py3    |  | 1.1.1-py3 |
| phonopy     | 2.6.1         | 3.7.7 | 2.9.3        |  | 2.6.1 |
| pybind11    | 2.2.4-py3.7   | 3.7.4 | 2.6.2-py3.8  |  | 2.2.4-py3, 2.2.4-py3.7 |
| reframe     | 2.17          | 3.6.7 | -            |  | - |
|snakemake    | 5.2.2         | 3.6.6 | 6.0.5        |  | 5.2.2 |
|snap-stanford| 5.0.9-py3.6   | 3.6.6 | 5.0.9-py3.8  |  | 4.1, 5.0.9-py3.6, snap-stanford-py/4.1 |
|spades       | 3.15.0        | 3.7.4 | 3.15.2       |  | 3.15.0 |
|thirdorder   | 1.1.1-py3     | 3.6.6 | -            |  | - |
|trimgalore   | 0.6.4         | 3.6.8 | -            |  | 0.4.5 |
| wasp        | 0.3.4         | 3.7.7 | -            |  | - |"
rc-website-fork/content/post/2020-june-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-06-02T00:00:00-05:00""
title = ""Rivanna Maintenance: June 17, 2020""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Wednesday, June 17, beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service.

Rivanna is expected to return to service later in the day.


The following software modules will be removed from Rivanna during the maintenance period:

intel/16.0 & toolchains - replaced by intel/18.0, 20.0
imsl/7.1.0 - expired license



The following upgrades will take place during the maintenance period:

intel/20.0 & toolchains - default 18.0
goolfc/8.3.0_3.1.6_10.2.89 - GCC 8 toolchain with OpenMPI, CUDA support, and numerical libraries
cuda/10.2.89
R/3.6.3 (default), 3.5.3, 3.4.4; removed 3.6.[0-2], 3.5.1, 3.4.3  (For more details, see ""R Updates"" and ""New Libraries"")
matlab/R2020a
hdf/4.2.14 - added shared libraries but disabled Fortran (if you need Fortran please contact us)
netcdf/4.7.3
sagemath/9.0 - removed 8.0
salmon/1.2.1 - removed 1.0.0 and 1.1.0 due to segfault bug
snap-stanford/5.0.9-py3.6



New tools:

R/4.0.0 under intel/18.0
python/3.7.7 under intel/20.0 - Intel Distribution for Python
gpustat/0.6.0 - GPU monitoring tool
orca/4.2.1 - quantum chemistry package
alamode/1.1.0
anvio/6.2
atat/3.36


"
rc-website-fork/content/post/2023-october-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-09-21T00:00:00-05:00""
title = ""Rivanna Maintenance: October 3, 2023""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on Tuesday, October 3, 2023 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. All systems are expected to return to service by 6 a.m. on Wednesday, October 4.
IMPORTANT MAINTENANCE NOTES
New largemem nodes
RC engineers will be adding 36 nodes, each with 40 cores and 750 GB total memory, to the largemem partition on Rivanna. Jobs that need more memory than 9 GB per core should be submitted to the largemem partition rather than the standard partition. Some examples are given below.
I need 4 cores and 100 GB memory. Since this amounts to 25 GB memory per core, the job should be submitted to largemem.
```bash
SBATCH -p largemem
SBATCH -c 4
SBATCH --mem=100G
```
I need 10 cores and 50 GB memory. Since this amounts to 5 GB memory per core, the job should be submitted to standard without specific memory requests. By default 9 GB per core will be allocated.
```bash
SBATCH -p standard
SBATCH -c 10
```
I am not sure how much memory I need. First submit the job to the standard partition without specific memory requests. If the job runs out of memory, resubmit to the largemem partition. To check the memory usage of a completed job, you may either run the seff command or add to your Slurm script:
```bash
SBATCH --mail-user=your_computing_id@virginia.edu
SBATCH --mail-type=end
```
and check the report in your email.
NVIDIA driver upgrade and modules
The NVIDIA driver will be upgraded to version 535.104.12 (CUDA 12.2). The default CUDA module version will remain at 11.4.2. New modules will be added:

cuda/12.2.2
cudnn/8.9.4.25
pytorch/2.0.1
tensorflow/2.13.0

The corresponding Jupyter kernels for PyTorch and TensorFlow will be provided as well.
AlphaFold versions 2.1.2, 2.2.2, and their corresponding database will be removed. The 2.3 database will be migrated off of the current /project storage and the ALPHAFOLD_DATA_PATH environment variable will be updated accordingly.
QGIS (Open OnDemand) will be upgraded to 3.28.10.
Old scratch permanently retired on October 17
A reminder that the /oldscratch (i.e. /gpfs/gpfs0/scratch) filesystem will be permanently retired on October 17 and all the data it contains will be deleted. A sample script for users who wish to transfer files to the new /scratch system can be found here.
If you have any questions or concerns about the maintenance period, you may contact us here."
rc-website-fork/content/post/2022-women-in-hpc-202301.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-10-26T10:18:25-05:00""
title = ""RC's Data Analytics Center (DAC): Now Serving UVA's Research Community""
draft = false
tags = [""rivanna"",""whpc"",""va-whpc"",""hpc"",""computational-biophysics""]
categories = [""feature""]
+++
The Data Analytics Center is UVA's new hub for the management and analysis of your large research data. Need help with your computational research? DAC staff specialize in key domain areas such as image processing, text analysis, bioinformatics, computational chemistry and physics, neural networks, and more. And because the DAC team is located within Research Computing, they can assist in getting your workflows running on the University‚Äôs high-performance cluster or secure data system. They can answer your basic computational questions or, through funded engagements, be embedded in your projects.
Big data doesn‚Äôt have to be a big deal. Learn how DAC can assist with your computational research ‚Äì schedule an initial consultation with one of their data analysts by submitting a consultation request. Information on DAC's full range of services is available on the DAC webpage."
rc-website-fork/content/post/2023-03-matlab-seminar.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-03-06T10:18:25-05:00""
title = ""MATLAB Seminar March 16 3-4PM: Medical Image Analysis and AI Workflows""
draft = false
tags = [""matlab"",""image analysis"",""artificial intelligence""]
categories = [""feature""]
+++
Medical images come from multiple sources such as MRI, CT, X-ray, ultrasound, and PET. Analysis of these images requires a comprehensive environment for data access, visualization, processing, and algorithm development. The main challenge is to extract clinically meaningful information based on advanced techniques such as Artificial Intelligence (AI). To achieve this, one needs to clean, segment, register, and label a large collection of images. For the AI analysis, there are many more challenges such as iteratively adjusting AI models or learning parameters. MATLAB provides tools such as Medical Imaging Toolbox and Deep Learning Toolbox and algorithms for end-to-end medical image analysis and AI workflow. 
When: March 16, 2023, 3-4PM EDT


Highlights
In this presentation, you will learn how to:

Easily import and visualize 2D images and 3D volumes interactively
Segment, register, and label medical image and volume data
Import and edit pre-trained networks for processing
Design, train, and test AI and deep learning models

About the Presenter
Dr. Elvira Osuna-Highley
Principal Education Application Engineer
MathWorks
eosunahi@mathworks.com
Elvira Osuna-Highley, Ph.D. is part of a global team supporting academic research and teaching at MathWorks. Before joining MathWorks, she was on the faculty of the Computational Biology Department at Carnegie Mellon University. She holds a doctorate in Biomedical Engineering from Carnegie Mellon University, where her research involved applying machine learning techniques to fluorescence microscope images."
rc-website-fork/content/post/2018-fall-workshops.md,"+++
images = [""""]
author = ""RC Staff""
description = """"
date = ""2018-09-02T15:18:25-05:00""
title = ""Fall 2018 Workshops""
draft = true
tags = [""education"",""workshops"",""feature""]
categories = [""feature""]
+++

School of Medicine Research Computing provides training opportunities covering a variety of data analysis, basic programming 
and computational topics. 
Workshops break roughly into the three main areas relevant to computationally-intensive research: code, data, and computing.
All of the classes are taught by RC experts and are freely available to UVa faculty, staff and students.

R / R package development
Python
Matlab
Biomedical Image Processing
Bioinformatics on HPC
Data manipulation
Data visualization
Databases
Machine Learning
Cloud Computing
Containers
Rivanna (HPC)
Ivy (Secure Computing)
"
rc-website-fork/content/post/2024-september-17-open-house.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-08-19T00:00:00-05:00""
title = ""Research Computing Open House 2024""
url = ""/maintenance""
draft = false
tags = [""afton"", ""rivanna"", ""hpc""]
categories = [""feature""]
+++
UPDATE: The Research Computing Open House was held on a blustery, rainy day, but the spirits of the staff and attendees were not dampened.  Turnout was above expectations despite the wet weather.  Attendees enjoyed the buffet and their interactions with RC staff.  
The winners of the random-drawing prizes were
* Maria Luana Morais, SOM
* Matt Panzer, SEAS
* Artun Duransoy, SEAS
{{< rawhtml >}}




{{< /rawhtml >}}

Please join us at the Research Computing Open House on Tuesday, September 17, 2024, from 2-5 p.m. in the Commonwealth Room at Newcomb Hall. We are excited to host the UVA community to share updates on a new supercomputer and services that we are offering. 
Why Attend?

 Talk with research computing experts and staff.  Have your questions answered. 
 Receive the latest information on research computing at UVA, including 

 Afton, UVA‚Äôs new supercomputer, and other high-performance computing resources 
 Secure compute & storage solutions 
 Research collaboration and grant support services 
 the Data Analytics Center: dataset management and analytics including AI 
 the Digital Technology Core: use of wearables, smartwatches, smartphones or IoT devices in your research 
 Upcoming RC workshops 

 Learn about our student workers program 
 Enjoy light refreshments 

How to Attend
You are welcome to drop in anytime during the event and stay as long as you would like.   
If you are planning to attend, please RSVP for the Open House in advance. Those who RSVP and attend will be entered into a drawing for for one of three prizes totaling $150! 


Time & Location:

 Tuesday, September 17 
 2-5 p.m. 
 Commonwealth Room at Newcomb Hall 


Event Schedule:



 


Time
Activity


2:05-2:20
  Opening Remarks: 
        Presented by Joshua Baller, Associate Vice President for Research Computing 
    


2:20-5:00
  Networking and Information Tables: 
         Explore information tables and interactive demos. Research Computing staff will be available to answer questions. 
    


Have a Question?
Contact Research Computing at hpc-support@virginia.edu."
rc-website-fork/content/post/2022-01-women-in-hpc.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-01-02T10:18:25-05:00""
title = ""Virginia Women in HPC - Research Highlights Event, Jan. 25""
slug = ""2022/07/va-whpc-event""
draft = false
tags = [""rivanna"",""drug-discovery""]
categories = [""feature""]
+++
We are proud to announce the founding of Virginia's first Women in High-Performance Computing (VA-WHPC) program. Join us for our first event of 2022: Female research leaders of the Commonwealth sharing and discussing how HPC has facilitated their scientific research and professional careers.
Topic: How does HPC help with your scientific research -- Faculty perspectives, Part II 
When: Jan 25, 2022 01:00 PM, Eastern Time (US and Canada) 


Our speakers:


Anne Brown (VT) is an Assistant Professor of Biochemistry, Science Informatics Consultant and Health Analytics Coordinator at Virginia Tech. Her research interests include utilizing computational modeling to answer biological questions and aid in drug discovery and the application of computational molecular modeling to elucidate the relationship between structure, function, and dynamics of biomolecules. 


Jenna Cann (GMU) is a postdoctoral fellow at NASA Goddard Space Flight Center. Jenna received a PhD in Physics from George Mason University. Her research focuses on studying black holes in dwarf and low metallicity galaxies, in an effort to constrain the origins of supermassive black holes that can be up to billions of times the mass of our Sun. To do this, they use both theoretical modeling with the Cloudy spectral simulation code and infrared and X-ray observations to determine the most effective ways to find these elusive objects. Jenna currently serves as a co-officer in the NASA Goddard Association for Postdoctoral and Early Career Scholars (NGAPS+) and was a co-founder of the GMU Physics and Astronomy department's diversity, equity, inclusion, and accessibility (DEIA) organization, SPECTRUM.


Alexis Edwards (VCU) is an Associate Professor of Psychiatry at Virginia Commonwealth University. Her research focuses on understanding the etiology of substance use disorders, suicidal behavior, and internalizing problems, including how these outcomes are related to one another. 


Virginia WHPC is committed to increasing diversity and inclusion by promoting and encouraging the participation of women in high-performance computing and related fields."
rc-website-fork/content/post/2023-july-scratch-transfer.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-07-18T00:00:00-05:00""
title = ""New Scratch System on Rivanna: July 18, 2023""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}During the July 18th maintenance, RC engineers installed a new /scratch file storage system on Rivanna. We have created sample scripts and instructions to help you transfer your files from the previous file system to the new one.(Expand the link below for details.){{< /alert-green >}}
The previous scratch filesystem, now called /oldscratch, will be permanently retired on October 17, 2023 and all the data it contains will be deleted.
Users should clean up their /oldscratch directory in preparation, to minimize the load.  A sample script is posted below.
Modified queue limits have been implemented to provide maximum read/write performance of the new /scratch filesystem. Please refer to our updated documentation and adjust your job scripts accordingly.
Transfer Instructions
Example script to copy files
{{< pull-code file=""/static/scripts/demo-copy-scratch.slurm"" lang=""bash"" >}}
The script will also be available through the Open OnDemand Job Composer:

Go to Open OnDemand Job Composer
Click: New Job -> From Template
Select demo-copy-scratch
In the right panel, click ""Create New Job""
This will take you to the ""Jobs"" page. In the ""Submit Script"" panel at the bottom right, click ""Open Editor""
Enter your own allocation. You may edit the script as needed. Click ""Save"" when done.
Going back to the ""Jobs"" page, select demo-copy-scratch and click the green ""Submit"" button.

As we expect a high volume of data migration, please refrain from doing so directly on the login nodes but instead submit it as a job via the provided Slurm script as described above.
The new scratch is subject to the same 10 TB quota and 90-day purge policy. There is no restriction on the number of files. A friendly reminder that scratch is intended as a temporary work directory, not long-term storage space. It is not backed up and old files need to be purged periodically for system stability. RC offers a number of low-cost storage options to researchers. For more information, visit our storage page."
rc-website-fork/content/post/2022-december-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2022-12-06T00:00:00-05:00""
title = ""Rivanna Maintenance: December 19, 2022""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
{{< alert-green >}}Rivanna will be down for maintenance on December 19, 2022 beginning at 6 a.m.{{< /alert-green >}}
You may continue to submit jobs until the maintenance period begins, but if the system determines your job will not have time to finish, it will not start until Rivanna is returned to service. Users will not be able to access the Globus data transfer node during the maintenance period.
All systems are expected to return to service by 6 a.m. on Tuesday, December 20. Globus users may need to rebuild their shared collections.
IMPORTANT MAINTENANCE NOTES
Two new toolchains are now available: goolf/11.2.0_4.1.4 and intel/2022.11. The former consists of GCC 11.2.0, OpenMPI 4.1.4, and math libraries. The latter is Intel oneAPI that consists of the Intel compilers, MKL, and MPI. (Note that there is no need to load intelmpi from this version onwards.) The default versions have not been upgraded this time. Users with their own compiled codes are encouraged to build them with the new toolchains and report any issues.
Modules

The following software modules will be removed from Rivanna during the maintenance period:

| Module | Removed version | Replacement |
|---|---|---|
|blender     |2.78c | 3.2.1 |
|cellranger      |4.0.0 | 5.0.0, 6.0.1, 7.0.1 |
|cellranger-atac |1.2.0 | 2.0.0 |
|diamond     |0.9.13| 2.0.14|
|drmaa       |1.1.2 | 1.1.3 |
|fsl         |6.0.0 | 6.0.5 |
|go          |1.8.1, 1.13.4 | 1.18.4 |
|gparallel   |20170822 | parallel/20200322 |
|gurobi      |9.0.1, 9.1.1 | 9.5.0 |
|htslib      |1.4.1 | 1.7, 1.9 |
|igvtools    |2.8.9 | 2.12.0 |
|librmath    |3.6.2 | 3.6.3 |
|macs2       |2.1.2 | 2.2.7.1 |
|micromamba  |0.7.14| 0.24.0 |
|nextflow    |0.26.0.4715 | 20.10.0 |
|salmon      |1.2.1 | 1.5.1 |
|seqoutbias  |1.2.0 | 1.3.1 |
|sratoolkit  |2.8.0, 2.9.1 | 2.10.5 |
|trimmomatic |0.36  | 0.39 |
|vcftools    |0.1.15| 0.1.16 |
*Archived containers can be found in /share/resources/containers/singularity/archive.


Upgrades:

eccodes/2.26.0 - under gcc/9.2.0 openmpi/3.1.6
hic-pro/3.1.0
kraken2/2.1.2 - from kraken/0.10.5; note module name change
openfoam/v2206 - under goolf/9.2.0_3.1.6
R/4.2.1 - under goolf/9.2.0_3.1.6
ruby/3.1.2 - under gcc/9.2.0

Default version changes:
- alphafold/2.2.0 ‚Üí 2.3.0
- cellranger/5.0.0 ‚Üí 7.0.1
- cmake/3.16.5 ‚Üí 3.23.3
- deeplabcut/2.2 ‚Üí 2.2.1.1-anipose
- matlab/R2022a ‚Üí R2022b
- pytorch/1.10.0 ‚Üí 1.12.0 (includes PyTorch Geometric)
- qiime2/2020.8 ‚Üí 2022.2
- tensorflow/2.7.0 ‚Üí 2.10.0


New modules:

clang/10.0.1
crossftp/1.99.9
gawk/5.1.1
isaacgym/1.0.preview4
nibabies/22.1.3
rnaeditor/1.1a


"
rc-website-fork/content/post/2024-aug-maintenance.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-07-31T00:00:00-05:00""
title = ""HPC Maintenance: Aug 13, 2024""
draft = false
tags = [""afton""]
categories = [""feature""]
+++
{{< alert-green >}}The HPC cluster will be partially down for maintenance on Tuesday, Aug 13, 2024 beginning at 6 a.m.{{< /alert-green >}}
The following nodes will be unavailable during this period:
- all of parallel
- afton nodes in standard and interactive
- A40 GPU nodes in gpu
The nodes are expected to return to service by Wednesday, Aug 14 at 6 a.m.
There is no impact on other nodes or services. Jobs on other nodes will continue to run."
rc-website-fork/content/education/courses.md,"+++
draft = false
date = ""2022-02-02T10:55:28-05:00""
title = ""Courses""
description = """"
author = ""RC Staff""
categories = [""education""]
tags = [""R"",""cloud"",""DSI"",""CS"",""SDS"",""UVA"",""BIMS""]
+++
In addition to providing  free, in-person workshop training, 
UVA Research Computing staff teach for-credit courses. Below is a selection of courses that members of our group have taught, 
co-taught or provided guest lectures:
BIMS 8382: Introduction to Biomedical Data Science
Spring 2017, Spring 2018
This course introduces methods, tools, and software for reproducibly managing, manipulating, analyzing, and visualizing large-scale biomedical data. Specifically, the course introduces the R statistical computing environment and packages for manipulating and visualizing high-dimensional data, covers strategies for reproducible research, and culminates with analysis of data from a real RNA-seq experiment using R and Bioconductor packages.

CS 6501: Distributed & Cloud Computing
Spring 2017, Spring 2018
This graduate course introduces a basic grounding in designing and implementing distributed and cloud systems. It aims to acquaint students with principles and technologies of server clusters, virtualized datacenters, Grids/P2P, Internet clouds, social networks, Internet of Things (IoT), and applications. Students will have the opportunity to gain hands-on experience on public cloud such as Amazon EC2. Selected scientific applications will also be used as case studies to gain hands-on experiences.

CS 4740: Cloud Computing
Spring 2018, Fall 2020, Fall 2021
Investigates the architectural foundations of the various cloud platforms, as well as examining both current cloud computing platforms and modern cloud research. Student assignments utilize the major cloud platforms.

DS 3002: Data Science Systems
Spring 2021
Exposes students to contemporary platforms, tools, and pipelines for data analysis through a series of steadily escalating use cases. 
The course will begin with simple local database construction and evolve to cloud based providers such as AWS or Google Cloud. 
Attention is given to data lakes and NoSQL as appropriate.

Data Science Bootcamp: Computing, Storage & Data Analysis in the Cloud
Summer 2017
This 1.5-day course introduces MSDS students to the basics of cloud computing in AWS, and the independent management of code, data, and computing resources in a research environment. Particular concern is given to the concepts of programmable, reusable, scalable resources in the AWS cloud, through hands-on labs in EC2 and S3."
rc-website-fork/content/education/rivanna-instructional.md,"+++
draft = false
date = ""2023-09-13T10:55:28-05:00""
title = ""Instructional Use of High Performance Computing""
description = """"
author = ""RC Staff""
categories = [""education"",""workshops""]
tags = [""Rivanna"",""instructional"",""courses""]
+++
Instructors can request instructional allocations on Rivanna and Afton for classes and extended workshops.  These allocations are time-limited and generally allow access to a restricted set of nodes and only one special Slurm partition, but are otherwise equivalent to any allocation.

Resource Availability
Hardware and Partition
Instructional allocations may use interactive partition.  The instructional allocation is 100,000 SUs for the semester during which the course is conducted.  For workshops, the allocation will persist during the workshop and for two days afterwards. RC offers several low-cost storage options to researchers, including 10TB of Research Standard storage for each eligible PI at no charge. Instructors are encouraged to utilize this 10TB of storage for both research and teaching activities. For more detailed descriptions of our storage options, visit https://www.rc.virginia.edu/userinfo/storage/. 
Software & Storage Environment
Research Computing's primary focus is supporting the direct research mission of the University. Instructional allocations are provided in recognition of the many areas where the educational and research missions of the University meet, and in recognition that there is value in providing UVA's diverse communities with experience in an HPC environment. However, staff time is a highly limited resource and instructional use of RC systems as a largely 'as-is' service with standardized software and storage environments. We are unable to provide customization of the environment for specific classes.
Interface
For most classes, we recommend the Open OnDemand interface if it suits the expected usage.  This does not require knowledge of Unix and greatly reduces the training burden.  The Open OnDemand interface requires only Netbadge credentials and can be accessed without a VPN from off Grounds.
If Open OnDemand is not adequate, the other recommended interface is FastX Web.  This is a remote desktop application and requires the students to be able to navigate a Unix desktop system.  Access from off Grounds via FastX requires a VPN connection.
FastX connects only to a frontend.  We significantly restrict the time, memory, and cores available to frontend jobs.  If students are running anything but very short jobs, the Open OnDemand applications should be utilized.  These access the compute nodes and are far less limited.  Open OnDemand provides a remote desktop on compute nodes as well as direct access to JupyterLab, the Matlab Desktop, and Rstudio Server.

How to Submit a Request
Instructors planning to use HPC should fill out the form.  You will need to create the Grouper (requires VPN connection) allocation group.  We suggest a generic group name related to the course rubric, e.g. cs5014.  Once the group is created, the instructor or a designated group administrator will need to add the student IDs.  The instructor should empty the membership of the group after the class or workshop has terminated. Instructors will need to submit an instructional allocation renewal request at the start of each semester. 

Using the Allocation
Prior to the first class use, instructors should test the allocation and the software applications required during class.  Please do not wait until multiple students are attempting to use it.  
Passwords
Students, particularly undergraduates, frequently experience password difficulties.  Rivanna and Afton use the Eservices password to authenticate, but few students know this password.  Instructors are urged to communicate to students that they should go to the ITS password page at least several hours in advance and change their Netbadge password before using the system.  Changing the Netbadge password will sync the Eservices password with it.
Partition and Reservations
The allocation will have access to the interactive partition.  Students can enter this with the -p or --partition options to Slurm.
```
SBATCH -p interactive
or
SBATCH --partition=interactive
```
If students use the Open OnDemand interface, they will enter this into the appropriate textbox when starting their interactive job application.
Instructors are urged to request reservations for their classes.  The reservation will be created to coincide with the class meeting time.  Students must add an option --reservation=your-reservation in order to access the reserved resources.  Students may still use the instructional partition outside the reservation, but those jobs will wait like any other queued job. Outside the dedicated reservation window jobs should be submitted without the --reservation flag for immediate queueing; otherwise the job will be pending until the next reservation window opens.
For batch jobs, the reservation can be entered on the command line
sbatch --reservation=your-reservation myscript.slurm
or it can be added to the job script preamble
```
SBATCH --reservation=your-reservation
```
For Open OnDemand interactive applications, it should be entered as an additional Slurm option.
Training
Research Computing staff are available to come to a class session to provide training to the students.  This can be done in-person, when possible, or virtually through Zoom."
rc-website-fork/content/education/workshops.md,"+++
author = ""RC Staff""
description = """"
title = ""Workshops""
date = ""2023-02-23T10:55:28-05:00""
draft = false
tags = [""R"",""Python"",""Matlab"",""Shiny"",""HPC"",""Rivanna"",""Ivy"",""image processing"",""bioinformatics"",""containers"",""programming""]
categories = [""education"", ""workshops""]
images = [""""]
+++
UVA Research Computing provides training opportunities covering a variety of data analysis, basic programming and computational topics. All of the classes listed below are taught by experts and are freely available to UVa faculty, staff and students.
New to High-Performance Computing?
We offer orientation sessions to introduce you to the Afton & Rivanna HPC systems on Wednesdays (appointment required).


Wednesdays 3:00-4:00pm 
Sign up for an ""Intro to HPC"" 
session


Upcoming Workshops
{{% upcoming-workshops-smart %}}
Research Computing is partnering with the Research Library and the Health Sciences Library to deliver workshops covering a variety of research computing topics.  
All Upcoming Workshops from UVA Library Research Data Services
All Upcoming Workshops from UVA Health Sciences Library

Workshop Material
Course material and exercises are available through a companion site. Feel free to browse classes, tutorials and workshop material and learn at your own pace.

https://learning.rc.virginia.edu


New Tutorials
Specifically, check out these new tutorials!
{{% new-tutorials %}}
Previous Workshops

Advanced Computing in the Cloud
Advanced Data Manipulation with R
Advanced Data Visualization with R
Analyzing 16s RNA Amplicons
Building Shiny Web Applications in R
Conditionals and Iteration in R
Data Analysis & Visualization with Python
Data Visualization with Matlab
Databases and How to Use Them
Docker Containers for Scientific Research
How to Work With Databases
Image Processing with Matlab
Introduction to Cloud Computing
Introduction to Docker Containers
Introduction to Git and GitHub
Introduction to High Performance Computing (Rivanna)
Introduction to Highly-Sensitive Data Analysis (Ivy)
Introduction to Matlab
Introduction to Python
Introduction to R
Introduction to Rivanna
Introduction to the Command Line
Introduction to UVA Research Computing Resources
Machine Learning in the Cloud
Machine Learning with MatLab
Next Generation Sequence Alignment
Optimizing R
Parallel Computing with Matlab
R For Beginners
R Package Development Tools
Writing in Functions in R

{{% callout %}}
Do you need a specific workshop and have a group of people to attend? Let us know.{{% /callout %}}"
rc-website-fork/content/about/employment.md,"+++
type = ""about""
description = ""Join our team. Do good work.""
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""employment"",
]
tags = [
  ""rc"",
  ""staff"",
]
date = ""2018-12-06T15:25:19-05:00""
draft = true
title = ""Employment""
+++
We currently have no openings. Please check again in the future!"
rc-website-fork/content/about/mission.md,"+++
tags = [
  ""rc"",
  ""uvarc"",
  ""hpc"",
  ""data"",
  ""storage"",
  ""medicine"",
  ""research"",
]
draft = false
date = ""2022-08-06T14:53:45-05:00""
title = ""Mission""
about = true
author = ""UVARC Staff""
images = [
  ""/2016/10/image.jpg"",
]
description = ""The mission of UVA Research Computing""
categories = [
  ""about"",
]
+++

Research Computing empowers UVA researchers to achieve more with cutting-edge computational resources. Our support team strives to create innovative solutions for researchers who need 
help solving complex optimization, parallelization, workflow, and data analysis issues. We build and maintain the University's best computing platforms while educating the next 
generation of researchers on the power of advanced computing.
"
rc-website-fork/content/about/students.md,"+++
tags = [
  ""rc"",
  ""uvarc"",
  ""hpc"",
]
draft = false
date = ""2022-08-06T14:53:45-05:00""
title = ""Research Computing Student Workers""
about = true
author = ""UVARC Staff""
description = ""The mission of UVA Research Computing""
categories = [
  ""about"",
]
+++

The Research Computing Student Worker Program is dedicated to supporting RC staff and advancing computational research at UVA. Through this program, student workers will undertake short-term projects, relieving RC staff of certain responsibilities and allowing them to devote more time to scaling support for complex, research-domain endeavors. This initiative not only benefits RC staff but also provides students with valuable exposure to high-performance computing (HPC) and scientific computing early in their academic journey.


 Student Manager:  Gladys K. Andino, PhD,  Strategic Services and Education Manager  
For any questions, please email rc-studentjobs@virginia.edu.

Check out our Student Workers!
{{< rawhtml >}}
  {{< current-students >}}
{{< /rawhtml >}}
Interested in Becoming a Student Worker?

  {{< button 
     button-url=""https://uva.wd1.myworkdayjobs.com/UVAStudentJobs/job/Charlottesville-VA/Research-Computing-Student-Worker--Student-Wage-_R0066673"" 
     button-class=""primary"" 
     button-text=""Apply for the Student Worker Position"" 
  >}}
"
rc-website-fork/content/about/staff.md,"+++
tags = [
  ""rc"",
  ""staff"",
  ""uvarc""
]
draft = true
date = ""2022-07-17T15:25:10-05:00""
title = ""Staff""
type = ""people""
description = """"
author = ""RC Staff""
images = [
  """"
]
categories = [
  ""staff"",
  ""about""
]
+++



Rick Downs
Director of Research Computing
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Andrew Bell, PhD
Communications and Business Operations
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Ravi Kiran R. Chamakuri, PMP
Full Stack Developer
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Michele Co, PhD
HPC Systems Specialist
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Christina Gancayco
Research Computing Associate
    {{< expertise subjects=""matlab,image-processing,data-science,python"" >}}
  


Katherine Holcomb, PhD
Computational Research Consultant
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Ed Hall, PhD
Computational Research Consultant
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Jacalyn Huband, PhD
Computational Research Consultant
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Byoung-Do Kim, PhD
Special Projects
    {{< expertise subjects=""hpc,parallel-computing,storage,infiniband,data"" >}}
  


Neal Magee, PhD
HPC/Cloud Solution Architect
    {{< expertise subjects=""globus,cloud,containers,databases,infrastructure,python"" >}}
  


Adam Munro
HPC Systems Specialist
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


VP Nagraj
Research Computing Associate
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Gisoo Park
HPC Systems Specialist
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


David Parsley
HPC Systems Specialist
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Alex Ptak
HPC Systems Specialist
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Karsten Siller, PhD
Computational Research Consultant
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  


Alden Stradling, PhD
HPC Systems Specialist
    {{< expertise subjects=""hpc,rivanna,parallel-computing,storage"" >}}
  
"
rc-website-fork/content/project/ed-triage.md,"+++
title = ""Predicting ER Triage Levels with Machine Learning""
description = """"
author = ""RC Staff""
images = ""/images/projects/uva-er.jpg""
categories = [""projects""]
tags = [""clinical-research"",""machine-learning""]
draft = false
projecttype = [""clinical-research"",""machine-learning""]
date = ""2018-05-03T14:33:50-05:00""
+++
Before patients are admitted to the emergency room, they are assigned a triage level based on the severity of their health problems. This is accomplished using the Emergency Severity Index (ESI), an emergency department triage algorithm that classifies patient cases into five different levels of urgency. Researchers are interested in using machine learning to develop a model to predict patient triage level. This model would not only analyze the typical vital signs that are used in the ESI, but also demographic data and patients‚Äô history of health.
Demographic and health data have been collected. RC is helping to prepare and normalize the data for use in a machine learning model. RC is currently developing preliminary machine learning models for predicting triage level.
PI: Thomas Hartka (Department of Emergency Medicine)"
rc-website-fork/content/project/basic-science.md,"+++
author = ""RC Staff""
description = """"
date = ""2018-04-23T17:17:35-04:00""
title = ""Basic Science Projects""
draft = true
tags = [""collaborations""]
categories = [""projects""]
images = """"
+++

School of Medicine Research Computing is engaged in multiple collaborative projects in support of basic science research. Below is a list of some recent collaborations in this area.

Microbiome Analysis of Hospital Sink Drains
Sink drains are notoriously characterized as reservoirs of pathogens causing nosocomial transmissions in hospitals worldwide. Outbreaks where sinks have been implicated as source of antibiotic resistant bacteria have upsurged over the last few years. To understand transmission dynamics University of Virginia School of Medicine has established a unique ""Sink Lab"" for this research. This one-of-the kind laboratory establishes UVa as worldwide frontrunners in investigating sink related antibiotic resistant bacteria and how they spread. RC is working with the UVa Sink Lab for genomic analysis of the sink biomass. 
RC is contributing to:

Comparative genomic analysis of gram-negative bacterial isolates:
    The analysis aims at tracking the mobile genetic element blaKPC gene, which encodes for Klebsiella pneumoniae carbapenemase (KPC) enzyme that confers resistance to all beta lactam agents including penicillins, cephalosporins, monobactams and carbapenems. As a part of this project, whole-genome shotgun sequencing data for about 1500 bacterial isolates will be analyzed to assess the risk of acquisition of Carbapenemase producing Enterobacteriaceae from exposure to contaminated waste water premise plumbing.   
Metagenomic analysis: 
    This project, under a contract for the Center for Disease Control and Prevention (CDC), aims at understanding the temporal dynamics of hospital sink microbiome. Taxonomic and functional analysis of whole metagenomic shotgun sequencing data from longitudinal sampling will shed light on the transfer and sustenance of high-risk antibiotic resistance genes in the hospital environments.

PI: Amy Mathers (Infectious Diseases & UVa Sink Lab)

Genomic Locus Overlap Enrichment Analysis (LOLAweb)
The past few years have seen an explosion of interest in understanding the role of regulatory DNA. This interest has driven large-scale production of functional genomics data resources and analytical methods. One popular analysis is to test for enrichment of overlaps between a query set of genomic regions and a database of region sets. In this way, annotations from external data sources can be easily connected to new genomic data.
SOM Research Computing is working with faculty in the UVA Center for Public Health Genomics to implement LOLAweb, an online tool for performing genomic locus overlap annotations and analyses. This project, written in the statistical programming language R, allows users to specify region set data in BED format for automated enrichment analysis. LOLAweb provides interactive plots and annotated data based on specific reference genomes and region databases. 
http://lolaweb.databio.org/
Manuscript under review
PI: Nathan Sheffield (Center for Public Health Genomics)

PHACTR1 and Smooth Muscle Cell Behavior
Coronary artery disease (CAD) is the major cause of morbidity and mortality worldwide. Recent genome wide association studies (GWAS) have revealed more than 50 genomic loci that are associated with increased risk for CAD. However, the pathological mechanisms for the majority of the GWAS loci leading to increased susceptibility to this complex disorder are still unclear. RC is working with Redouane Aherrahrou (CPHG) who aims to study the impact of the CAD-associated genetic factors on the cellular and molecular SMC phenotypes. Support for this project has included preparation of scripts for programmatic data analyses, data visualization, statistical modeling, and assistance with use of the Rivanna high-performance computing cluster.
Preliminary results were presented as a poster at the 2016 International Vascular Biology Meeting.
PI: Redouane Aherrarou (Center for Public Health Genomics)

Functional Connectome Fingerprinting
Functional magnetic resonance imaging (fMRI) can be used to assess functional activity in the brain and connectivity between different regions of interest (ROIs), and a functional connectome is a map of the interactions between ROIs. Previous research has shown that a functional connectome contains enough unique characteristics, not unlike a fingerprint, that it can be used for accurate identification of an individual subject from a large group. RC is working with the UVA Functional Neuroradiology Lab to perform this fingerprinting analysis for a wide variety of populations and to develop innovative ways to visualize the results.
PI: Jason Druzgal (Radiology and Medical Imaging)

Sonomicrometry Signal Classification
Researchers are using sonomicrometry to study the biomechanics of the human brain. While at times the signals collected do not require any preprocessing, more frequently they do require some denoising or are too noisy to analyze. Currently, researchers are manually categorizing the quality of thousands of these sonomicrometry signals and preprocessing them individually. RC is helping researchers develop a machine learning model to classify the signals and to determine the necessary preprocessing steps.
Preliminary sonomicrometry data have been collected, and RC is working to classify, prepare, and normalize the data for use in a machine learning model. RC is currently developing preliminary models to classify the data by signal quality and preprocess automation techniques that will later be applied to noisy signals.
PI: Matthew Panzer (Center for Applied Biomechanics)

epihet
RC is working with researchers in the Center for Public Health Genomics to write an R package to calculate Relative Proportion of Sites with Intermediate Methylation (RPIM) scores, which represent the epigenetic heterogeneity in a bisulfite sequencing sample.
https://github.com/databio/epihet
PI: Nathan Sheffield (Center for Public Health Genomics)

Transcription factor-chromatin Binding Dynamics
Two important measures of the in vivo interaction of transcription factors with chromatin are the search time and the residence time. The former refers to the time it takes a factor to find its binding location, while the latter is the time the factor physically attaches to the chromatin. By quantifying the interaction dynamics of transcription factors, researchers hope to understand the role of these factors in basic cellular processes such as transcription and gene regulation. The RC team is working with collaborators from UVA and the NIH to understand the dynamics of the Gal4 protein in yeast. The project involves quantitatively analyzing ChIP-qPCR data, writing and running non-linear regression and statistical routines in Mathematica, and developing numerical simulations to determine the error bounds on the kinetic parameters. 
PI: Stefan Bekiranov (Biochemistry and Molecular Genetics)"
rc-website-fork/content/project/nicu-bpd.md,"+++
title = ""Bradycardia and Desaturation Events in Infants""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/infant-rn.jpg""
categories = [""projects""]
tags = [
    ""NICU"",
]
draft = false
projecttype = [""clinical-research"",""machine-learning""]
publications = [{authors = ""Fairchild KD, Nagraj VP, Sullivan BA, Moorman JR, Lake DE"", title = ""Oxygen desaturations in the early neonatal period predict development of bronchopulmonary dysplasia"", journal = ""Pediatric Research"", year = ""2018"", doi = ""10.1038/s41390-018-0223-5""}]
+++
Episodes of bradycardia and oxygen desaturation (BD) are common among preterm very low birthweight (VLBW) infants and their association with adverse outcomes such as bronchopulmonary dysplasia (BPD) is unclear. A better understanding of this relationship could lead to improved clinical interventions.
RC is helping neonatologists describe BD events in a large single-NICU VLBW cohort and test the hypothesis that measures of BD in the neonatal period add to clinical variables to predict BPD or death and other adverse outcomes. RC has implemented statistical modeling and machine learning techniques to assess the primary outcome of BPD in the context of a combination of clinical characteristics (like birthweight and gestational age) and bedside monitor features.
PI: Karen Fairchild (Department of Pediatrics‚ÄìNeonatology) & Doug Lake (Center for Advanced Medical Analytics)"
rc-website-fork/content/project/covid-saliva.md,"+++
title = ""COVID Saliva Testing""
description = """"
author = ""RC Staff""
date = ""2022-03-01T14:33:50-05:00""
images = ""/images/projects/covid-saliva.jpg""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""covid-19"",
  ""data"",
  ""health""
]
draft = false
projecttype = [""basic-science"", ""tools"", ""clinical-research"", ""containers""]
+++
In cooperation with the UVA Saliva Testing Lab, the UVA Health System, and the Virginia Department of Health, the ""Be SAFE"" saliva
testing program was launched in late 2020. Now a retired project, Be SAFE used saliva samples to detect the COVID-19 virus through a diagnostic PCR test.
Research Computing provided computational, storage, and data integration expertise to this project."
rc-website-fork/content/project/bartweb.md,"+++
title = ""BART Web""
author = ""RC Staff""
categories = [""projects""]
tags = [""tools"",""containers"",""hpc"",""scripts"",""architecture"",""code"",""cphg"",""docker""]
images = ""/images/projects/bart-web.png""
description = """"
date = ""2021-10-22T17:18:27-04:00""
draft = false
audio = true
projecttype = [""basic-science"", ""tools"", ""containers""]
publications = [{authors = ""Wenjing Ma, Zhenjia Wang, Yifan Zhang, Neal E Magee, Yayi Feng, Ruoyao Shi, Yang Chen, Chongzhi Zang"", title = ""BARTweb: a web server for transcriptional regulator association analysis"", journal = ""NAR Genomics and Bioinformatics"", year = ""2021"", volume = 3, issue = 2, month = ""June"", doi = ""10.1093/nargab/lqab022""}]
+++
BART (Binding Analysis for Regulation of Transcription) Web
Working with researchers in the Zang Lab in the Center for Public Health Genomics 
(CPHG), RC helped launch BARTweb,
an interactive web-based tool for users to analyze their Genelist or ChIP-seq datasets. BARTweb is a containerized
Flask front-end (written in Python) that ingests files and submits them to a more robust Python-based genomics pipeline 
running on Rivanna, UVA's high performance computing cluster (HPC). This architecture -- of a public web application that 
uses a supercomputer to process data -- is a new model for UVA, and one that eases the learning curve for researchers who 
may not have access to an HPC system or the expertise to run a BART pipeline in the command-line.
http://bartweb.org/
PI: Chongzhi Zang (Center for Public Health Genomics)"
rc-website-fork/content/project/bii-covid.md,"+++
title = ""COVID-19 Surveillance Dashboard""
description = """"
date = ""2020-07-23T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/covid-dashboard.png""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""covid-19"",
  ""web""
]
draft = false
projecttype = [""hpc-computing"", ""basic-science"", ""tools""]
+++
The Biocomplexity Institute at the University of Virginia has been at the forefront of epidemiological modeling to track the COVID-19 pandemic and has developed a suite of COVID-19 epidemic response resources including a series of dashboards to better help the public and the government better understand the pandemic. This is a static view of the Institute‚Äôs interactive COVID-19 Surveillance Dashboard, which provides a visualization of COVID-19 cases, recoveries, and deaths across the globe. In an effort to support the planning and response efforts for the recent Coronavirus pandemic, researchers prepared this visualization tool that provides a unique way of examining data curated by different data sources.
https://nssac.bii.virginia.edu/covid-19/dashboard/
"
rc-website-fork/content/project/refgenie.md,"+++
title = ""Refgenie: A Reference Genome Resource Manager""
description = """"
date = ""2020-02-23T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/refgenie-project.png""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""containers"",
  ""docker"",
  ""cphg"",
  ""python""
]
draft = false
projecttype = [""basic-science"", ""tools"", ""containers""]
publications = [{authors = ""Michal Stolarczyk, Vincent P. Reuter, Jason P. Smith, Neal E. Magee, Nathan C. Sheffield"", title = ""Refgenie: a reference genome resource manager"", journal = ""GigaScience"", year = ""2020"", doi = ""10.1093/gigascience/giz149""}]
+++
Reference genome assemblies are essential for high-throughput sequencing analysis projects. Typically, genome assemblies are stored on disk alongside related resources; e.g., many sequence aligners require the assembly to be indexed. The resulting indexes are broadly applicable for downstream analysis, so it makes sense to share them. However, there is no simple tool to do this.
Refgenie is a reference genome assembly asset manager. Refgenie makes it easier to organize, retrieve, and share genome analysis resources. In addition to genome indexes, refgenie can manage any files related to reference genomes, including sequences and annotation files. Refgenie includes a command line interface and a server application that provides a RESTful API, so it is useful for both tool development and analysis.
RC staff supported this project through its design phase, underlying infrastructure and final deployment of a Refgenie server within containers, which are attached to reference data in high performance storage.
http://refgenie.databio.org/
PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/age-mvc.md,"+++
title = ""Predicting Injury Severity""
description = """"
date = ""2020-10-05T09:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/age-mvc.jpg""
categories = [""projects""]
tags = [
  ""r""
]
draft = false
projecttype = [""basic-science""]
publications = [{authors = ""Hartka, T., Gancayco, C., McMurry, T., Robson, M., Weaver, A."", title = ""Accuracy of algorithms to predict injury severity in older adults for trauma triage"", journal = ""Traffic Injury Prevention"", year = ""2019"", doi = ""https://doi.org/10.1080/15389588.2019.1688795""}]
+++
Previous research has shown that older adults are more susceptible to severe injury than their younger counterparts after being involved in a motor vehicle collision. Dr. Hartka was interested in determining whether there are age-related differences in the accuracy of severe injury prediction following a motor vehicle collision. Using R, Research Computing developed age-specific logistic regression models and assessed their accuracy, and generated unique graphs and animations to visualize the data more effectively. 
PI: Thomas Hartka"
rc-website-fork/content/project/dest.md,"+++
title = ""Drosophila Evolution through Space and Time 2.0""
description = """"
author = ""RC Staff""
date = ""2025-03-11""
images = ""/images/projects/dest.jpg""
caption = ""Artwork by Roberto Torres, CC BY""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""data"",
  ""hpc"",
  ""parallel-computing""
]
draft = false
projecttype = [""hpc-computing"", ""tools"", ""basic-science"", ""dac""]
publications = [{authors = ""Martin Kapun, Joaquin C B Nunez, Mar√≠a Bogaerts-M√°rquez, Jes√∫s Murga-Moreno, Margot Paris, Joseph Outten, ‚Ä¶, Alan O Bergland"", title = ""Drosophila Evolution over Space and Time (DEST): A New Population Genomics Resource"", journal = ""Molecular Biology and Evolution"", volume = ""38"", issue = ""12"", month=""December"", year = ""2021"", pages = ""5782‚Äì5805"", doi = ""10.1093/molbev/msab259""}]
+++
Evolutionary biologists use population-based DNA sequencing to gain insight into the nature of adaptation, genetic diversity, and organismal form and function.  When collecting DNA data, scientists are often sample limited because of the logistical challenges of collecting DNA from wild individuals across large portions of a species range.  This can be mitigated when groups of scientists work together to create data and then share it with the larger community.  The Bergland Lab has been a central participant in developing and maintaining DEST (‚ÄúDrosophila Evolution through Space and Time‚Äù), a large (~10TB) repository of Drosophila melanogaster population genomic data which has been processed and standardized.  The DEST dataset is a unique, spatially resolved, genomic time-series dataset for one of the premier model organisms in genetics. 
UVA‚Äôs Research Computing has been the primary host for the DEST dataset and bioinformatics pipeline since 2020.  Users access data through a combination of an http-passthrough, Globus, and a website.  The website has been viewed nearly 5,000 times by over 2,500 unique visitors since its launch in 2020 and has been used by members of the broader research community in dozens of published research projects.
The Bergland Lab is working on a new version of the DEST dataset (DEST 2.0) that includes genomic data for over 50,000 flies from 500 population-based samples collected at ~100 localities throughout the world, with many localities sampled through time for upwards of a decade.  Research Computing‚Äôs Data Analytics Center supported this work by debugging and streamlining one of the main parallel processes in the bioinformatics pipeline to efficiently use UVA HPC. 
PI: Alan Bergland, PhD (Department of Biology)"
rc-website-fork/content/project/ncaa.md,"+++
title = ""In-Silico NCAA-Containing Peptide Design""
description = """"
author = ""RC Staff""
date = ""2025-04-04""
images = ""/images/projects/ncaa.png""
categories = [""projects""]
tags = [
  ""data"",
  ""hpc"",
  ""parallel-computing"",
  ""drug-discovery"",
  ""data-science""
]
draft = false
projecttype = [""hpc-computing"", ""dac"", ""basic-science"", ""clinical-research"", ""engineering"", ""data-science""]
+++
Bacteria are an important type of human pathogen that can cause life-threatening infections. Increasingly, these microorganisms can survive the effects of antibiotics previously used to kill them. As bacteria become resistant to multiple kinds of antibiotics, the diseases they cause become ever more difficult to cure. Accordingly, infections caused by ‚Äòmultidrug-resistant‚Äô (MDR) pathogens are associated with frequent treatment failures, high hospitalization costs, and substantial mortality. New therapeutics are needed to treat infections caused by MDR bacteria. Towards developing these critical countermeasures, our group has discovered a unique peptide that efficiently kills many of the most challenging antibiotic-resistant pathogens and also demonstrates therapeutic efficacy in pre-clinical animal models of bacterial infection. Interested in investigating the effect of replacing the canonical-amino acids by non-canonical amino acids (NCAA) to increase the efficacy of the peptide, a DAC team member  has created a computational strategy to perform the screening of multiple-peptide positions using a NCAA peptide library and the high-performance computing capabilities of Research Computing. With good agreement between computational predictions and bench-top experiments, our collaboration with the DAC led to the following key points: 
* Determination of NCAA-containing preferred peptides with better predicted binding (under experimental testing) 
* Construction of the first structural model of the peptide (both canonical and non-canonical variants) with the potential bacterial target
* Structure-function insights in search of optimized antimicrobial peptides towards better therapeutics (by utilizing NCAAs)
These investigations have set the stage for our continued collaboration with the DAC team member in this area including multiple state and federal grants that we will be targeting in 2025.
PIs: Matthew A Crawford, PhD (Department of Medicine, Division of Infectious Diseases & International Health) and Molly A Hughes, PhD (Department of Medicine, Division of Infectious Diseases & International Health)"
rc-website-fork/content/project/transcription-factor.md,"+++
title = ""Transcription factor-chromatin Binding Dynamics""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/chromatin-unwinding.jpg""
categories = [""projects""]
tags = [""basic-science""]
draft = false
projecttype = [""basic-science""]
+++
Two important measures of the in vivo interaction of transcription factors with chromatin are the search time and the residence time. The former refers to the time it takes a factor to find its binding location, while the latter is the time the factor physically attaches to the chromatin. By quantifying the interaction dynamics of transcription factors, researchers hope to understand the role of these factors in basic cellular processes such as transcription and gene regulation. The RC team is working with collaborators from UVA and the NIH to understand the dynamics of the Gal4 protein in yeast. The project involves quantitatively analyzing ChIP-qPCR data, writing and running non-linear regression and statistical routines in Mathematica, and developing numerical simulations to determine the error bounds on the kinetic parameters. 
PI: Stefan Bekiranov (Biochemistry and Molecular Genetics)"
rc-website-fork/content/project/sink-microbiome.md,"+++
title = ""Microbiome Analysis of Hospital Sink Drains""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/klebsiella_pneumoniae.jpg""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""sink-lab""
]
draft = false
projecttype = [""basic-science""]
+++
Sink drains are notoriously characterized as reservoirs of pathogens causing nosocomial transmissions in hospitals worldwide. Outbreaks where sinks have been implicated as source of antibiotic resistant bacteria have upsurged over the last few years. To understand transmission dynamics University of Virginia School of Medicine has established a unique ""Sink Lab"" for this research. This one-of-the kind laboratory establishes UVa as worldwide frontrunners in investigating sink related antibiotic resistant bacteria and how they spread. RC is working with the UVa Sink Lab for genomic analysis of the sink biomass. 
RC is contributing to:

Comparative genomic analysis of gram-negative bacterial isolates:
    The analysis aims at tracking the mobile genetic element blaKPC gene, which encodes for Klebsiella pneumoniae carbapenemase (KPC) enzyme that confers resistance to all beta lactam agents including penicillins, cephalosporins, monobactams and carbapenems. As a part of this project, whole-genome shotgun sequencing data for about 1500 bacterial isolates will be analyzed to assess the risk of acquisition of Carbapenemase producing Enterobacteriaceae from exposure to contaminated waste water premise plumbing.   
Metagenomic analysis: 
    This project, under a contract for the Center for Disease Control and Prevention (CDC), aims at understanding the temporal dynamics of hospital sink microbiome. Taxonomic and functional analysis of whole metagenomic shotgun sequencing data from longitudinal sampling will shed light on the transfer and sustenance of high-risk antibiotic resistance genes in the hospital environments.

PI: Amy Mathers (Infectious Diseases & UVa Sink Lab)"
rc-website-fork/content/project/brodie-biology.md,"+++
title = ""Tracking Bug Movements""
description = """"
date = ""2019-06-13T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/bug-tracking.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""biology"",
  ""rivanna"",
  ""parallel-computing""
]
draft = false
projecttype = [""hpc-computing""]
+++
Ed Hall worked with the Brodie Lab in the Biology department, to set up a workflow to analyze videos of bug tracking experiments on the Rivanna Linux cluster. They wanted to use the community Matlab software (idTracker) for beetle movement tracking. Their two goals were to shorten the software runtime and to automate the process. There was a large backlog of videos to go through. Ed installed the idTracker software on Rivanna and modified the code to parallelize the bug tracking process. He wrote and documented shell scripts to automate their workflow on the cluster.
PI: Edmund Brodie, PhD (Department of Biology)"
rc-website-fork/content/project/clinical-research.md,"+++
title = ""Clinical Research Projects""
draft = true
tags = [""collaborations""]
categories = [""projects""]
images = """"
author = ""RC Staff""
description = """"
date = ""2018-05-03T14:33:50-05:00""
+++

Bringing expertise in data analysis and large scale computation, School of Medicine Research Computing is supporting clinical research at UVa. Several recent collaborations are listed below.

Bradycardia and oxygen desaturation events in VLBW infants
Episodes of bradycardia and oxygen desaturation (BD) are common among preterm very low birthweight (VLBW) infants and their association with adverse outcomes such as bronchopulmonary dysplasia (BPD) is unclear. A better understanding of this relationship could lead to improved clinical interventions.
RC is helping neonatologists describe BD events in a large single-NICU VLBW cohort and test the hypothesis that measures of BD in the neonatal period add to clinical variables to predict BPD or death and other adverse outcomes. RC has implemented statistical modeling and machine learning techniques to assess the primary outcome of BPD in the context of a combination of clinical characteristics (like birthweight and gestational age) and bedside monitor features.
Manuscript under review
Karen Fairchild (Department of Pediatrics‚ÄìNeonatology) & Doug Lake (Center for Advanced Medical Analytics)

Predicting Triage Level in the Emergency Department with Machine Learning
Before patients are admitted to the emergency room, they are assigned a triage level based on the severity of their health problems. This is accomplished using the Emergency Severity Index (ESI), an emergency department triage algorithm that classifies patient cases into five different levels of urgency. Researchers are interested in using machine learning to develop a model to predict patient triage level. This model would not only analyze the typical vital signs that are used in the ESI, but also demographic data and patients‚Äô history of health.
Demographic and health data have been collected. RC is helping to prepare and normalize the data for use in a machine learning model. RC is currently developing preliminary machine learning models for predicting triage level.
PI: Thomas Hartka (Department of Emergency Medicine)

Customized Secure Computing Environment for Surgical Research
RC is working with Dr. Eric Schneider to create a secure computing environment for the research of the Healthcare Surgical Outcome team. Data from this project will contain HIPAA identifiers, as well as Medicare information, and requires more security and control of data ingress/egress than projects previously hosted on the Ivy platform. After successful implementation of this project, RC will create a similar computing environment for DoD blast and traumatic brain injury data collected by Dr. Schneider before he joined UVA.
PI: Eric Schneider (Department of Surgery)

Heart Rate Ranges in Premature Neonates Using High Resolution Physiologic Data
There are limited evidence-based published heart rate ranges for premature neonates. However, knowing heart rate reference ranges in the premature neonatal population can be beneficial for bedside assessment in the Neonatal Intensive Care Unit (NICU).
RC is collaborating with clinical researchers in the Department of Pediatrics to establish baseline ranges for heart rate data in premature infants. These results are summarized from more than two billion data points collected via bedside monitoring in the NICU. RC staff has contributed data analysis and visualization expertise to aggregate the data, generate interactive heatmaps and produce tables of these ranges by gestational age.
Manuscript under review
PI: Corrie Alonzo (Department of Pediatrics‚ÄìNeonatology) & Mike Spaeder (Department of Pediatrics)"
rc-website-fork/content/project/smooth-muscle-cells.md,"+++
title = ""PHACTR1 and Smooth Muscle Cell Behavior""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/smooth-muscle-cells.jpg""
categories = [""projects""]
tags = [
  ""basic-science"",
  ""r"",
]
draft = false
projecttype = [""basic-science""]
+++
Coronary artery disease (CAD) is the major cause of morbidity and mortality worldwide. Recent genome wide association studies (GWAS) have revealed more than 50 genomic loci that are associated with increased risk for CAD. However, the pathological mechanisms for the majority of the GWAS loci leading to increased susceptibility to this complex disorder are still unclear. RC is working with Redouane Aherrahrou (CPHG) who aims to study the impact of the CAD-associated genetic factors on the cellular and molecular SMC phenotypes. Support for this project has included preparation of scripts for programmatic data analyses, data visualization, statistical modeling, and assistance with use of the Rivanna high-performance computing cluster.
Preliminary results were presented as a poster at the 2016 International Vascular Biology Meeting.
PI: Redouane Aherrarou (Center for Public Health Genomics)"
rc-website-fork/content/project/functional-connectome.md,"+++
title = ""Functional Connectome Fingerprinting""
date = ""2018-05-03T14:33:50-05:00""
description = """"
author = ""RC Staff""
images = ""/images/projects/functional-connectome-fingerprint.png""
categories = [""projects""]
tags = [""basic-science""]
draft = false
projecttype = [""basic-science""]
+++
Functional magnetic resonance imaging (fMRI) can be used to assess functional activity in the brain and connectivity between different regions of interest (ROIs), and a functional connectome is a map of the interactions between ROIs. Previous research has shown that a functional connectome contains enough unique characteristics, not unlike a fingerprint, that it can be used for accurate identification of an individual subject from a large group. RC is working with the UVA Functional Neuroradiology Lab to perform this fingerprinting analysis for a wide variety of populations and to develop innovative ways to visualize the results.
PI: Jason Druzgal (Radiology and Medical Imaging)"
rc-website-fork/content/project/zhigilei-materialsci.md,"+++
title = ""Pulse Laser Irradiation and Surface Morphology""
description = ""Blah blah blah here we are with a placeholder.""
date = ""2019-06-24T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/zhigilei.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""materials-science"",
  ""simulations"",
  ""rivanna""
]
draft = false
projecttype = [""hpc-computing"",""engineering""]
+++
Dr. Zhigilei and his team are using Rivanna to perform large-scale atomistic simulations aimed at revealing fundamental processes responsible for the modification of surface morphology and microstructure of metal targets treated by short pulse laser irradiation. The simulations are performed with a highly-optimized parallel computer code capable of reproducing collective dynamics in systems consisting of up to billions of atoms.  As a result, the simulations naturally account for the complexity of the material response to the rapid laser energy deposition and provide clear visual representations, or ‚Äúatomic movies,‚Äù of laser-induced dynamic processes. The mechanistic insights revealed in the simulations have an immediate impact on the development of the theoretical understanding of laser-induced processes and assist in optimization of laser processing parameters in current applications based on laser surface modification and nanoparticle generation in laser ablation.
PI: Leonid V. Zhigilei, PhD (Department of Materials Science & Engineering)"
rc-website-fork/content/project/periasamy-flim.md,"+++
title = ""Fluorescence Lifetime Imaging Microscopy in Cancer Research""
description = """"
date = ""2020-07-22T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/periasamy-flim.png""
categories = [""projects""]
tags = [
  ""image analysis"",
  ""biology"",
  ""cancer""
]
draft = false
projecttype = [""basic-science"", ""image-analysis"", ""tools""]
publications = [{authors = ""Wallrabe H, Svindrych Z, Alam SR, Siller KH, Wang T, Kashatus D, Hu S, Periasamy A"", title = ""Segmented cell analyses to measure redox states of autofluorescent NAD(P)H, FAD & Trp in cancer cells by FLIM"", journal = ""Scientific Reports"", year = ""2018"", doi = ""https://doi.org/10.1038/s41598-017-18634-x""}]
+++
Multiphoton FLIM microscopy offers many opportunities to investigate processes in live cells, tissue and animal model systems. For redox measurements, FLIM data is mostly published by cell mean values and intensity-based redox ratios. Our method is based entirely on FLIM parameters generated by 3-detector time domain microscopy capturing autofluorescent signals of NAD(P)H, FAD and novel FLIM-FRET application of Tryptophan and NAD(P)H-a2%/FAD-a1% redox ratio. Furthermore, image data is analyzed in segmented cells thresholded by 2‚Äâ√ó‚Äâ2 pixel Regions of Interest (ROIs) to separate mitochondrial oxidative phosphorylation from cytosolic glycolysis in a prostate cancer cell line. Hundreds of data points allow demonstration of heterogeneity in response to intervention, identity of cell responders to treatment, creating thereby different subpopulations. Histograms and bar charts visualize differences between cells, analyzing whole cell versus mitochondrial morphology data, all based on discrete ROIs. This assay method allows to detect subtle differences in cellular and tissue responses, suggesting an advancement over means-based analyses.
RC staff supported this project with development of custom image analysis tools.
PI: Ammasi Periasamy (Keck Center for Cellular Imaging)"
rc-website-fork/content/project/radiology-tustison-stone.md,"+++
title = ""Quantifying Cerebral Cortex Regions""
description = ""Blah blah blah here we are with a placeholder.""
date = ""2019-05-10T14:33:50-05:00""
author = ""UVARC Staff""
images = ""/images/projects/tustison-stone-radiology.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""radiology"",
  ""rivanna""
]
audio = true
draft = false
projecttype = [""image-analysis"",""clinical-research"",""tools"",""hpc-computing""]
+++
A powerful new technique for quantifying regions of the cerebral cortex was developed by Nick Tustison and James Stone at the University of Virginia along with collaborators from the University of Pennsylvania. It was evaluated using large data sets comprised of magnetic resonance imaging (MRI) of the human brain processed on a high-performance computing cluster at the University of Virginia.  By making this technique available as open-source software, other neuroscientists are now able to investigate various hypotheses concerning the relationship between brain structure and development. Tustison‚Äôs and Stone‚Äôs software has been widely disseminated and is being actively incorporated into a variety of clinical research studies, including a collaborative effort between the Department of Defense and Department of Veterans Affairs, exploring the long term effects of traumatic brain injury (TBI) among military service members.
Learn more about the ITK Insight Toolkit on GitHub.
PIs: Nick Tustison and James Stone (Department of Radiology)"
rc-website-fork/content/project/calcium-oscillations.md,"+++
title = ""Quantifying Calcium Oscillations""
description = """"
date = ""2020-10-05T09:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/cell-rosettes.png""
categories = [""projects""]
tags = [
  ""pharmacology"",
  ""matlab"",
  ""gui""
]
draft = false
projecttype = [""basic-science"",  ""image-analysis"", ""tools""]
publications = [{authors = ""Guagliardo, N.A., Klein, P.M., Gancayco, C.A. et al."", title = ""Angiotensin II induces coordinated calcium bursts in aldosterone-producing adrenal rosettes"", journal = ""Nature Communications"", year = ""2020"", doi = ""https://doi.org/10.1038/s41467-020-15408-4""}]
+++
Calcium oscillations signify communication between zona glomerulosa cells of the mouse adrenal gland. Researchers in the Barrett Lab can capture these oscillatory events with calcium imaging, but they had difficulty analyzing the results. The Barrett Lab was in need of a comprehensive MATLAB program for quantitative analysis of the intracellular calcium signals from their cell imaging experiments. Prior to Research Computing's involvement in the project, the Barrett Lab had been using fragments of code to analyze their data with little success. Research Computing developed a MATLAB application to create an efficient, centralized workflow that is also accessible to people who are new to MATLAB and programming.
With this application, the Barrett Lab was able to analyze the characteristics of calcium oscillatory events for the first time in two years. The app allows researchers to:


Choose data and analysis parameters


View full or partial fluorescent reading traces


Perform quantitative analysis of individual events and bursts of events


**PI: Paula Barrett"
rc-website-fork/content/project/cloud-migrations.md,"+++
title = ""Cloud Migration for HIPAA Data""
description = """"
date = ""2019-03-08T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/aws/aws-smile-logo.jpg""
categories = [""projects""]
tags = [
  ""cloud"",
  ""cbht"",
  ""aws"",
  ""hipaa"",
]
draft = true
projecttype = [""tools""]
+++
UVA Research Computing worked with researchers in the UVA Center for Behavioral Health and Technology to move many of its web-based research instruments to the cloud. 
CBHT ""has been involved in eHealth, specifically the development and testing of clinical interventions delivered via the Internet.  We believe the Internet can be used to implement engaging, interactive, and comprehensive interventions.  Our interventions are designed to improve outcome by tailoring the programs to the individual user.  We were among the first to test the feasibility and effectiveness of delivering Internet interventions."" Their research depends upon secure, data-driven websites that both enable participants to make regular submissions via web form, but also to safeguard private health data for researchers to learn from.
When their previous generation of hardware was ready to be retired, CBHT developers contacted Research Computing to help redesign their platform and plan their migration into the Amazon public cloud. Their setup includes private, encrypted databases and web servers, and a strictly-controlled environment for highly sensitive data. 
Learn more about the Center for Behavioral Health & Technology."
rc-website-fork/content/project/guagliardo.md,"+++
title = ""Analysis of Calcium Activity in Ex Vivo Adrenal Slices""
date = ""2024-09-23T00:00:00-05:00""
author = ""RC Staff""
images = ""/images/projects/guagliardo.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""pharmacology"",
  ""image-processing"",
  ""rivanna""
]
draft = false
projecttype = [""hpc-computing"", ""dac"", ""image-analysis""]
+++
This proposal is to enhance the computational analysis of calcium imaging data from live adrenal tissue in the study of primary aldosteronism, a leading cause of hypertension. The significant computational challenges associated with large calcium imaging datasets and complex analyses are addressed by first improving the current workflow for greater efficiency and accessibility, and second, establishing a robust computaional workflow and data management strategy.
PI: Nick Guagliardo, Dept. of Pharmacology"
rc-website-fork/content/project/reidenbach-envirosci.md,"+++
title = ""Fluid Dynamics and Reef Health""
description = ""Blah blah blah here we are with a placeholder.""
date = ""2019-06-20T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/fluid-dynamics.jpg""
categories = [""projects""]
tags = [
  ""hpc"",
  ""fluid-dynamics"",
  ""rivanna"",
  ""science""
]
draft = false
projecttype = [""hpc-computing""]
+++
Professor Reidenbach and his team are using Rivanna to run computational fluid dynamics simulations of wave and tide driven flows over coral reefs in order to determine how storms, nutrient inputs, and sediments impact reef health. This is an image of dye fluxing from the surface of the Hawaiian coral Porites compressa utilizing a technique known as planar laser induced fluorescence (PLIF). Reefs such as this one have been severely impacted by human alteration, both locally through additional inputs of sediments and nutrients, and globally through increased sea surface temperatures caused by climate change. Reidenbach is hopeful that his computational models will allow scientists to better predict the future health of reefs based on human activity and improve global reef restoration efforts.
PI: Matthew Reidenbach, PhD (Department of Environmental Sciences)"
rc-website-fork/content/project/nicu-vital-signs.md,"+++
title = ""Heart Rate Ranges in Neonates""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/heart-rate.jpg""
categories = [""projects""]
tags = [
    ""NICU""
]
draft = false
projecttype = [""clinical-research""]
publications = [{authors = ""Alonzo CJ, Nagraj VP, Zschaebitz JV, Lake DE, Moorman JR, Spaeder MC"", title = ""Heart rate ranges in premature neonates using high resolution physiologic data"", journal = ""Journal of Perinatology"", year = ""2018"", doi = ""10.1038/s41372-018-0156-1""}]
+++
There are limited evidence-based published heart rate ranges for premature neonates. However, knowing heart rate reference ranges in the premature neonatal population can be beneficial for bedside assessment in the Neonatal Intensive Care Unit (NICU).
RC is collaborating with clinical researchers in the Department of Pediatrics to establish baseline ranges for heart rate data in premature infants. These results are summarized from more than two billion data points collected via bedside monitoring in the NICU. RC staff has contributed data analysis and visualization expertise to aggregate the data, generate interactive heatmaps and produce tables of these ranges by gestational age.
PI: Corrie Alonzo (Department of Pediatrics‚ÄìNeonatology) & Mike Spaeder (Department of Pediatrics)"
rc-website-fork/content/project/esfarjani-aladyn.md,"+++
title = ""Thermal properties of materials from first-principles""
date = ""2025-04-07T00:00:00-05:00""
author = ""RC Staff""
images = ""/images/projects/esfarjani.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""materials-science"",
  ""simulations"",
  ""rivanna""
]
draft = false
projecttype = [""hpc-computing"",""engineering""]
publications = [{authors = ""Keivan Esfarjani, Harold Stokes, Safoura Nayeb Sadeghi, Yuan Liang, Bikash Timalsina, Han Meng, Junichiro Shiomi, Bolin Liao, Ruoshi Sun"", title = ""ALATDYN: A set of Anharmonic LATtice DYNamics codes to compute thermodynamic and thermal transport properties of crystalline solids"", journal = ""Computer Physics Communications"", volume = ""312"", year = ""2025"", pages = ""109575"", doi = ""10.1016/j.cpc.2025.109575""}]
+++
Prof. Esfarjani's group is using the HPC cluster to develop the Anharmonic LAttice DYNamics (ALADYN) software suite to calculate thermal transport properties and phase transitions from first-principles. The codes can extract force constants, solve the Boltzmann transport equation, predict thermal equilibrium based on the self-consistent phonon theory, and run molecular dynamics simulations within an anharmonic force field. The figure shows the phonon density of states and dispersion curve of Ge obtained from ALADYN.
PI: Keivan Esfarjani, PhD (Department of Materials Science & Engineering)"
rc-website-fork/content/project/shukla-nikhil.md,"+++
title = ""OmegaSync""
date = ""2025-03-20T00:00:00-05:00""
author = ""RC Staff""
images = ""/images/projects/shukla.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""parallel-computing"",
  ""kubernetes"",
  ""rivanna"",
  ""shiny""
]
draft = false
projecttype = [""hpc-computing"",""engineering"",""dac"",""tools"",""containers""]
+++
The Computing Hardware Research Lab (CHRL) worked with the DAC to develop a pipeline connecting a web app to HPC resources for solving computationally hard combinatorial optimization problems such as computing the MaxCut in complex graphs using OmegaSync. The DAC created an RShiny app running on Kubernetes that collects user information and graph files. The app formats data, saves it to the HPC filesystem, and automates job submissions. It also triggers email notifications to users upon job start and completion, providing the results they need. This project highlights the DAC's role in supporting faculty with complex research workflows.
PI: Nikhil Shukla, PhD (Department of Electrical and Computer Engineering)"
rc-website-fork/content/project/sonomicrometry-signal.md,"+++
title = ""Sonomicrometry Signal Classification""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/sonomicrometry.jpg""
categories = [""projects""]
tags = [""basic-science""]
draft = false
projecttype = [""basic-science"",""machine-learning""]
+++
Researchers are using sonomicrometry to study the biomechanics of the human brain. While at times the signals collected do not require any preprocessing, more frequently they do require some denoising or are too noisy to analyze. Currently, researchers are manually categorizing the quality of thousands of these sonomicrometry signals and preprocessing them individually. RC is helping researchers develop a machine learning model to classify the signals and to determine the necessary preprocessing steps.
Preliminary sonomicrometry data have been collected, and RC is working to classify, prepare, and normalize the data for use in a machine learning model. RC is currently developing preliminary models to classify the data by signal quality and preprocess automation techniques that will later be applied to noisy signals.
PI: Matthew Panzer (Center for Applied Biomechanics)"
rc-website-fork/content/project/epihet.md,"+++
title = ""epihet""
description = """"
author = ""RC Staff""
date = ""2018-05-03T14:33:50-05:00""
images = ""/images/projects/r-language.png""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""cphg"",
  ""r"",
]
draft = false
projecttype = [""basic-science"", ""tools""]
+++
RC is working with researchers in the Center for Public Health Genomics to write an R package to calculate Relative Proportion of Sites with Intermediate Methylation (RPIM) scores, which represent the epigenetic heterogeneity in a bisulfite sequencing sample.
https://github.com/databio/epihet
PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/arctic.md,"+++
title = ""Integrated Sensor Networks in Arctic Alaska""
description = """"
date = ""2022-03-04T09:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/weather-station.png""
categories = [""projects""]
tags = [
  ""storage"",
  ""microservices"",
  ""data"",
  ""nsf""
]
audio = true
draft = false
projecttype = [""basic-science"", ""containers"", ""engineering"",""tools""]
publications = [{authors = ""Hartka, T., Gancayco, C., McMurry, T., Robson, M., Weaver, A."", title = ""Accuracy of algorithms to predict injury severity in older adults for trauma triage"", journal = ""Traffic Injury Prevention"", year = ""2019"", doi = ""https://doi.org/10.1080/15389588.2019.1688795""}]
+++
Understanding the Changing Natural-Built Landscape in an Arctic Community
Navigating the New Arctic (NNA) is one of The National Science Foundation's 10 Big Ideas. NNA projects address convergence scientific challenges in the rapidly changing Arctic. Arctic research is needed to inform the economy, security and resilience of the Nation, the larger region and the globe. NNA empowers new research partnerships from local to international scales, diversifies the next generation of Arctic researchers, enhances efforts in formal and informal education, and integrates the co-production of knowledge where appropriate. This award fulfills part of that aim by addressing interactions among social systems, natural environment, and built environment in the following NNA focus areas: Arctic Residents, Data and Observation, Education, and Resilient Infrastructure. 
Arctic communities face many challenges as they grow and develop in the context of a rapidly changing environment. These challenges include coastal erosion, permafrost thaw, and ecosystem change. This project is developing and deploying a network of environmental sensors collecting continuous information over a five-year period in terrestrial and aquatic locations within the community of Utqiagvik. The sensor network yields an unprecedented dataset for examining the interactive effects of the natural and built environments. This project is improving the health and economic well-being of Utqiagvik and potentially other North Slope Borough villages in Alaska. 
This research investigates two essential challenges for the Arctic city of Utqiagvik, Alaska: i) the impacts of existing community infrastructure practices on the surrounding tundra, coastal, and lagoon landscapes within and around the city, and ii) the impacts of a changing environment on the design and future planning of community infrastructure and buildings. The ultimate goal of the project is to understand how the natural and built environments interact with social systems in an Arctic city.
UVA Research Computing is supporting this research by hosting computational services to ingest, process, transform, store, and serve sensor data over the life of the project. A scalable data service named HSDS runs within a containerized environment in the HPC networks to aggregate the collected sensor data. This service is backed by object storage, and is then served internally for research using a variety of data analysis tools.
Data: https://arcticdata.io/
Award: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2022639
PI: Howard Epstein, Chair, Dept. of Environmental Sciences."
rc-website-fork/content/project/primed.md,"+++
title = ""Center for Diabetes Technology PriMed""
description = ""Blah blah blah here we are with a placeholder.""
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/dexcom.jpg""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""machine-learning"",
]
draft = false
projecttype = [""tools"",""clinical-research"",""machine-learning""]
+++
In their research around constant glucose monitoring and the automated maintenance of insulin for patients, the CDT is exploring data drawn from external data sources such as DexCom and FitBit. RC has assisted the CDT by designing a secure computing footprint in Amazon Web Services to pull in these data, parse and process them, in order to perform deeper analytics through machine learning. In January 2018, CDT sponsored a ski camp at Wintergreen Resort for a group of youth diagnosed with Type I diabetes with the goal of importing glucose, insulin, and exercise metrics at the end of each day through remote web APIs. This proof of concept has enabled the CDT to move forward with further monitoring efforts that draw upon existing devices and providers, rather than re-inventing them in a singular system. 
PI: Marc Breton, PhD (Center for Diabetes Technology)"
rc-website-fork/content/project/ciliberto-economics.md,"+++
title = ""Economic Market Behavior""
description = ""Blah blah blah here we are with a placeholder.""
date = ""2019-06-13T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/market-trends.jpg""
categories = [""projects""]
tags = [
  ""hpc"",
  ""economics"",
  ""rivanna"",
  ""parallel-computing""
]
draft = false
projecttype = [""hpc-computing"",""social-science""]
+++
While conducting research for a highly-technical study of market behavior, Dr. Ciliberto realized that he needed to parallelize an integration over a sample distribution. RC staff member Ed Hall successfully parallelized Ciliberto‚Äôs Matlab code and taught him how to do production runs on the University‚Äôs high-performance clusters. ‚ÄúThe second stage estimator was computationally intensive,‚Äù Ciliberto recalls. ‚ÄúWe needed to compute the distribution of the residuals and unobservables for multiple parameter values and at many different points of the distribution, which requires parallelizing the computation. Ed Hall‚Äôs expertise in this area was crucial. In fact, without Ed‚Äôs contribution, this project could not have been completed.‚Äù
PI: Federico Ciliberto, PhD (Department of Economics)"
rc-website-fork/content/project/lolaweb.md,"+++
title = ""LOLAweb""
author = ""RC Staff""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""containers"",
  ""docker"",
  ""r"",
  ""cphg"",
  ""shiny""]
images = ""/images/projects/LOLAweb-logo-square.png""
description = """"
date = ""2020-02-23T14:33:50-05:00""
draft = false
audio = true
projecttype = [""basic-science"", ""tools"", ""containers""]
publications = [{authors = ""Nagraj VP, Magee NE, Sheffield NC"", title = ""LOLAweb: a containerized web server for interactive genomic locus overlap enrichment analysis"", journal = ""Nucleic Acids Research"", year = ""2018"", doi = ""10.1093/nar/gky464""}]
+++
The past few years have seen an explosion of interest in understanding the role of regulatory DNA. This interest has driven large-scale production of functional genomics data resources and analytical methods. One popular analysis is to test for enrichment of overlaps between a query set of genomic regions and a database of region sets. In this way, annotations from external data sources can be easily connected to new genomic data.
SOM Research Computing is working with faculty in the UVA Center for Public Health Genomics to implement LOLAweb, an online tool for performing genomic locus overlap annotations and analyses. This project, written in the statistical programming language R, allows users to specify region set data in BED format for automated enrichment analysis. LOLAweb provides interactive plots and annotated data based on specific reference genomes and region databases. 

https://github.com/databio/LOLAweb/
PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/johnson-steven.md,"+++
title = ""Distribution and Discovery of Digital Information ""
date = ""2025-03-30T00:00:00-05:00""
author = ""RC Staff""
images = ""/images/projects/johnson.png""
categories = [""projects""]
tags = [
  ""hpc"",
  ""cloud"",
  ""data-analysis"",
  ""data"",
  ""data-transfer""
]
draft = false
projecttype = [""social-science"",""dac""]
+++
A team of UVA Commerce faculty partnered with the DAC to explore how information about personal health and finance propagates on online platforms and influences decisions. The DAC developed for them a process to merge and standardize data from CoreLogic and ComScore, two sources of consumer and real estate information. The DAC assisted by developing a method to map real estate data to Designated Market Areas (DMAs), which are geographic regions where the population receives similar types of information through the media. The DMA mapping allowed the researchers to link consumer behavior in the real estate market with demographic characteristics. To make the data analysis more efficient, the DAC automated the data ingestion and analysis processes using high-performance computing. This project demonstrates how the DAC can assist faculty in tackling complex and socially relevant research questions using big data.
PI: Steven Johnson, PhD (UVA McIntire School of Commerce)"
rc-website-fork/content/project/cardiovascular-genomics.md,"+++
description = """"
title = ""Cardiovascular Genomics""
draft = false
date = ""2018-05-03T14:33:50-05:00""
tags = [""genomics"",""cphg""]
categories = [""projects""]
images = ""/images/projects/cphg.png""
author = ""RC Staff""
+++
Coronary artery disease (CAD) is the major cause of morbidity and mortality worldwide. Recent genome wide association studies (GWAS) have revealed more than 50 genomic loci that are associated with increased risk for CAD. However, the pathological mechanisms for the majority of the GWAS loci leading to increased susceptibility to this complex disorder are still unclear. Many of the CAD loci appear to act through the vessel wall, presumably affecting smooth muscle cell (SMC) function. 
UVA Research Computing (RC) is working with Redouane Aherrahrou from the Center for Public Health Genomics who aims to study the impact of the CAD-associated genetic factors on the cellular and molecular SMC phenotypes, as well as the underlying biological pathways that are perturbed by these genetic factors. 
While providing scientific programming and data analysis support for this project, Research Computing has:

Developed a series of scripts to programmatically normalize and summarize experimental data
Created interactive and static data visualizations
Performed statistical hypothesis tests
Provided guidance on the use of the local high-performance computing cluster (Rivanna)
"
rc-website-fork/content/project/political-sentiment.md,"+++
title = ""Political Sentiment Analysis""
author = ""RC Staff""
categories = [""projects""]
tags = [""hpc"", ""R"", ""text analysis"",""data-science"",""hpc-computing"" ]
images = ""/images/projects/political-sentiment.png""
description = """"
date = ""2021-03-28T17:18:27-04:00""
draft = false
audio = true
projecttype = [""social-science"", ""hpc-computing"", ""data-science""]
+++
The nature of political communication has been fundamentally altered by the emergence of social media. In earlier eras, social scientists, journalists, and citizens could focus on static statements by politicians and candidates in order to understand the nature of political discourse. Social scientists studying political communication would design surveys and focus groups to understand which messages were received by citizens, and with what effect. Today, as news moves to digital platforms and as political figures increasingly rely on social media, political communication is fundamentally dynamic. Studying patterns of communication among politicians, their supporters, and their critics requires scholarly focus on the content, sentiment, and framing of posts on various social media platforms. Similarly, making sense of the contours of an election campaign requires that scholars explore the interplay among online messages and real-world events such as endorsements, scandals, and standing in the polls. All of this means that the tools of political analysis must be combined with the tools of data science.
Over the past several years a team of researchers from the Department of Politics, the School of Data Science, and Research Computing have collaborated to create a system to capture and store every Tweet to and from every major presidential candidate since the 2016 primaries, continuing with every Tweet to and from President Trump since Election Day. This collaboration has produced a unique database of approximately one billion tweets (currently 5 TB). While the analysis of tweets from candidates and office holders is commonplace, collecting, organizing, and analyzing tweets to these figures, both from supporters and opponents, requires significant effort and sustained collaboration. 
PI: Paul Freedman (Department of Politics)"
rc-website-fork/content/project/simpleCache.md,"+++
title = ""simpleCache""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/r-language.png""
categories = [""projects""]
tags = [
  ""bioinformatics"",
  ""r"",
]
draft = false
projecttype = [""tools""]
publications = [{authors  = ""Sheffield NS, Nagraj VP, Reuter V"", title = ""simpleCache: R caching for reproducible, distributed, large-scale projects"", journal = ""Journal of Open Source Software"", year = ""2018"", doi = ""10.21105/joss.00463""}]
+++
In partnership with researchers in the Center for Public Health Genomics, School of Medicine Research Computing has contributed to the development of a novel package for computationally efficient caching and loading of data in R. simpleCache provides an interface to a series of functions to store and retrieve cached objects, including in the context batch processing or HPC environments. The package further extends base R functionality of saving and loading external representations of objects by enabling caching to pre-defined directories and timed cache operations.
RC helped document and develop new functions for the package ahead of its release to the Comprehensive R Archive Network (CRAN). An accompanying article was selected for publication in the Journal of Open Source Software in early 2018.
https://CRAN.R-project.org/package=simpleCache
PI: Nathan Sheffield (Center for Public Health Genomics)"
rc-website-fork/content/project/surgical-research.md,"+++
title = ""Secure Computing for Surgical Research""
description = """"
date = ""2018-05-03T14:33:50-05:00""
author = ""RC Staff""
images = ""/images/projects/secure-surgery.png""
categories = [""projects""]
tags = [
  ""secure-computing"",
  ""ivy"",
  ""hipaa""
]
draft = false
projecttype = [""clinical-research""]
+++
RC is working with Dr. Eric Schneider to create a secure computing environment for the research of the Healthcare Surgical Outcome team. Data from this project will contain HIPAA identifiers, as well as Medicare information, and requires more security and control of data ingress/egress than projects previously hosted on the Ivy platform. After successful implementation of this project, RC will create a similar computing environment for DoD blast and traumatic brain injury data collected by Dr. Schneider before he joined UVA.
PI: Eric Schneider (Department of Surgery)"
rc-website-fork/content/project/ercp.md,"+++
title = ""Outcomes of Dexmedetomidine During ERCP on Post-ERCP Pancreatitis""
description = """"
author = ""RC Staff""
date = ""2025-03-18""
images = ""/images/projects/ercp.png""
caption = ""Image by Davee T, et al. CC BY-NC-SA""
categories = [""projects""]
tags = [
  ""health"",
  ""data"",
  ""machine-learning""
]
draft = false
projecttype = [""clinical-research"", ""machine-learning"", ""dac""]
+++
Endoscopic retrograde cholangiopancreatography (ERCP) is a procedure in which an endoscope is guided into the first part of the small bowel for various instruments to be passed into the biliary and pancreatic ducts to remove obstructions, drain infectious collections or diagnose diseases of the bile ducts.  ERCP is associated with higher rates of complications than other endoscopic procedures, one of the most severe being post-ERCP pancreatitis which is thought to take place in up to 10% of patients.
Dr. Podboy and Internal Medicine Resident Physician Dr. Jason Erno are interested in factors that may contribute to reduced outcomes of post-ERCP pancreatitis, such as the use of dexmedetomidine as an anesthetic agent.  Research Computing‚Äôs Data Analytics Center investigated this by performing statistical and machine learning analyses of data from a retrospective cohort study who underwent ERCP at UVA in the last five years.  These analyses examined associations between post-ERCP pancreatitis and factors such as dexmedetomidine use, patient demographics, pre-procedural medications, and specific intra-operative stent placements and procedures.
Full image attribution: Davee T, Garcia JA, Baron TH. Precut sphincterotomy for selective biliary duct cannulation during endoscopic retrograde cholangiopancreatography. Annals of Gastroenterology (2012). PMCID: PMC3959408
PI: Alexander Podboy, MD (Division of Gastroenterology and Hepatology)"
rc-website-fork/content/userinfo/pricing.md,"+++
author = ""RC Staff""
description = """"
title = ""Pricing""
date = ""2024-12-03T10:08:29-05:00""
draft = false
tags = [""compute"",""rivanna"",""ivy"",""hpc"",""allocations"",""storage""]
categories = [""userinfo""]
images = [""""]
+++
Below is a schedule of prices for Research Computing resources.

HPC Service Unit Allocations
{{< pricing allocations >}}
About Allocations

HPC Dedicated Computing
{{< pricing dedicated_computing >}}

Storage

{{< rawhtml >}}
  {{< pricing storage >}}
{{< /rawhtml >}}
Storage Details  ¬†¬† Request Storage

Ivy Virtual Machines
{{< pricing ivy >}}
Ivy Details ¬†¬† Request Ivy Resources


"
rc-website-fork/content/userinfo/storage.md,"+++
author = ""Staff""
description = """"
title = ""Research Data Storage""
date = ""2020-02-03T10:08:29-05:00""
draft = false
tags = [""storage"",""security"",""ivy"",""rivanna"",""project"",""standard"",""data""]
categories = [""userinfo""]
images = [""""]
aliases = [ ""/storage"" ]
+++

{{< getstatus keyword=""storage"" >}}


There are a variety of options for storing research data at UVA. Public, internal use, and sensitive data storage systems can be accessed from the Rivanna and Afton high performance computing systems. Highly sensitive data can be stored and accessed within the Ivy secure computing environment. University Information Security provides an overview of the data sensitivity classifications, while our  Data Sensitivity and Research Computing Systems table  specifies where each type of data can be stored or analyzed in compliance with regulations.
{{% highlight %}}
{{% pi-eligibility %}}
{{% /highlight %}}
{{< highlight >}}
  Information Technology Services (ITS) also provides multiple tiers of data storage for personal and non-research storage needs.
{{< /highlight >}}


Public, Internal Use, and Sensitive Data Storage {#public-internal-use-sensitive-data-storage}
Public data are intentionally made available to the public. Examples of public data in research computing include, but are not limited to: 

Data intended for display on a  public website
Public Data Sets obtained from a publicly available source
Open source code

Internal use data are classified as public records available to anyone in accordance with the Virginia Freedom of Information Act (FOIA) but are not intentionally made public. Examples of internal use data within a research computing context include but are not limited to: 

audits
models, scripts, and logfiles
Preliminary analyses or reports
Correspondence

Sensitive data is the default classification for all data that is not explicitly defined as highly sensitive data, may be held from release under FOIA, or that is not intended to be made publicly available. Examples of sensitive data within a research computing context include, but are not limited to:

University ID numbers
FERPA-protected student information not covered by the definition of highly sensitive data
Health information where all Protected Health Information (PHI) have been systematically removed (i.e., de-identified) or aggregated, making identification impossible
Personnel and financial information not covered by the definition of highly sensitive data, but not intended to be public
Any information that doesn‚Äôt fit into public, internal use, or highly sensitive data categories

{{< storage_main_page_first >}}
1For PIs with existing Research Standard Storage, the charges will be adjusted accordingly. PIs without existing Research Standard Storage will need to submit a request for the storage.
2Snapshot files are uneditable backup copies of all the files and folders in your account, taken at a daily interval. The Research Project Storage system keeps these snapshots for a week. Snapshot files are deleted sequentially after a week has passed. This saving method is useful for human error prevention as any accidentally deleted files may be recovered. Look to our FAQ page to learn how to access your snapshots.
3Replication is a data management process that stores copies of data fragments over a distributed cluster or database. By having replicated data across each node or server on a given database, data can be accessed more reliably than data that only resides on a single server. This saving method is useful for disaster scenarios where if data is stored on multiple disks, and one disk fails, the data is still accessible.
4Backup files are copies of files that are stored on a separate disk storage than that of the original copies. Files may be backed up on a separate disk storage or within cloud storage. Backed up files are not synced with their original, so any edits to the original are not reflected on the backup. This saving method is useful for disaster scenarios where if the original disk storage is unsalvageable, the backups may still be accessible.

Highly Sensitive Data Storage
Highly sensitive data (HSD) are data that require restrictions on access under the law or that may be protected from release in accordance with applicable law or regulation. HSD includes personal information that can lead to identity theft or health information that reveals an individual's health condition and/or medical history. Examples of HSD include, but are not limited to: 

Personally identifying information (PII) is any information that can be used to identify a person. Examples of PII include social security number, passport number, driver‚Äôs license number, military identification number, or biometric records (e.g. photographic facial images, fingerprints, voice signature, etc.).
Health information that reveals an individual‚Äôs health condition and/or medical history, including information defined by the Health Insurance Portability and Accountability Act (HIPAA)

{{< storage_main_page_second >}}
Researchers who request space on High-Security Research Standard must first request an Ivy account using the Ivy request form. Further information on Ivy and the High-Security Research storage can be found here.
High-Security Research Standard Storage is accessible by using Globus and connecting to the High-Security DTN.
Ivy Central Storage (ICS)
Ivy Central Storage has been replaced by High-Security Research Standard storage.

Request Storage
Storage requests can be made via this form:
Request / Purchase Storage"
rc-website-fork/content/userinfo/resources.md,"+++
description = """"
title = ""Resources""
draft = true
tags = [""tag1"",""tag2""]
date = ""2017-01-26T12:05:54-05:00""
categories = [""userinfo""]
images = [""""]
author = ""RC Staff""
+++
Resources are defined here."
rc-website-fork/content/userinfo/transition_new_r_libraries.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-06-16T00:00:00-05:00""
title = ""Transitioning to New R Modules: June 17, 2020""
draft = false
tags = [""Rivanna""]
categories = [""userinfo""]
+++

The recommended steps for transitioning your R programs after the June maintenance are as follows:

Determine which version of R you will be using (e.g., R/3.6.3).
Open a terminal window on the HPC system and load the version of R that you chose in step #1 (e.g., module load goolf R/3.6.3).
(Optional) Run our script to rebuild your existing R library for the newer version of R.  For example, if you had been using R/3.5.1 and are switching to R/3.6.3, type the following in the terminal window:  updateRlib 3.5.1 .  Make sure that you have loaded any other modules (e.g., curl, gdal) that your packages may need.
Update your Slurm scripts to load the newer version of R.

We at Research Computing understand that you may have some issues during the transition.  To help with the transition, we will have additional office hours specifically for your R questions:


Thursday, 18 June,  3:00-5:00pm   Join us via Zoom



Friday, 19 June,  3:00-5:00pm Join us via Zoom

"
rc-website-fork/content/userinfo/hipaa-compliance.md,"+++
title = ""HIPAA Compliance""
description = """"
date = ""2017-01-20T13:47:27-05:00""
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""support"",
  ""userinfo"",
]
tags = [
  ""hipaa"",
  ""security"",
]
draft = true
+++
UVA School of Medicine Research Computing can assist medical researchers in both understanding what HIPAA compliance requires of their work, and how to implement technical solutions to achieve and verify such compliance.
Review & Assessment
From a compliance perspective, RC offers three levels of review/assessment:


Security Plan - Having a security plan in place is important to your success at UVA School of Medicine. If your lab or department doesn‚Äôt have a plan in place Research Computing Information Security will be happy to help you develop such a plan.


Security Review - If you have a security plan in place already we can help you verify that your computing resources are functioning as documented in your security plan. As part of security review, we will do a risk analyst and provide you a list of recommended enhancements.


Risk Assessment - This is the systematic process of evaluating the potential security risks/hazards and any business impact they could present.  We analyze the likelihood of events occurring. As we conduct a risk assessment we look for vulnerabilities and weaknesses that could your system more susceptible to an event. We will provide list of finds and either work with your local support person or other resources to help mitigate the risk.  



Implementation
For implementation, RC offers a number of skills and services:

PHI and De-identification
Encryption best practices
Encryption of files, databases, and systems


PHI & De-Identification
Some research data are ""de-identified"" in order to remove them from HIPAA security requirements. This is particularly useful when researchers are processing across many hundreds or thousands of patients for trends and statistically meaningful insights that do not rely upon patient-specific data points.
Here are some common examples of PHI research data that may need to be de-identified. De-identification can mean the complete removal of such fields from your dataset, or the complete replacement of these data with meaningless placeholder values.
A. Names
B. All geographic subdivisions smaller than a state, including street address, city, county, precinct, ZIP code, and their equivalent geocodes, except for the initial three digits of the ZIP code if, according to the current publicly available data from the Bureau of the Census:


The geographic unit formed by combining all ZIP codes with the same three initial digits contains more than 20,000 people; and


The initial three digits of a ZIP code for all such geographic units containing 20,000 or fewer people is changed to 000


C. All elements of dates (except year) for dates that are directly related to an individual, including birthdate, admission date, discharge date, death date, and all ages over 89 and all elements of dates (including year) indicative of such age, except that such ages and elements may be aggregated into a single category of age 90 or older
D. Telephone numbers
E. Vehicle identifiers and serial numbers, including license plate numbers
F. Fax numbers
G. Device identifiers and serial numbers
H. Email addresses
I. Web Universal Resource Locators (URLs)
J. Social security numbers
K. Internet Protocol (IP) addresses
L. Medical record numbers
M. Biometric identifiers, including finger- and voice-prints
N. Health plan beneficiary numbers
O. Full-face photographs and any comparable images
P. Account numbers
Q. Any other unique identifying number, characteristic, or code, except as permitted as a ""re-identifier""
R. Certificate/license numbers

Resources

HIPAA for Professionals & Providers (HHS)
"
rc-website-fork/content/userinfo/data-transfer.md,"+++
date = ""2020-02-21T15:12:46-05:00""
tags = [
    ""data-transfer"",
    ""globus"",
    ""sftp""
    ]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
description = """"
title = ""Data Transfer""
draft = false
+++
Efficient and reliable data transfer is a critical component of research computing. A variety of useful tools is available for rapid data transfer, whether you are transferring data from an external site or within different computing environments at UVA. 

Common Scenarios

Transfer public or internal use data between local workstation/laptop and UVA storage
Transfer sensitive or highly sensitive data to Ivy storage
Transfer data between external institutions/supercomputing centers and UVA 
Transfer between UVA HPC and Cloud storage

The data transfer method you choose heavily relies on the data sensitivity classification, where the data are currently located and to where you want to transfer the data. Click on a row in the table below to learn more about the data transfer methods available for a specific scenario.







System 1
System 2
Example Scenario







Lab Workstation




Storage for Public and Internal Use Data



Copy data from a lab workstation to a /project storage share.

Copy result files from a /scratch directory on Rivanna or Afton.

 Expand




Local Computer

Laptop
Lab Workstation



Remote System

/home on Rivanna or Afton
/scratch on Rivanna or Afton
Research Project Storage
Research Standard Storage



Data Transfer Methods

Globus Connect
Graphical SFTP Clients
Command Line Tools














System 1
System 2
Example Scenario







Secure Lab Workstation




Storage for Sensitive and Highly Sensitive Data



Transfer HIPAA data from a Health Systems workstation to High-Security Research Standard Storage.

¬†
 Expand




Sources

Lab Workstation
Health Systems Workstation



Destination

High-Security Research Standard Storage



Data Transfer Methods

Globus Connect














System 1
System 2
Example Scenario







External Institution




University of Virginia



Transfer public or not-highly sensitive data collected at another institution to UVA storage.

Transfer results from an analysis carried out on a remote supercomputer at a national lab or supercomputing center.

 Expand




Sources
                Any institution that uses Globus, such as:
                
Other universities
Supercomputing facilities



Destinations

/home
/scratch
Research Project Storage
Research Standard Storage
High-Security Research Standard Storage



Data Transfer Methods

Globus Connect














System 1
System 2
Example Scenario







Rivanna/Afton Storage




Cloud Storage



Transfer public or not-highly sensitive data from Research Project & Research Standard storage or Rivanna/Afton home & scratch directories to AWS cloud storage.

 Expand




Sources
                Any institution that uses Globus, such as:
                
/home
/scratch
Research Project Storage
Research Standard Storage



Destinations

AWS S3
AWS S3 Deep Glacier



Data Transfer Methods

AWS command line tools







Data Transfer Methods




Globus Connect
Large-scale research data transfer


                    Transferring large amounts of research data is fast and simple with Globus Connect. Globus gives researchers unified access to their data through an easy-to-use web interface, and can be used to transfer data between your laptop and storage systems mounted on the HPC system. Globus can also be used to transfer data from other universities or supercomputing facilities.
                
Learn more ¬†
            Access Globus

   
                    For transferring highly sensitive data such as HIPAA or CUI data to the Ivy secure computing environment, researchers must use the secure Globus data transfer node (DTN).
                
Learn more







Graphical SFTP Clients

Secure file transfer protocol (SFTP)

                    Programs such as MobaXterm, Filezilla, and Cyberduck provide a graphical user interface to transfer data between a local computer and a remote storage location that permits scp or sftp. These applications allow drag-and-drop file manipulation.
                
Learn more







Command Line Tools

Transferring Files from a Terminal

                    Researchers who are comfortable with the command line can use a variety of command line tools to transfer their data between their laptops and storage systems. Programs such as scp, sftp, rsync and aws cli can be used to quickly transfer files.
                
Learn more



Local Data Transfer with the Command Line
When using a Linux file system, users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod) and navigate the file system (cd, ls, pwd).  If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below:

10 Essential Linux Commands
How To Manage Files From The Linux Terminal


Shell Novice

Transfering Data to Cloud Storage
Several command line tools are available to transfer data from your UVA storage locations to the cloud. On the HPC system we provide the rsync and aws cli tools to transfer files from Research Project, Research Standard and Rivanna/Afton home & scratch directories to AWS storage.
Learn more about the AWS CLI tools
{{% callout %}}
For more help, please feel free to contact RC staff to set up a consultation or visit us during office hours. 
{{% /callout %}}"
rc-website-fork/content/userinfo/computing-environments.md,"+++
author = ""RC Staff""
description = """"
title = ""Computing Environments at UVA""
date = ""2025-01-15T08:08:29-05:00""
draft = false
tags = [""compute"",""rivanna"",""ivy"",""containers"",""systems"",""kubernetes"",""hipaa"",""hpc"",""accord""]
categories = [""userinfo""]
images = [""""]
aliases = [ ""/facilities"" ]
+++
Research Computing (UVA-RC) serves as the principal center for computational resources and associated expertise at the University of Virginia (UVA). Each year UVA-RC provides services to over 433 active PIs that sponsor more than 2463 unique users from 14 different schools/organizations at the University, maintaining a breadth of systems to support the computational and data intensive research of UVA‚Äôs researchers.
High Performance Computing
Standard Security Zone
UVA-RC‚Äôs High Performance Computing (HPC) systems are designed with high-speed networks, high performance storage, GPUs, and large amounts of memory in order to support modern compute and memory intensive programs. UVA-RC operates two HPC systems within the standard security zone, Rivanna and Afton. In total these systems are comprised of over 900 compute nodes, with a total of more than 48,000 X86 64-bit compute cores. Scheduled using SLURM, these resources can support over 1.5 PFLOP of peak CPU performance. HPC nodes are equipped with between 375 GB and 1.5 TB of RAM to support applications that require small and large amounts of memory, and 55 nodes include various configurations of the NVIDIA general purpose GPU accelerators (RTX2080, RTX3090, A6000, V100, A40, A100 and H200), from 4- to 10-way.
UVA-RC also acquires and maintains capability systems focused on providing novel environments. This includes an 18-node DGX BasePOD system with 8x A100 GPU devices per node, as well as newly added HGX H200 GPU nodes. The BasePOD provides a shared memory space across all GPUs in the system, allowing it to work collectively on models with memory needs larger than what can be held in a single node. The addition of H200 nodes further enhances UVA-RC‚Äôs support for large-scale AI workloads and memory-intensive applications. More information can be found here.
High Security Zone
The High-Security HPC (Rio) cluster is a high-performance computing system specifically designed for the processing and analysis of controlled-access and highly sensitive data. It features high-speed networks, high-performance storage, and GPUs - including an NVIDIA HGX H200 GPU - to support demanding computational tasks. Currently, Rio comprises 39 compute nodes, providing a total of 1,560 x86 64-bit compute cores. Each HPC node is equipped with 375 GB of RAM to accommodate memory-intensive applications. Additional GPU nodes designed to support AI and machine learning workloads will be integrated in the near future. 
Situated within the high-security zone, Rio can only be accessed through Ivy Linux virtual machines, ensuring compliance with stringent security requirements for data storage and processing. Compute time is managed through service unit allocations and fairshare models, with SLURM serving as the job scheduler. This architecture provides a secure, efficient, and robust environment for handling sensitive research workloads.
Interactive Computing and Scientific Visualization
UVA-RC supports specialized interfaces (i.e., Open OnDemand, FastX) and hardware for remote visualization and interactive computing. Interactive HPC systems allow real-time user inputs in order to facilitate code development, real-time data exploration, and visualizations. Interactive HPC systems are used when data are too large to download to a desktop or laptop, software is difficult or impossible to install on a personal machine, or specialized hardware resources (e.g., GPUs) are needed to visualize large data sets.
Expertise
UVA-RC aggregates expertise to provide consulting and collaboration services to researchers addressing all levels of the Research Computing technology stack.
UVA-RCs user support staff provide basic support and general onboarding through helpdesk and regularly scheduled tutorials. The Data Analytics Center (DAC) serves as a central hub for computational services and expertise in Data Analytics, including bioinformatics, image processing, text analysis, Artificial Intelligence. By offering specialized support and resources, the DAC connects UVA researchers with the advanced computing capabilities essential for their data-intensive research and analysis. The DAC provides a comprehensive suite of services designed to enhance research efforts. The services are organized into three primary areas: Training and Technical Support, Consultations, and Collaborations. Training and technical support, offered as a general service, includes tutorials and technical assistance with research computing systems. Consultations, also free of charge, facilitate knowledge sharing and the design of analysis workflows through dedicated meetings. DAC team members are available for collaborations on grants. Collaborations involve more in-depth work than consultations and in general require a DAC member to be embedded into a researcher‚Äôs grant. The DAC members, all holding advanced degrees, are adept at reviewing published techniques and adapting them to specific domain use cases, ensuring tailored and effective support for researchers.
Senior support staff have advanced degrees in relevant research domains such as biology, imaging, physics, computer science and material science, enabling in-depth collaboration on complex projects. For projects that require significant application development work, UVA-RC maintains a Solutions & DevOps team capable of rapid iteration while leveraging non-traditional HPC technologies. Lastly, UVA-RC's Infrastructure Services team enables projects that may require custom hardware or configurations outside of the standard images. Beyond their availability for direct project support, together these teams provide the R&D and operations expertise needed to ensure that UVA-RC is providing a modern research computing ecosystem for UVA researchers.
Cloud Computing¬†{#ivy}
Ivy is a secure computing environment for researchers consisting of virtual machines (Linux and Windows) backed by a total of 45 nodes and 2048 cores. Researchers can use Ivy to process and store sensitive data with the confidence that the environment is secure and meets HIPAA, FERPA, or CUI requirements.
For standard security projects, UVA-RC supports microservices in a clustered orchestration environment that leverages Kubernetes to automate the deployment and management of many containers in an easy and scalable manner. This cluster has 876 cores and 4.9TB of memory allocated to running containerized services, including one node with 4 x A100 GPUs. It also has over 300TB of cluster storage and can attach to UVA-RC's broader storage offerings.¬†
ACCORD
The ACCORD project (NSF Award: #1919667) offers flexible web-based interfaces for sensitive and highly sensitive data in a system focused on supporting cross-institutional access and collaboration. The ACCORD platform consists of 8 nodes in a Kubernetes cluster, for a total of 320 cores and ~3.2TB of memory. Cluster storage is approximately 1PB of IBM Spectrum storage (GPFS).
Researchers from non-UVA institutions can be brought into the ACCORD system through a memorandum of understanding between the researcher‚Äôs institution and UVA, security training for the researcher, and a posture-checking client installed on the researcher‚Äôs laptop/desktop.¬†
Data Storage
All researchers on UVA-RC's systems have access to a high-performance parallel storage platform. This system provides 8PB (PetaBytes) of storage with sustained read and write speeds of up to 10 GB/sec. The integrity of the data is protected by daily snapshots. UVA-RC also supports a second-tier storage solution, 3 PB, designed to address the growing need for resources that support data-intensive research by offering a lower cost, scalable solution. The system is tightly integrated with other UVA-RC storage and computing resources in order to support a wide variety of research data life cycles and data analysis workflows.¬†
Data Centers, Network Connectivity, and Office Facilities
UVA-RC enables interdisciplinary research through its robust data center facilities with over 1.5 MW of IT capacity to support leading edge computational and data storage systems. UVA-RC's equipment occupies a data center near campus, connected to the 10 Gbps campus network. Dedicated 10 and 100 Gbps links to our regional optical network and Internet2 give our researchers the network capacity and capability needed to collaborate with researchers from around the world. A Globus data transfer node enables data access and transfers to transcend institutional credentials. Located in the Michie North Building at 918 Emmet Street, UVA-RC‚Äôs offices are a short shuttle ride away from the central UVA grounds."
rc-website-fork/content/userinfo/xsede.md,"+++
date = ""2020-02-21T15:12:46-05:00""
tags = [
    ""xsede"",
    ""hpc"",
    ""containers"",
    ""parallel"",
    ""matlab"",
    ""linux"",
    ""cloud"",
    ""supercomputer"",
    ""education""
    ]
categories = [""userinfo""]
images = [""""]
author = ""RC Staff""
description = """"
title = ""XSEDE: Extreme Science and Engineering Development Environment""
draft = false
+++
XSEDE's Mission was to substantially enhance the productivity of a growing community of scholars, researchers, and engineers through access to advanced digital services that support open research; and coordinate and add significant value to the leading cyberinfrastructure resources funded by the NSF and other agencies.  

The XSEDE project ended on August 31, 2022 and was succeeded by the ¬†ACCESS project. 
XSEDE Home:

¬†www.xsede.org
"
rc-website-fork/content/userinfo/access-ci.md,"+++
date = ""2020-02-21T15:12:46-05:00""
tags = [
    ""access"",
    ""hpc"",
    ""containers"",
    ""parallel"",
    ""linux"",
    ""cloud"",
    ""supercomputer"",
    ""education""
    ]
categories = [""userinfo""]
images = [""""]
author = ""RC Staff""
description = """"
title = ""ACCESS: Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support""
draft = false
+++
The NSF‚Äôs ACCESS (Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support) program builds upon the successes of the 11-year XSEDE project, while also expanding the ecosystem with capabilities for new modes of research and further democratizing participation.

---

# ACCESS Home:
 - ¬†[access-ci.org](https://access-ci.org)
 - ¬†[access-ci.org/about](https://access-ci.org/about)

# Allocations
 -  Allocations: [allocations.access-ci.org](https://allocations.access-ci.org)

# Documentation
 -  Support: [support.access-ci.org](https://support.access-ci.org)

# Community Engagement
-  ACCESS: [support.access-ci.org/affinity-groups](https://support.access-ci.org/affinity-groups)
-  Campus Champions: [https://campuschampions.cyberinfrastructure.org](https://campuschampions.cyberinfrastructure.org)
   - UVa Research Computing has two Champions, Ed Hall and Katherine Holcomb

{{% callout %}}
For more help, please feel free to contact RC staff to set up a consultation or visit us during [office hours](/support/#office-hours).
{{% /callout %}}"
rc-website-fork/content/userinfo/globus.md,"+++
author = ""RC Staff""
description = """"
title = ""Globus Data Transfer""
date = ""2023-01-24T10:08:29-05:00""
draft = false
tags = [""data-transfer"",""rivanna"",""storage"",""ivy"",""globus"",""dtn"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
+++

Globus is a simple, reliable, and fast way to access and move your research data between systems. 

Globus allows you to transfer data to and from systems such as:

Laptops & personal workstations
Rivanna/Afton HPC clusters
High-Security Research Standard Storage
Lab / departmental storage
Tape archives
Cloud storage
Off-campus resources (ACCESS, National Labs)

Globus can help you share research data with colleagues and co-investigators, or to move data back and forth between a lab workstation and Rivanna/Afton or your personal computer.
Are your data stored at a different institution? At a supercomputing facility? All you need is your institution's login credentials.
Getting Started
{{% callout %}}
Before you are able to transfer files from or to your personal laptop/workstation, you must set up a Globus Collection on that computer, aka your local endpoint. A collection can be a storage volume or specific file folder on

Your local workstation,
A departmental server,
A ""DTN"" (Data Transfer Node) connected to Rivanna, Afton, or High-Security Research Standard Storage, or
A server operated by another university or by a national computing center.

{{% /callout %}}
Create a Personal Collection for your laptop
{{% callout %}}
In order to transfer data to/from a lab or personal computer, you must
install the Globus Connect Personal application.
{{% /callout %}}

Open a browser and navigate to https://www.globus.org/globus-connect-personal. 
Select your operating system in the ""Install Globus Connect Personal"" section and click ""install now"". Follow the installation instructions to install Globus Connect Personal.
Launch the newly installed Globus Connect Personal. 
A box will appear and ask you to login. It will redirect you to a webpage where you will allow access and provide a label for your collection, i.e. your computer. We recommend using something very descriptive, such as mstk3-laptop or smith-genlab-workstation.

After clicking allow, a new page will pop up asking you to provide a collection name and a description. Again, use something descriptive like mstk3-laptop. Do not select the ""High Assurance"" checkbox.

Your collection is now set up and ready to use.

On Windows and Mac OSX, the agent will run in the background on your laptop or workstation and will restart when the machine is booted. Click on the agent icon (in the tray for Windows users, in the toolbar for macOS users) to change your preferences or to see the web console. On Linux you must start the agent manually upon rebooting.
Your local computer is now able to serve as a Globus Collection.
Check your new Collection
Globus transfers data between two ""collections"" or endpoints. You must log in to the Globus website to initiate any transfers.

Open a browser window to https://www.globus.org/ and click on Log In.

Select ‚ÄúUniversity of Virginia‚Äù from the drop-down list of organizations. You may also type the name into the textbox next to the down arrow.  Click Continue.
  


Next to you will be directed to sign in using UVA NetBadge. Once logged in you will see the File Manager page:
  

Click the Collection box and you should see your newly created collection

Transferring Files
{{% callout %}}
You can search for the collections to use for your transfer from the File Manager at the Globus website, then use their Web interface to initiate and monitor your transfers.
{{% /callout %}}
The official UVA managed collections are:

UVA Standard Security Storage - generally available; maps to Rivanna/Afton home directories, scratch, Research Standard & Research Project storage.
uva#ivy-DTN - available to Ivy secure platform users, for moving files into High-Security Research Standard Storage.
    Globus is the only permitted data-transfer protocol for highly sensitive data. To transfer data to High-Security Research Standard Storage, please see the special instructions here.

You can transfer files to or from your personal collection to a managed collection, one run either by UVA or by another institution.  You can transfer files between two managed collections.  You cannot transfer files from one personal collection to another personal collection.  If you wish to do this, contact Research Computing to convert at least one personal collection to a Globus Plus collection.
To transfer a file:


From the File Manager page, select a collection by clicking on the ""Collection"" link near the top of the screen (""start here, select a collection"").  Start typing the name of the collection to see the options containing the string as you type.



Once the collection is found, click on its link.  Wait while it finds your folders.  When complete, click on ""Transfer or sync to..."" on the right sidebar.  If you do not remember the exact name of the second collection, click the magnifying glass to search.  If your second collection is one you have registered with Globus, you may also click Your Collections.  This will open a second pane.  Either pane may be the source or destination.


Select a file or folder from your ""source"" pane.


Click the Start > or < Start button at the bottom of the pane to begin the transfer into the ""destination"" pane.



After you initiate a transfer, it will be assigned a Task ID that you can use to reference that specific transfer. This is a useful way of identifying transfers when checking on the status of a job or viewing your past Globus activity.
Your transfer may take several minutes or hours to complete depending upon the size of the data. Globus transfers are persistent, which means that if there is a network interruption, or one collection is turned off, the transfer will resume whenever the connection is restored.  The transfer takes place in the background, so once it is assigned an ID and you receive the notification that it has begun, you can log out from the Globus page.
The Globus Personal Connection application will show only a limited default set of paths on your computer.  If you need to use another folder, such as one on an external hard drive, as the source or destination, you will have to add it.  With the Globus Personal Collection running, click on the g logo in your taskbar or tray.  Mac: go to Preferences/Access.  Click the + button to add a path.  Windows: Options/Access, click + to add the path to the drive.  Navigate as usual to the location you wish to add.

Monitoring Transfer Activity
You can check on the status of your transfer using the Task ID.
From the lefthand navigation bar of the Globus Connect manager, click on ""Activity"". Or visit https://app.globus.org/activity

Here you will see a list of your current and past transfer jobs.  Click on a job and you will get details and status.

Notifications
Users are notified via email for both successful and failed transfers. The email looks something like this, and provides a URL for more information:
TASK DETAILS
Task ID: 7c0351b4-9c1c-11ed-a29d-8383522b48d9
Task Type: TRANSFER
Status: SUCCEEDED
Source: Gancayco Laptop (e6b14dc6-34a8-11ed-ba40-d5fb255a47cc)
Destination: UVA Standard Security Storage (e6b338df-213b-4d31-b02c-1bc2c628ca07)
Label: n/a
https://app.globus.org/activity/7c0351b4-9c1c-11ed-a29d-8383522b48d9/overview

Sharing Folders
{{% callout %}}
You can share folders to either specific individuals, or to groups that you create and manage within Globus. A group must be populated with at least one user.  A shared folder must be created on a managed collection or on a Globus Plus collection; personal collection can receive shared folders but cannot create shares.
{{% /callout %}}

Open the Globus web interface and log in using UVA Netbadge.
From the Transfer Files interface, log in to the UVA Standard Security Storage collection as described above.  
Navigate in the folder structure of that collection until you find the folder you want to share. Highlight it.
Next, select the Share link on the right side of the files window.
  
Globus regards shared folders as collections, so you must create a new collection to share the folder.  Clicking on Share allows you to ""Add a Guest Collection."" You can only create and manage guest collections for directories or files that you own and can access. Provide a Display Share Name (required) and a description (optional).
Click ""Create Share"". Your new share will be created.
  
Now click ""Add Permissions - Share With"" in the upper right. You must go through this even if you do not change permissions from the default.
Path - Leave this set to / since it refers to the path relative to the directory you are sharing from.
Share With - Decide whether you want to share with individual users or with a group. Please do not set this to ""All Users"" or ""Public"". If you share with an individual user, follow the instructions below. If you choose to share with a group, you will first need to create that and add users to it by using the GROUPS tab at the top of the page.
Identity/E-mail - You can look up other Globus users by searching for a part of their name or institution. If you cannot find the individual, you should contact them to make sure they have signed in to Globus at least once. Generally, users at other colleges and universities can be identified with the simple form of their email address, like mst3k@virginia.edu or jdoe@mit.edu, etc. Users who are unaffiliated with a university can still sign in to Globus using Google (identified as username@gmail.com) or by creating a username and password in Globus (identified as userid@globusid.org)
--If you enter your collaborator's email address, it must exactly match the one associated with the recipient's Globus ID.
Permissions - You can specify whether this user has access to read or write to your share.  Keep in mind that permission to write to the folder also grants the recipient the ability to delete files within in.


Add a message if you wish, then click ""Add Permission"" whether you made any changes or not.
  
Since a share is a Globus collection, to manage it see the Managing Endpoints section below.  You may delete the share to remove access, once your recipient has obtained the folder.

Managing Endpoints
{{% callout %}}
The Globus interface makes it easy to manage and delete your endpoints.
{{% /callout %}}
You can view your collections, including your shared collections or other collections that have been shared with you, by clicking on the Endpoints submenu at the File Manager page.

From this page you can see endpoints

That are shared with you
That are shareable by you
That are administered by you

Clicking on the name of each collection will allow you to review or modify settings.  You can modify only collections that are administered by you.  To modify or delete a collection, click the Administered By You tab, then click the endpoint you wish to manage.  You can edit its properties, open it, or delete it.

Security
{{% callout %}}
UVA permits faculty and researchers to manage data transfer and sharing with colleagues and collaborators themselves. However, with this ability comes the responsibility to share wisely and carefully. Therefore, we ask that you follow a few basic principles when you share data using Globus:
{{% /callout %}}

Grant the least permissions necessary, not the most. If a colleague needs only to retrieve your data files, then grant read-only permissions.
Grant access to specific individuals only, not to ""all users"" or to the public. These settings risk your information going to people and places that you have not designated, or being used in ways you do not control.
Remove shared collections as soon as they are no longer needed. It is a good practice to revisit your endpoints page periodically to clean up and to cull unused resources.
Finally, monitor and track your large file transfers. When someone is transferring large data sets to you, or you to them, monitor their progress and keep in touch with the person or group on the other end. This helps identify any unusual behavior.

For Advanced Users

Globus has a command-line interface.
Globus also has an API and Python SDK.
For other technical details, see Globus Documentation.
"
rc-website-fork/content/userinfo/r_updates.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2020-06-16T00:00:00-05:00""
title = ""R Updates: June 17, 2020""
url = ""/maintenance""
draft = false
tags = [""Rivanna""]
categories = [""userinfo""]
+++

During the June maintenance, we will make changes to R which will affect how your R programs run on Rivanna.  Below is a list of the changes and how they will affect your code.
1. The gcc-built versions of R will be updated to goolf-built versions.
Instead of loading gcc before loading R, you will need to load goolf or gcc openmpi.  For example:  module load goolf R/4.0.0. 
Remember to update any Slurm scripts that have module load gcc R or module load gcc R/3.x.x.  
2. The locations of the R libraries will be updated.
We are changing the locations of the R libraries (i.e., the folders where local packages are installed).  This change will create separate folders for different compiler versions of R, which will prevent package corruption.
As a result, R will not see the packages that you had installed before the maintenance.  (The only exception would be gcc openmpi R/4.0.0, which already uses the new library location).  You will need to reinstall your R packages.  To help with this effort, we are providing a script that will scrape the list of packages installed in an older library and will attempt to install these packages in the new library. Details are provided at ""New Libraries"".
3. The versions of R will be streamlined to 3.4.4, 3.5.3, 3.6.3, and 4.0.0.
If you had hard-coded another version of R in your scripts (e.g., R/3.6.1), you will need to update your scripts to specify one of these newer versions.
To see what modules would need to be loaded prior to loading R, you can use the module spider command (e.g.,module spider R/3.6.3)."
rc-website-fork/content/userinfo/user-guide.md,"+++
title = ""User Guides""
description = """"
author = ""Staff""
images = [
  """",
]
date = ""2017-01-19T09:55:56-05:00""
categories = [""userinfo""]
tags = [
  ""hpc"",
  ""ivy"",
  ""storage"",
  ""cloud"",
]
draft = false
quell_footer = true
+++




High Performance Computing
Standard and high security HPC to run your code, generally written in R, Python or shell scripts.


Get Started ‚Ä∫






Secure Computing
Secure virtual machines and interactive notebooks for processing HIPAA and other highly sensitive data.


Get Started ‚Ä∫






Storage
Need large, or extremely large storage offsite or on grounds? Can you count in GB, TB, or PB? Learn more about storage options and pricing.


Get Started ‚Ä∫








Cloud
Have an idea you'd like to test? Need an environment provisioned in short-order? We can help you build in the AWS cloud.


Get Started ‚Ä∫






Image Analysis
Do you have a large imaging dataset to process? Do you want to automate your image processing pipeline? Learn more about tools and techniques to speed up your workflow. 
      


Get Started ‚Ä∫






Data Transfer
Sometimes you have the right data, but in the wrong place. There are several paths available for researchers depending upon the size and destination of your data.


Get Started ‚Ä∫



"
rc-website-fork/content/userinfo/systems.md,"+++
author = ""RC Staff""
description = """"
title = ""Computing Systems""
date = ""2025-01-15T10:08:29-05:00""
draft = false
tags = [""compute"",""rivanna"",""ivy"",""cloud"",""aws"",""hpc"",""containers""]
categories = [""userinfo""]
images = [""""]
+++
UVA Research Computing can help you find the right system for your computational workloads.
From supercomputers to HIPAA secure systems to cloud-based deployments with advanced infrastructure,
various systems are available to researchers.
{{< systems-boilerplate >}}




High Performance Computing - Rivanna and Afton
A traditional high performance cluster with a resource manager, a large file system, modules, and MPI processing. 
Get Started with UVA HPC








Secure High Performance Computing - Rio
A high performance cluster designed to process and store secure data accessed via ivy dedicated virtual machines.
Get started on Rio








Secure Computing for Highly Sensitive Data - Ivy
A multi-platform, HIPAA-compliant system for secure data that includes dedicated virtual machines (Linux and Windows), JupyterLab Notebooks, and Apache Spark.
Get started on Ivy







Secure Computing for Sensitive Data - ACCORD

A web based platform for researchers from different colleges and universities to collaborate, analyze, and store their sensitive data in a central location. ACCORD supports RStudio, JupyterLab, and Theia Python.
Get started on ACCORD








Virtual Machines - Public/Private Cloud

        Cloud-based computing solutions are also available in Amazon Web Services, Google Cloud Platform, and our private cloud UVA Skyline. All three provide options for quick deployments, short-term lifecycles, and unique requirements such as GPUs or clusters.
        
Learn more







Container Services


        Deploying software in containers has grown increasingly useful for research scenarios. Containers are portable, distributable,
        allowing developers to include both code and dependencies (libraries, modules, etc.) in a single bundle. We operate
        a Kubernetes cluster to support research-oriented microservices.
        
Learn more







NSF ACCESS-CI


        The Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support (ACCESS) is an NSF-funded virtual organization that integrates and coordinates the sharing of advanced digital services - including supercomputers and high-end visualization and data analysis resources - with researchers nationally to support science.
        
Learn more



"
rc-website-fork/content/userinfo/microservices.md,"+++
author = ""RC Staff""
description = """"
title = ""Container Services""
date = ""2023-02-23T23:59:16-05:00""
draft = false
tags = [""compute"",""containers"",""hybrid"",""infrastructure"",""docker"",""kubernetes"",""api"",""k8s"",""software-development""]
categories = [""userinfo"",""containers""]
images = [""""]
+++



  Container-based architecture, also known as ""microservices,"" is an approach to designing and running applications as a distributed set of components or layers. Such applications are typically run within containers, made popular in the last few years by Docker.


  Containers are portable, efficient, reusable, and contain code and any dependencies in a single package.
  Containerized services typically run a single process, rather than an entire stack within the same environment. 
  This allows developers to replace, scale, or troubleshoot portions of their entire application at a time.

{{< highlight >}}

General Availability (GA) of Kubernetes - Research Computing now manages microservice orchestration
with Kubernetes, the open-source tool from Google. New deployments are now launched directly within Kuberenetes.
¬ª Read about Kubernetes and user deployments.
{{< /highlight >}}
Microservices at UVA
Research Computing runs microservices in a clustered orchestration environment that automates the deployment and management of many containers easy and scalable. This cluster has >1000 cores and ~1TB of memory allocated to running containerized services. It also has over 300TB of cluster storage and can attach to project and value storage.
{{% highlight-danger %}}

UVA's microservices platform is hosted in the standard security zone. It is suitable for processing public or internal use data. Sensitive or highly sensitive data are not permitted on this platform. 
{{% /highlight-danger %}}

Basic Principles
1 Microservice architecture is a design approach, or a way of building things. Microservices can be considered the opposite of ""monolithic"" designs.
A few guiding design principles:

Separate components and services
Availability and resilience
Replaceable elements
Easily distributable
Reusable components
Decentralized elements
Easy deployment

Here's a talk given by Martin Fowler explaining the idea:
{{< youtube ""2yko4TbC8cI"" >}}

2 The easiest and most common way to run microservices is inside of containers.

We teach workshops on containers and how to use them. Browse the course overview for Building Containers for the HPC System at your own pace.
Docker provides an excellent Getting Started tutorial.
Users may inject ENV environment variables and encrypted secrets into containers at runtime. This means sensitive information does not need to be written into your container.


Uses for Research
Microservices are typically used in computational research in one of two ways:

Standalone microservices or small stacks - Such as interactive or data-driven web applications and APIs or scheduled task containers. Some examples:

Simple web container to serve Project files to the research community or as part of a publication.
      Reference APIs can handle requests based either on static reference data or databases.
      Shiny Server presents users with interactive plots to engage with your datasets.
      A scheduled job to retrieve remote datasets, perform initial ETL processing, and stage them for analysis.
    
Microservices in support of HPC jobs - Some workflows in HPC jobs require supplemental services in order to run such as relational databases, key-value stores or reference APIs.

Browse a list of recent UVA projects employing microservices.

Common Deployments




Service
Accessibility
Description





NGINX Web Server
Public
A fast web server that can run
        
Static HTML demo
Flask or Django apps demo
RESTful APIs demo
Expose Project storage demo





Apache Web Server
Public
An extremely popular web server that can run your static HTML, Flask or Django apps, RESTful APIs, or expose files stored in Project storage.



Shiny Server
Public
Runs R-based web applications and offers a dynamic, data-driven user interface. See a demo or try using LOLAweb



Recurring Tasks
n/a
Schedule or automate tasks or data staging using the language of your choice (bash, Python, R, C, Ruby).




Database Hosting
Research computing may be able to provide support for your database hosting needs. Please schedule a consultation request on our website. Follow the link here and fill out the form under ""Consultation Request"" on the right hand side of the page under ""All Forms"".
Service Eligibility & Limitations

To be eligible to run your microservice on our infrastructure, you must meet the following requirements:


Microservices and custom containers must be for research purposes only. We do not run production systems outside the scope 
of academic research support.
  Your container(s) must pass basic security checks. Containers may not contain passwords, SSH keys, API keys, or 
other sensitive information. There are secure methods for passing sensitive information into containers.
  If bringing your own custom container, it must be ready to go! Unfortunately, we cannot create custom containers for you 
unless it is part of a funded project.


Microservices may not run efficiently for all use cases. Some scenarios that cannot run successfully in containers include:


Services (apart from web-based services over HTTP/HTTPS) that need to be accessed from outside the HPC network.
  Services that require licensing, such as Microsoft SQL Server, MATLAB, etc.
  Services that require GPU to run.





Apptainer
Want to run your container within an HPC environment? It can be done, using Apptainer! 
Apptainer is a container application targeted to multi-user, high-performance computing systems. It interoperates well with Slurm and with the Lmod modules system. It can be used to create and run its own containers, or it can import Docker containers.
Learn more about Apptainer.

Next Steps
Have a containerized application ready for launch? Or want a consultation to discuss your microservice implementation?
Request Access ¬†¬† {{< consult-button >}}"
rc-website-fork/content/userinfo/lab-computing.md,"+++
title = ""Lab Computing""
draft = true
tags = [""storage"",""data"",""value"",""legacy"",""migration""]
categories = [""userinfo""]
images = [""""]
date = ""2017-03-03T10:06:40-05:00""
author = ""RC Staff""
description = """"
+++
Lab Computing services are now available to research groups in the UVA School of Medicine to assist in providing the 
best in data storage for this specific environment.

These services include:

# Data Migration & Storage

Assist in migration of data from legacy and non-managed data storage equipment to managed infrastructure storage on 
UVA Secured and Non-Secured networks. Ensure that labs, workers and managers have full access to their
data from all locations in a secure and stable environment.

Data storage options include:

* ES1 Data Storage
* ES3 Secured Data Storage
* Research Standard Storage (Option for Object Storage Backup)

- - -

# Training & Tools

Train researchers in the use of data storage migration and management tools. These tools are a combination of 
tested Open Source technologies and long-used tools.

These tools provide for:

* Synchronized and itemized data transfers for file syncs of live data
* Multithreaded rapid moves of data from Lab Storage to Managed Central Storage options
* Legacy platform data migration from older operating system platforms
* Data storage comparison to ensure all data copies have been tallied between multiple points

- - -

# Requirements & Consultation

As part of these services, we also provide an initial consultation to ensure that the data and laboratory requirements are met on
all levels. There is more to data migration than just uploading data.

Services may also include:  

* Retirement of outdated legacy equipment   
* Support the regular inventories of data, hardware and software
* Purchase and procurement of future storage options in line with long-term budgeting strategies

- - -

These resources help ensure that your lab data is protected and secured in compliance with ISPro, Grant and Security requirements,
and that your equipment, data management and software application needs are being met in a managed, professional manner."
rc-website-fork/content/userinfo/secure-computing.md,"+++
title = ""Secure Computing""
description = """"
date = ""2017-01-20T13:47:27-05:00""
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""support"",
  ""userinfo""
]
tags = [
  ""hipaa"",
  ""security"",
]
draft = true
+++
UVA School of Medicine Research Computing can assist medical researchers in both understanding what HIPAA compliance requires of their work, and how to implement technical solutions to achieve and verify such compliance.
Review & Assessment
From a compliance perspective, RC offers three levels of review/assessment:


Security Plan - Having a security plan in place is important to your success at UVA School of Medicine. If your lab or department doesn‚Äôt have a plan in place Research Computing Information Security will be happy to help you develop such a plan.


Security Review - If you have a security plan in place already we can help you verify that your computing resources are functioning as documented in your security plan. As part of security review, we will do a risk analyst and provide you a list of recommended enhancements.


Risk Assessment - This is the systematic process of evaluating the potential security risks/hazards and any business impact they could present.  We analyze the likelihood of events occurring. As we conduct a risk assessment we look for vulnerabilities and weaknesses that could your system more susceptible to an event. We will provide list of finds and either work with your local support person or other resources to help mitigate the risk.  



Implementation
For implementation, RC offers a number of skills and services:

PHI and De-identification
Encryption best practices
Encryption of files, databases, and systems


PHI & De-Identification
Some research data are ""de-identified"" in order to remove them from HIPAA security requirements. This is particularly useful when researchers are processing across many hundreds or thousands of patients for trends and statistically meaningful insights that do not rely upon patient-specific data points.
Here are some common examples of PHI research data that may need to be de-identified. De-identification can mean the complete removal of such fields from your dataset, or the complete replacement of these data with meaningless placeholder values.

Names
All geographic subdivisions smaller than a state, including street address, city, county, precinct, ZIP code, and their equivalent geocodes, except for the initial three digits of the ZIP code if, according to the current publicly available data from the Bureau of the Census:

  1. The geographic unit formed by combining all ZIP codes with the same three initial digits contains more than 20,000 people; and

  2. The initial three digits of a ZIP code for all such geographic units containing 20,000 or fewer people is changed to 000

All elements of dates (except year) for dates that are directly related to an individual, including birthdate, admission date, discharge date, death date, and all ages over 89 and all elements of dates (including year) indicative of such age, except that such ages and elements may be aggregated into a single category of age 90 or older
Telephone numbers
Vehicle identifiers and serial numbers, including license plate numbers
Fax numbers
Device identifiers and serial numbers
Email addresses
Web Universal Resource Locators (URLs)
Social security numbers
Internet Protocol (IP) addresses
Medical record numbers
Biometric identifiers, including finger and voice prints
Health plan beneficiary numbers
Full-face photographs and any comparable images
Account numbers
Any other unique identifying number, characteristic, or code, except as permitted as a ""re-identifier""
Certificate/license numbers


Resources

HIPAA for Professionals & Providers (HHS)
University of Virginia Information Security
"
rc-website-fork/content/userinfo/use-cases.md,"+++
author = ""RC Staff""
description = """"
title = ""Use Cases""
draft = true
date = ""2017-03-03T10:42:09-05:00""
tags = [""ivy"",""rivanna"",""cloud"",""genomics"",""bioinformatics"",""data""]
categories = [""userinfo""]
images = [""""]
+++

Web Distributions
Donec fermentum eu tortor eu dignissim. Curabitur elit diam, tempor in dui non, tincidunt rhoncus risus. Praesent pharetra nisl elit, vitae commodo odio rutrum et. Praesent ac ligula pharetra, mollis lorem tristique, convallis leo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nulla facilisi. Cras sit amet euismod elit, et iaculis ipsum. Fusce aliquet mauris sit amet elit euismod, in varius justo suscipit.
Genomics Pipelines
Donec fermentum eu tortor eu dignissim. Curabitur elit diam, tempor in dui non, tincidunt rhoncus risus. Praesent pharetra nisl elit, vitae commodo odio rutrum et. Praesent ac ligula pharetra, mollis lorem tristique, convallis leo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nulla facilisi. Cras sit amet euismod elit, et iaculis ipsum. Fusce aliquet mauris sit amet elit euismod, in varius justo suscipit.
Data Processing
Morbi in nisl est. Nam accumsan elementum semper. Vivamus enim nunc, fringilla at nibh in, ornare mattis eros. Curabitur volutpat sapien at risus convallis finibus sed sit amet urna. Mauris nec nunc sit amet dui interdum commodo. Curabitur vitae mollis mauris. Aenean nec scelerisque neque, ac vehicula quam. Nulla viverra metus eget eros maximus, et dignissim ipsum egestas. Suspendisse mi nisi, efficitur ut nunc quis, consequat fringilla nibh. Integer in enim eget quam dignissim imperdiet. Curabitur placerat suscipit augue quis lobortis.
Machine Learning
Nam accumsan elementum semper. Vivamus enim nunc, fringilla at nibh in, ornare mattis eros. Curabitur volutpat sapien at risus convallis finibus sed sit amet urna. Mauris nec nunc sit amet dui interdum commodo. Curabitur vitae mollis mauris. Aenean nec scelerisque neque, ac vehicula quam. Nulla viverra metus eget eros maximus, et dignissim ipsum egestas. Suspendisse mi nisi, efficitur ut nunc quis, consequat fringilla nibh. Integer in enim eget quam dignissim imperdiet. Curabitur placerat suscipit augue quis lobortis.
Batch Analysis of Bulk Data
Proin non volutpat diam, in imperdiet nulla. Sed quis lobortis elit, at lobortis turpis. Curabitur at eleifend ipsum. Etiam egestas consectetur massa nec dictum. Pellentesque feugiat ipsum massa, sed lobortis enim sodales nec. Aliquam erat volutpat. Nunc lobortis quam sit amet tellus interdum aliquam. Integer urna nisi, tempus vel fringilla a, dapibus non sapien.
Alexa Voice Skills / Natural Language Processing

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin tincidunt viverra elit vitae mollis. Donec ipsum erat, ornare id suscipit non, lobortis in orci. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed ac ante eget purus ultrices cursus. Vivamus pretium erat in mattis feugiat. Nullam accumsan dignissim erat non auctor. Quisque elementum faucibus lacus pretium pretium. Mauris luctus, sapien id suscipit semper, eros ipsum fringilla odio, in scelerisque diam sem a libero. Ut accumsan non nibh in gravida. Pellentesque non ornare ipsum. Sed sed tellus eu arcu consectetur convallis. Aenean feugiat turpis id ex pretium ornare. Morbi sed odio sodales lorem tempus egestas ac at magna."
rc-website-fork/content/userinfo/tools.md,"+++
author = ""RC Staff""
description = """"
title = ""Tools for Research""
date = ""2018-10-30T10:08:29-05:00""
draft = false
tags = [""rivanna"",""lola"",""bart""]
categories = [""userinfo""]
images = [""""]
+++
Tools and software projects that UVA Research Computing has collaborated on:




LOLAweb

        LOLAweb is a web server and interactive results viewer for enrichment of overlap between a user-provided query region set (a bed file) and a database of region sets. It provides an interactive result explorer to visualize the highest ranked enrichments from the database. LOLAweb is a web interface to the LOLA R package.
        
Launch LOLAweb







BARTweb

        There are a number of commercially licensed tools available to UVa researchers for free. These products, including UVa Box, Dropbox (Health System) and CrashPlan, are most suitable for small-scale storage needs.
Learn more


"
rc-website-fork/content/service/collaboration.md,"+++
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""resources"",
]
tags = [
  ""collaboration"",
  ""staff"",
]
draft = true
date = ""2017-05-19T09:15:36-05:00""
title = ""Collaboration""
+++
The RC is available for collaborative work with researchers, faculty, and students. These may include:

Code and package development, such as R and Python packages, Matlab toolboxes, pipelines, or containers.
Open source initiatives.
Ongoing analysis of large data sets and publication of results.
Architecture / design best practices for new research approaches.
"
rc-website-fork/content/service/acknowledgement.md,"+++
title = ""Acknowledgement in Publications""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""userinfo"",
]
date = ""2019-04-17T20:58:21-05:00""
tags = [
  ""publications"",
  ""grants"",
  ""acknowledgement"",
  ""citations"",
  ""rc"",
]
draft = false
+++
Recognition and documentation of the contribution that Research Computing‚Äôs systems and support play in breakthrough research is essential to ensuring continued support for and availability of cutting-edge computing resources at The University of Virginia. Please cite UVA Research Computing in any research report, journal article, or other publication that requires citation of an author‚Äôs contributions.
Suggested format:
{{% callout %}}
The authors acknowledge Research Computing at The University of Virginia for providing computational resources and technical support that have contributed to the results reported within this publication. URL: https://rc.virginia.edu
{{% /callout %}}"
rc-website-fork/content/service/map.md,"+++
categories = [""services""]
type = ""full-width""
images = []
description = """"
title = ""Service Map""
author = ""RC Staff""
tags = [
  ""hpc"",
  ""rivanna"",
  ""ssz"",
  ""hsz"",
  ""hipaa"",
  ""secure"",
  ""microservices""
]
date = ""2022-03-08""
draft = true
toc = true
private = true
+++


From a security perspective, research data generally breaks down into two types: Moderately Sensitive and Highly Sensitive. Each must be handled appropriately. For the purposes of computing environments, that means conducting your research in the right security zone.


Blue Zone
Orange Zone





Moderately Sensitive Data



HPC - Rivanna and Afton, our high performance clusters. Includes over 12k cores across 
          Storage
          Virtual Machines
          Microservices
        






Highly Sensitive Data



HPC
          Storage
          Virtual Machines
          Microservices
        











HPC


                      Computing with over 12k cores.
                    







Storage


                      All your data are belong to us.
                    









Virtual Machines


                      Virtual Machines for all your data.
                    







Microservices


                      Microservices for all your data.
                    




"
rc-website-fork/content/service/high-performance-computing.md,"+++
categories = [
  ""services"",
  ""hpc"",
]
tags = [
  ""hpc"",
  ""ivy"",
  ""rivanna"",
]
draft = false
date = ""2025-01-15T09:30:12-05:00""
title = ""High Performance Computing""
description = """"
author = ""RC Staff""
images = [
  """",
]
+++
Research Computing supports all UVA researchers who are interested in writing code to address their scientific inquiries. Whether these programming tasks are implemented interactively, in a series of scripts or as an open-source software package, services are available to provide guidance and enable collaborative development. RC has specific expertise in object-oriented programming in Matlab, R, and Python.
Examples of service areas include:

Collaborating on package development
Reviewing and debugging code
Preparing scripts to automate or expedite tasks
Developing web interfaces for interactive data exploration
Advising on integration of existing software tools


UVA has four local computational facilities available to researchers: Rivanna, Afton, Rio and Ivy. Depending upon your use case, privacy requirements, and the application(s) you need to run, we can help you create an account and start processing your data.


Afton
Standard Security HPC Cluster

      {{< get_allocation_blurb name=""Afton"" >}}
    
Read more about Afton




Rivanna
Standard Security HPC Cluster

      {{< get_allocation_blurb name=""Rivanna"" >}}
    
Read more about Rivanna




Rio
High-Security HPC Cluster

Rio is one of the recent University of Virginia‚Äôs High-Performance Computing (HPC) systems, specifically designed for the processing and analysis of controlled-access and highly sensitive data. Currenlty, Rio consists of 39 HPC nodes, each equipped with 375 GB of RAM and offering a combined total of 1,560 x86 64-bit compute cores. Researchers can use Rio to process and store sensitive data with the confidence that the environment is secure and meets HIPAA and FERPA requirements.     

Read more about Rio




Ivy
High-Security / HIPAA Computing Environment

    Ivy is a secure computing environment for researchers consisting of virtual machines (Linux and Windows) backed by a total of {{< ivy-node-count >}} nodes and approximately {{< ivy-core-count >}} cpu cores. Researchers can use Ivy to process and store sensitive data with the confidence that the environment is secure and meets HIPAA, FERPA, CUI or ITAR requirements.
    
Read more about Ivy

"
rc-website-fork/content/service/grant-support.md,"+++
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""grants"",
  ""funding""
]
tags = [
  ""grants"",
  ""support"",
]
draft = true
title = ""Grant Support""
date = ""2017-01-19T09:15:24-05:00""
description = """"
author = ""RC Staff""
+++
RC is interested in participating in funding proposals related to advanced computing efforts. If you have a project or grant application in mind, feel free to contact us for more information."
rc-website-fork/content/service/imaging.md,"+++
categories = [""services""]
images = []
description = """"
title = ""Image Processing & Scientific Visualization""
author = ""Christina Gancayco""
tags = [
  ""vizlab"",
  ""imaging"",
  ""image-processing"",
  ""radiology"",
]
date = ""2019-06-24""
draft = false
toc = true
+++

Image Processing and Scientific Visualization are two separate processes within the scientific research lifecycle, yet the two concepts often play off of one another. Image processing refers to the enhancement and transformation of images to prepare them for quantitative analysis. Scientific visualization is the graphical communication of data so that trends and anomalies can be more easily recognized. UVa Research Computing offers many services and resources to help researchers augment their work with image processing and scientific visualization techniques.


Image Processing
Overview

Image processing encompasses a variety of techniques to prepare images for analysis. Researchers often need to remove noise artifacts from their imaging data, or they need to analyze particular regions of interest. While manual image manipulation can easily yield the desired results, this can be time-consuming or even impossible with the amount of data we are able to collect with high throughput screening. By automating image processing steps such as noise filtering and segmentation, researchers are able to perform their work faster and for larger quantities of data.
Common Image Processing Techniques
The following techniques are commonly employed in imaging research. All of these processes can be automated and run locally on your computer or on UVa's high performance computing (HPC) cluster. With the parallelization capabilities of HPC, it is possible to fully process and analyze a large imaging data set in a few hours or less!



Preprocessing


                Image preprocessing can help enhance the quality of your images. Common preprocessing techniques include adjusting brightness and contrast, removing noise, sharpening images, and performing geometric and color transformations.
            


Segmentation


                Image segmentation is useful for determining one or multiple regions of interest. Segmentation can be used to identify foreground objects, cell boundaries, or tissue types.
            


Registration


                Image registration is useful when comparing two or more objects of differing size or morphological features. Registration can be used to align 2D or 3D images through linear or non-linear algorithms.
            


Analysis

                Image analysis is the measurement and statistical analysis of meaningful features in your imaging data, such as area or volume of a region of interest and mean intensity value throughout an image.
            



Popular Software


ImageJ/Fiji - ImageJ is a Java-based image processing program developed at the NIH. ImageJ can be used interactively through a graphical user interface or automatically with Java. Fiji is ImageJ with common plugins pre-installed for scientific image analysis.


MATLAB - Matlab is a numerical computing environment with its own proprietary programming language. Matlab provides an extensive Image Processing Toolbox for with built-in functions for image registration, segmentation, and analysis.


Python - Python is a powerful high-level programming language for general purpose programming. There are several open source packages available in Python for image processing, including: OpenCV, scikit-image, and Python Imaging Library.


ANTs - ANTs, or Advanced Normalization Tools, is a state-of-the-art medical image registration and segmentation toolkit. ANTs works in conjunction with Insight Toolkit(ITK) to read and visualize multidimensional imaging data.


R - R is an open source programming language and computing environment for statistical analysis and data visualization. There are a variety of R packages available for image processing, such as ANTsR, EBImage, and magick.


Additional Resources
We currently offer an online short course for image processing with Fiji/ImageJ.

Introduction to Image Processing with Fiji/ImageJ

Stay tuned for additional online tutorials as well as in-person workshops listed on our workshops page

Visualization
Overview
Visualization is the conversion of data into plots or images in order to view various features of the data. As humans, we are able to absorb large amounts of information through sight. We can use visualizations as an exploratory tool to gain insight into the data we collect and to create hypotheses for relationships. We can also use visualizations to communicate ideas to others.
Popular Software


MATLAB - MATLAB contains many built-in functions for data visualization, including those for 3D surfaces and meshes. MATLAB is also capable of medical image visualization and is compatible with DICOM and NIFTI filetypes.


ParaView - ParaView is an open-source application for visualization and analysis of data defined on meshes or grids. It allows for visualization of 2D or 3D data and is good for general purpose, rapid visualization.


VisIt - VisIt is software for the visualization of data defined on meshes or grids. It is compatible with file types that have an underlying HDF5 format.


Blender - Blender is a 3D graphics software that can be used for creating 3D objects and animations. It can be used for 3D modeling, rendering, motion tracking, and video editing.


Unity - Unity is a cross-platform software application for the creation of visualizations in augmented and virtual reality.


Additional Resources
We currently offer several online tutorials for data visualization.

MATLAB Data Processing and Visualization

Stay tuned for additional online tutorials as well as our workshops posted on our workshops page


"
rc-website-fork/content/service/status.md,"+++
title = ""Service Status""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""service"", ""status""
]
date = ""2024-07-01T00:00:00-00:01""
tags = [
  ""service"",
  ""status"",
  ""storage"",
]
draft = false
+++
Project Storage Data Migration
The storage hardware serving the Research Project Storage file system has been experiencing intermittent disruptions since October 2023. In response, we acquired and installed new, upgraded hardware offering faster, more reliable access and capacity.‚ÄØ 
However, because an Active File Management (AFM) connection between the old and new systems was used to facilitate automatic file transfers, the new system‚Äôs performance is being negatively impacted by the old hardware and causing accessibility issues for some users.  
To mitigate these issues, Research Computing engineers are switching to an alternate backend migration process on February 26, 2024, at 9:00 a.m. 
IMPORTANT: This action will make data that have not yet been transferred appear to have vanished from the new Project storage file system. Please be assured that all data will remain secure and intact throughout the migration process. Rivanna and RC‚Äôs other storage services, Scratch and Research Standard, continue to operate normally. 
{{% callout %}}
Key Points:

An alternate method of transferring data from old Research Project storage to new Research Project storage will be implemented on 2/26/24 at 9:00 a.m. EST.
All data on old Research Project and new Research Project are secure and intact.
Starting February 26, data will be transferred by RC staff to a new read-only share /stagedproject on the new storage system. Files that have already been migrated to  /project remain intact. 
Though the new Project storage system is operating with expected performance, the transfer of all data from the old storage system will take several months. The severe performance degradation of the old storage system will remain a bottleneck regardless of the change in data transfer method.
Rivanna and RC‚Äôs other storage services, Scratch and Research Standard, continue to operate normally.

Update: 2024-07-01 
- Before February 26: A total of 1.7 PB out of 4.3 PB were copied from old Project storage to /project folder on the new storage system using the automated migration process before February 26 (40%).
- Since February 26: 100% of the data from old /project has been copied and is now available in the /stagedproject or /project folders on the new storage system.
{{% /callout %}}
{{% highlight %}}
Do you have additional questions? 
Please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m. starting March 6.
{{% /highlight %}}
Incident Response
Research Computing will reach out to all known users of this storage system with instructions for accessing data before and after February 26, and for assistance prioritizing files for transfer.  
{{% accordion-group title=""Email Communications"" id=""commgroup"" %}}
{{% accordion-item title=""Email Communications"" id=""emails"" %}}
{{% accordion-group title=""Emails"" id=""emailgroup"" %}}
{{% accordion-item title=""Apr 22, 2024 - Project Storage Update"" id=""email-3"" %}}
Dear Research Project storage user:
We are pleased to report that our new Project storage filesystem is performing as expected.
A majority of the data on the legacy Project filesystem has been successfully copied to /stagedproject on the new system, enabling researchers to access their data via /stagedproject or /project. We are e-mailing individual share owners as soon as their groups‚Äô data have been transferred. The /stagedproject shares were provisioned as a temporary accommodation free of charge to expedite file transfers to the new storage system while allowing active work in /project.
**Please note: If you don‚Äôt have a /stagedproject share, no further action will be required. Otherwise users will need to consolidate all of their files from /stagedproject to /project storage. Step-by-step instructions are available on the RC website (‚ÄúHow can I consolidate my files in /stagedproject and /project?‚Äù). If you need assistance with consolidating your files, you may reach out to us during office hours or contact our user services team. 
In addition, monthly billing for Research Project storage quotas will resume on May 1st. Billing was suspended in October 2023 due to the filesystem‚Äôs performance issues. These bills will be based on quotas on the ‚Äònew‚Äô /project storage space which has been working with expected performance for the past 2 months. Usage on /stagedproject and the legacy system will not be charged. Billing questions should be directed to RC_Billing@virginia.edu.
Detailed documentation on the Project storage incident, including previous email communications and frequently asked questions, is available on our Data Migration status page. We are committed to working diligently until the data transfer is complete and the legacy system is decommissioned. Our technical support teams will continue to be available to you to answer questions and address any concerns.
With regards,
Karsten Siller
Director, Research Computing User Services
Information Technology Services
University of Virginia
{{% /accordion-item %}}
{{% accordion-item title=""Feb 22, 2024 - Reminder: Upcoming Changes to Data Transfer Process for Project Storage File System"" id=""email-2"" %}}
Dear Colleagues,‚ÄØ 
This email serves as a friendly reminder of the upcoming changes to the Research Project Storage data transfer process that take effect on Monday, February 26 at 9 a.m. EST. For your convenience, a copy of the initial announcement that was released on February 16 is included below.   
You can find detailed documentation of the planned changes, previous email communications, and sections for frequently asked questions on our Data Migration status page (this page).  
We are committed to working diligently until data transfer is complete and the legacy system is decommissioned. Our technical support teams will continue to be available to you to answer questions and address any concerns.‚ÄØ 
Thank you for your continued patience and partnership.‚ÄØ 
With regards,‚ÄØ 
Karsten Siller
Director, Research Computing User Services
Information Technology Services
University of Virginia
{{% /accordion-item %}}
{{% accordion-item title=""Feb 16, 2024 - Upcoming Changes to Data Transfer Process for Project Storage File System"" id=""email-1"" %}}
Dear Colleagues,‚ÄØ 
As previously shared, efforts are still underway to transfer data in the Project Storage file system from the legacy GPFS hardware to the new, upgraded hardware. To date, we have successfully transferred about 35% of Project Storage files. These files were transferred because they were either needed for a scheduled research project, or they had been recently accessed. However, now we are finding that the Active File Management (AFM) connection being used to transfer files is causing too great of a system load for the legacy hardware, as the old system continues to degrade. This is causing additional disruptions and file access issues. Although there are issues with access, rest assured that all files remain safe and secure, and will be transferred to the new system by fall.‚ÄØ 
What we are doing now‚ÄØ 
On Monday, 2/26, we will move from the AFM connection to a new, manual process to transfer the remaining data. As part of this manual process, we have launched a high priority transfer request for files actively needed for your research. ‚ÄØAs part of this manual process, we are prioritizing files identified as actively needed for current research projects. If you need to access files on the legacy system for active project work, please indicate which directories or files should be prioritized for transfer using our data transfer request form. You can also use this form to request a list of your files that remain on the old system.  Please note that we cannot guarantee a timeline for transferring prioritized files due to the uncertainty of the old hardware. Prioritized file transfer may still take weeks to months to complete. 
If you have already reached out to prioritize file transfer or do not anticipate immediate use of these files, no further action is required.‚ÄØ 
What you may experience now until the transfer is complete 
Today, when you log in to Project Storage, you will see your complete file list in your directory. Although the file list is complete, it is possible that some files in your directory have already been migrated to the new hardware and are readily available for use, while others remain on the old cluster. Accessing files that remain on the old cluster will likely result in excessively slow access speeds or a ‚Äúfile not found‚Äù error. This ‚Äúfile not found‚Äù error only indicates that your file has not yet been transferred.‚ÄØ‚ÄØ 
On 2/26, we will move to the new, manual file transfer process. Because this process is manual and no longer based on the file connection method, file names of files not yet transferred to the new system will be removed from your /Project storage directory. These file names will automatically repopulate in your directory under a new /stagedproject folder as they are transferred to the new hardware.‚ÄØ‚ÄØ 
Additional information about the file transfer efforts and system status is available on our new Data Migration status page (this page). We will provide ongoing updates at this location.
I understand the impact these disruptions may have had on your work, and I share in your frustrations. Our mission is to provide excellent service and support for a seamless research computing experience.‚ÄØ‚ÄØ 
We are committed to working diligently until data transfer is complete and the legacy system is decommissioned. Our help desk and technical support teams will continue to be available to you to address any concerns.‚ÄØ 
Thank you for your continued patience and partnership.‚ÄØ 
With regards,‚ÄØ 
Joshua Baller, PhD
Associate Vice President for Research Computing
Information Technology Services
University of Virginia
{{% /accordion-item %}}
{{% /accordion-group %}}
{{% /accordion-item %}}
{{% /accordion-group %}}
What to expect on February 26
Before February 26, your /project folder contains a mix of files, including those that have already been transferred and those that still reside physically on the old Project storage system. Files that are still on the old system appear as empty ""stub files"" in the new system. Because the old and new systems are still connected, if you try to access a file that is still on the old system, the empty stub file is replaced by the original file as it is transferred on-demand to the new system.  
On February 26, the old and new Project storage systems will be disconnected. Researchers will not have any direct access to the old Project storage. The current /project folder on the new storage system should perform optimally without the tether to the old storage system. We will begin deleting the empty stub files on /project. These are empty files and are not needed for the new migration process. The original files are still intact and secure on the old system. 
A new filesystem /stagedproject will be mounted read-only on Rivanna login nodes, compute nodes, and the UVA Standard Security Storage data transfer node (DTN). This folder will be used as a target to stage your data as it is being transferred from the old system to the new system. Setting up a new destination for the not yet transferred files prevents potential interference with your active work in /project. 
Your Project storage folders on February 26:


/project/MY_SHARE: This is located on the new storage system. It contains files that have already been transferred since Fall 2023 as well as newly-created files. 


/stagedproject/MY_SHARE: This is a new share set up on the new storage system. It will be empty initially. Files will begin to appear here as they are transferred, starting Feb 26.  
‚ÄúMY_SHARE‚Äù refers to your personal project name


Note: The /stagedproject/MY_SHARE folder will only be created for you if you have folders/files on the old storage system that still need to be migrated. 
FAQ
{{% accordion-group title=""Group"" id=""faqgroup""%}}
{{% accordion-item title=""1. How should I prepare for the changes coming on February 26?"" id=""faq-1"" %}}
If you have already reached out to us to prioritize transfer of a specific subset of your folders or files, no further action is required. These files will be copied to same-named folder in your active /project share on the new Project storage system.  
If you have not yet contacted us with a list of priority folders or files to transfer, or if there are additional folders and files that you urgently need for your active work, please reach out to RC with a specific list of those folders/files and we will add them to the file transfer queue. See ""How can I get help with migration of my data?"" for details. 
Questions about the data migration process should be directed to our user services team. 
{{% /accordion-item %}}
{{% accordion-item title=""2. Some of my Project storage files disappeared. Where did they go?"" id=""faq-2"" %}}
Until February 26, all files are shown in the new Project storage system, including those that have already been transferred and those that are still on the old Project storage system. Files that are still on the old system present as empty stub files in the new system. When accessed for the first time, the empty stub file is replaced by the original file as it is transferred on-demand from the old system to the new system. 
On February 26, the empty stub files will be deleted from the new system as they are not needed for the new migration process. This is a gradual process that may take a few weeks to complete, so you may see different files, depending on when you access the new system. However, the original files behind the stub files still exist and are secure on the Old Project system. 
See ""How do I find out what files are on the old Project storage system?""
{{% /accordion-item %}}
{{% accordion-item title=""3. Where are you copying my files?"" id=""faq-3"" %}}
Until February 26, files that you have already requested be transferred will be copied to the same-named directories in your active /project share on the new Project storage system. 
Beginning February 26, all your files, including files that are still on the old system and files that have already been transferred to the new system, will start being copied to the same-named directories in a new /stagedproject share. The /stagedproject share was created for your to-be-migrated files to prevent potential interference with your active work in /project. Note: Your folder in /stagedproject will be empty on February 26, but will gradually fill with your files as they are copied over.
{{% /accordion-item %}}
{{% accordion-item title=""4. How do I access the new /stagedproject folder?""  id=""faq-4"" %}}
On February 26, a new /stagedproject folder will become available in read-only mode on the Rivanna login nodes and the UVA Standard Security Storage data transfer node (DTN). It will not be available on compute nodes. This folder will be used as destination to stage data transferred from your old Project storage to the new storage system. 
{{% /accordion-item %}}
{{% accordion-item title=""5. How can I work with the files that have been transferred into my /stagedproject folder?""  id=""faq-5"" %}}
On Feb 26, your folder in /stagedproject is set up as read-only on the Rivanna login nodes, compute nodes, and the UVA Standard Security Storage data transfer node (DTN).

Option 1 (preferred): 
For compute jobs we recommend you first copy files from /stagedproject into your /project or /scratch folder. For transfer of large folders see ‚ÄúHow can I consolidate my files in /stagedproject and /project?‚Äù. 
{{% highlight %}}
Note: A subset of files may not copy over because of existing ""stub files"" on the /project storage system. Stub files are ""placeholders"" for files that exist on the old project storage system but had not been copied over to the new project storage system. They are not needed for the new data migration process. We began with deletion of these empty placeholder stub files on February 26. This process is still ongoing. The original files are still intact and secure on the old system. 
{{% /highlight %}}
If you do not need any of the files affected by any failed copy operation immediately, you may continue to work out of /project and /scratch folders as usual. We will inform you when all stub files have been deleted and you may consolidate the remaining files from /stagedproject to /project then, following the copy instructions one more time. See ‚ÄúHow can I consolidate my files in /stagedproject and /project?‚Äù 
Option 2:
If the copy of any needed files to /project fails, you can update your job scripts to read the necessary input files from your /stagedproject folder and write the output to a new folder in your existing /project share. We will inform you when all stub files have been deleted and you may consolidate the remaining files from /stagedproject to /project then by following the copy instructions one more time, See ‚ÄúHow to consolidate files from /stagedproject to /project?‚Äù. 
{{% /accordion-item %}}
{{% accordion-item title=""6. Why can't I access the old Project storage system directly to copy my own files?"" id=""faq-6"" %}}
Performance of the old Project storage system is severely degraded. Any exploratory search for folders or file listings by users would create additional strain on the system, which would further reduce the already limited data transfer rates from the old to new Project storage system.
Because of these performance issues, RC set up a managed process that transfers all files from the old Project storage system to a new /stagedproject folder for you in the new system. See ‚ÄúHow do I access the new /stagedproject folder?‚Äù.
You can reach out to RC to request a list of your files on the old Project storage system. See ""How do I find out what files are on the old Project storage system?""
RC will work with you to prioritize the list of your files so that those files most urgently needed for your active work can be transferred first. See ""How can I get help with the migration process?""
{{% /accordion-item %}}
{{% accordion-item title=""7. How do I find out what files are on the old Project storage system?"" id=""faq-7"" %}}
After February 26, you will not be able to connect to the old Project storage system. See ""Why can't I access the old Project storage system directly to copy my own files?""
However, we have placed a list of your old Project storage files in the top-level folder of your new share on /stagedproject (i.e. /stagedproject/my_share/old-project-file-list.txt). You may use this list to prioritize folders and files for your data migration (see ""Can I pick which of my files are transferred first?"").
Please keep in mind that the list in old-project-file-list.txt represents all your files stored on the old Project storage system, some of which have already been transferred to the new system. Eventually all files will be transferred from the old to the new Project storage system. If you already have all the data needed for your active work on the new Project storage system, no action is required.
{{% /accordion-item %}}
{{% accordion-item title=""8. Are all files being transferred to the new Project storage system at once?""  id=""faq-8"" %}}
No. We are prioritizing transfer of files that you actively need for your research. You may reach out to us to provide a specific list of your high priority, essential folders and files. 
The severe performance degradation of the old storage system will remain a bottleneck regardless of the change in data transfer method. However, the more selective this list, the better we can help you with this transition. See ""Can I pick which of my files are transferred first?"" for details. 
Once the transfer process has been stabilized, engineers will begin transferring any remaining files that users did not explicitly request to be moved.
{{% /accordion-item %}}
{{% accordion-item title=""9. Can I pick which of my files are transferred first?""  id=""faq-9"" %}}
Yes. Please complete this web form to provide us with a list of specific, high priority folders and files for migration. Please indicate as precisely as possible which folders or files should be transferred first so our storage engineers can prioritize these items. The more selective this list, the better we can help you with the transition.  
If you need help with your file prioritization, you may reach out to RC to request a list of files that you still have on the old Project storage system. 
{{% /accordion-item %}}
{{% accordion-item title=""10. How can I get an estimate of when my files will be transferred?""  id=""faq-10"" %}}
Though the new Project storage system provides vastly improved performance, the overall transfer of files is limited by the degraded performance of the old system. This issue with the old storage system will remain a bottleneck regardless of the change in data transfer method. 
However, you can facilitate the data transfer process by providing us with a narrowed down list of files that you need for your research over the next few months. This will allow us to deprioritize less urgently needed files. 
See ""Can I pick which of my files are transferred first?"" for details.
All data will be migrated eventually, but this process is expected to take several months to complete. We will post weekly progress of data migration on this page. 
We will notify the PI of the storage allocation when all their folders have been copied over. We will not purge any files on the old Project storage system until the PI has had an opportunity to verify that their files have been migrated to the new Project storage system. 
{{% /accordion-item %}}
{{% accordion-item title=""11. Why is the transfer of folders to the new Project storage system taking longer than expected?""  id=""faq-11"" %}}
The old Project storage hardware (GPFS) is in a degraded state. Due to this state, transferring data from the system is unusually slow even in otherwise ideal conditions. With a wide range of researchers and research workflows attempting to simultaneously access the system, the system becomes overloaded resulting in the ‚ÄúIO Error‚Äù and similar errors that researchers have been experiencing. As the system gets overloaded the transfer rates drop even further.  
By having RC manage the data transfer from the old to the new system we will be able to limit the load on the system and keep it closer to ideal conditions in order to maximize the transfer rate going forward. 
{{% /accordion-item %}}
{{% accordion-item title=""12. What caused this issue with the old Project storage file system?""  id=""faq-12"" %}}
The old GPFS hardware was beyond its expected end-of-life and due to historical sporadic financial investment in RC the hardware was not replaced. That changed recently with substantial University investments in RC and replacement hardware was immediately purchased. Unfortunately, we were not able to get the new GPFS hardware up and running before we started to experience failures on the old GPFS hardware. Given that the hardware was already end-of-life, we prioritized completing installation of the new hardware quickly and transferring from the old hardware to the new. Because the old GPFS hardware is still experiencing failure, these transfers have been slow with ~35% of the data transferred to-date. 
{{% /accordion-item %}}
{{% accordion-item title=""13. What is RC doing to ensure increased reliability and improved service?""  id=""faq-13"" %}}
With recent University investments in Research Computing we have planned annual purchases of new hardware with a model that allows for the seamless addition of new hardware and decommissioning of old hardware as it reaches its end-of-life. This will reduce the need for downtimes and manual data migrations as we ensure the integrity of our infrastructure. 
Research Computing is also working to improve its approach to user communications. If a major disruption occurs to an RC system, we will be using pages like this one to make sure there is a single location researchers can go to see both the current status and the history of the incident. 
Looking forward, Research Computing‚Äôs service roadmap includes filling in some services currently missing from our portfolio. Critically, there should be options available for researchers to elect for redundant access to their key datasets.  This means not only additional levels of protection in case of a disaster but also an alternate location where data can be retrieved. 
{{% /accordion-item %}}
{{% accordion-item title=""14. How were files prioritized for transfer for the new system?""  id=""faq-14"" %}}
At this time, about 35% of files have been successfully migrated. These files were moved as part of prioritized research datasets or were transferred over by AFM when accessed by a researcher.‚ÄØ 
The prioritized datasets were known to be actively used and were prime candidates to transfer when extra transfer capacity was available. 
{{% /accordion-item %}}
{{% accordion-item title=""15. What are stub files and how can I find them?"" id=""faq-15"" %}}
Stub files are ""placeholders"" for files that exist on the old project storage system but had not been copied over to the new project storage system. They are not needed for the new data migration process. We began with deletion of these empty placeholder stub files on February 26. This process is still ongoing. The original files are still intact and secure on the old system. 
You may create a list of stub files currently present in your /project folder by running this command: 
find /project/MY_SHARE -ls > regular-files.log 2> stubfiles.log 
This produces two files, regular-files.log and stubfiles.log. The stubfiles.log contains all files that the system cannot list which is indicative of being stub files. 
{{% /accordion-item %}}
{{% accordion-item title=""16. Why do I get File Not Found Errors when accessing some of my files in my /project subfolders?"" id=""faq-16"" %}}
Stub files may be present which are placeholders that linked the new storage system to the legacy storage system. As a part of the data migration process, stub files linking to the legacy system were also attempted to be deleted. A subset of these stub files remains visible on the new system, but attempting to access them will result in File Not Found Errors, as they are no longer coupled with the old system. These files are scheduled for deletion through an automated process eventually.
{{% /accordion-item %}}
{{% accordion-item title=""17. How can I verify that all my old project storage files are now in /stagedproject?"" id=""faq-17"" %}}
To verify the list of files in /stagedproject you can run the following on the command line:
check_stagedproject MYSHARE
Replace MYSHARE with the name of your project's share. This will create ~/stagedproject-file-list.txt which contains a list of files that have been copied from /oldproject to /stagedproject. This list is compared with /stagedproject/MYSHARE/old-project-file-list.txt to ensure that all files have been transferred from /oldproject.
~/stagedproject-file-list.txt will change if your data transfer is still in progress. The share's owner will be notified once all the data is transferred.
{{% /accordion-item %}}
{{% accordion-item title=""18. How can I consolidate my files in /stagedproject and /project?"" id=""faq-18"" %}}
To organize your files efficiently: 


If you relocated files to new folders to avoid issues with file access performance issues, or if you have duplicates in project storage, please put them back in their original folders using the ""mv"" command. This keeps things neat and prevents duplicates. Be cautious not to overwrite newer files with older ones while moving them.


Use the ""rsync"" command to copy files from the /stagedproject folder to the main project folder (/project). This ensures that all essential files are consolidated in one location (/project). 


If you require assistance with these steps, please contact us via our support webform or during office hours for help. 
Submit the following script to copy large directories in bulk:
```
!/bin/bash
SBATCH -A your_allocation  # to find your allocation, type ""allocations""
SBATCH -t 12:00:00   # up to 7-00:00:00 (7 days)
SBATCH -p standard
STAGEDPROJECTFOLDER=/stagedproject/MYSHARE/      #replace MYSHARE  with your share name
PROJECTFOLDER=/project/MYSHARE/                  #replace MYSHARE with your share name
rsync -uav ${STAGEDPROJECTFOLDER} ${PROJECTFOLDER} 1> ~/rsync.log 2> ~/rsync-error.log
```
The script will also be available through the Open OnDemand Job Composer: 

Go to Open OnDemand Job Composer 
Click: New Job -> From Template 
Select demo-copy-stagedproject
In the right panel, click ‚ÄúCreate New Job‚Äù 
This will take you to the ‚ÄúJobs‚Äù page. In the ‚ÄúSubmit Script‚Äù panel at the bottom right, click ‚ÄúOpen Editor‚Äù 
Enter your own allocation. Edit the MY_SHARE placeholder in the script as needed. Click ‚ÄúSave‚Äù when done. 
Going back to the ‚ÄúJobs‚Äù page, select demo-copy-stagedproject and click the green ‚ÄúSubmit‚Äù button. 

As we expect a high volume of data migration, please refrain from doing so directly on the login nodes but instead submit it as a job via the provided Slurm script as described above. 
{{% callout %}}
If you have problems with errors in the above rsync command you can try adding the following flags: --no-owner --no-group --no-perms --no-times. E.g.
rsync -uav  --no-owner --no-group --no-perms --no-times ${STAGEDPROJECTFOLDER} ${PROJECTFOLDER} 1> ~/rsync.log 2> ~/rsync-error.log
{{% /callout %}}
{{% /accordion-item %}}
{{% accordion-item title=""19. How can I get help with the data migration process?""  id=""faq-19"" %}}
We have placed a list of your old Project storage files in the top-level folder of your new share on /stagedproject (i.e. /stagedproject/my_share/old-project-file-list.txt). You may use this list to prioritize folders and files for your data migration (see ‚ÄúCan I pick which of my files are transferred first?‚Äù).
Researchers with an urgent need to access files that have not been migrated to the new Project storage system yet may submit a support request through our webform. Please indicate as precisely as possible which folders or files should be transferred first so we can prioritize these items.
All files will be transferred eventually. 
{{% highlight %}}
Do you have additional questions? 
Please contact our user services team, or join us for our virtual office hours every Tuesday, 3-5 p.m. and Thursday, 10-12 p.m. starting March 6.
{{% /highlight %}}
{{% /accordion-item %}}
{{% /accordion-group %}}
Technical Details
On February 26, RC engineers will disconnect the Active File Management (AFM) tether, remount the old Project storage system (GPFS) separately, and purge all ""stub files"" (files that are staged on new storage system and appear in a directory listing, but have not yet been transferred from old Project). This will allow a clear differentiation between transferred and un-transferred data. Data already transferred to the new Project storage system are expected to perform optimally without any issues, while un-transferred data on the old system will need to be manually transferred by RC staff. Staff will continue to respond to data transfer requests on a first come, first served basis. Once the transfer process has been stabilized, engineers will begin transferring any remaining files that users did not explicitly request to be moved.
Incident Status Log
{{% scrollable height=""500px"" %}}


2024-02-27, 2:51 PM The /stagedproject folder is now available read-only on Rivanna login nodes.


2024-02-27, 7:03 AM About 33% of all stub files have been deleted on the new Project storage system. A subset of the stub files are still visible on the new system. Access of these stub files will produce File Not Found Errors since they don‚Äôt physically exist on the new storage system and are uncoupled from the old system now. All stub files will be deleted through an automated process eventually over the next days and weeks.


2024-02-26, 05:32 PM: The new Project storage was remounted on all Rivanna nodes and the UVA Standard Security Storage data transfer node (DTN). Deletion of stub files was initiated.


2024-02-26, 04:15 PM: The old and new Project storage systems were unmounted on all Rivanna nodes and the UVA Standard Security Storage data transfer node (DTN) to complete the disconnection process.


2024-02-26, 09:00 AM: Engineers began to disconnect the old and new Project storage systems.


2024-02-19, 02:00 PM
Project storage is currently unavailable on Open OnDemand. On 26 February at 9:00 am EST, RC engineers will switch to an alternate data transfer mechanism between the legacy Research Project Storage filesystem and the new Project Storage system. As a result, users will no longer have direct access to the legacy system. Files will be staged to an intermediate location for users to copy. To facilitate the migration process, please indicate which directories or files should be prioritized for transfer using our data transfer request form. Additional information about the file transfer efforts and Project Storage system status is available on our Data Migration status page. 


2024-02-16, 01:22 PM
On 26 February at 9:00 am EST, RC engineers will switch to an alternate data transfer mechanism between the legacy Research Project Storage filesystem and the new Project Storage system. As a result, users will no longer have direct access to the legacy system. Files will be staged to an intermediate location for users to copy. To facilitate the migration process, please indicate which directories or files should be prioritized for transfer using our data transfer request form. Additional information about the file transfer efforts and Project Storage system status is available on our Data Migration status page. 


2024-02-09, 06:22 PM
SMB/NFS exports have been enabled for new Project storage. Data migration from old Project storage to new Project storage is ongoing. First-time access of old files and old directory listings is significantly slower than normal. Users may encounter on occasion ‚ÄúOSError: [Errno 5] Input/output errors‚Äù which should be reported through our support webform https://www.rc.virginia.edu/form/support-request/. For their ongoing work users should create new Project storage directories that are as close to the top level directory of their storage share as possible. Directory listings and traversals in these new top level directories is expected to show better performance. 


2024-02-08, 08:05 AM
Rivanna is back in service following maintenance. Data migration from old Project storage to new Project storage is ongoing. First-time access of old files and old directory listings is significantly slower than normal. Users may encounter on occasion ‚ÄúOSError: [Errno 5] Input/output errors‚Äù which should be reported through our support webform https://www.rc.virginia.edu/form/support-request/. For their ongoing work users should create new Project storage directories that are as close to the top level directory of their storage share as possible. Directory listings and traversals in these new top level directories is expected to show better performance.


2024-02-07, 06:00 AM
Rivanna, Research Project storage, and Research Standard storage will be down for maintenance on Tuesday, February 6 beginning at 6 a.m. All systems are expected to return to service by 6 a.m. on Wednesday, February 7.


2024-02-05, 07:55 AM
Data migration from old Project storage to new Project storage is ongoing. First-time access of old files and old directory listings is currently significantly slower than normal. For their ongoing work users should create new Project storage directories that are as close to the top level directory of their storage share as feasible. Directory listings and traversals in these new top level directories is expected to show better performance. NFS and SMB mounts for new Project storage will be enabled on February 6. new Project storage will be made available through the Open OnDemand file browser at the same time.


{{% /scrollable %}}"
rc-website-fork/content/service/tiers.md,"+++
title = ""Support Services""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""userinfo"",
]
date = ""2025-04-16""
tags = [
  ""userinfo"",
  ""services"",
  ""consulting"",
  ""collaboration"",
  ""rc"",
]
draft = false
+++
We offer a variety of services, most of which are provided at no charge to the researcher. 
Training and Technical Support {#tier-1-training-technical-support}

Research Computing provides training for the use of our systems.  The training includes onboarding, introductory sessions, and computational workshops. We also provide customized group trainings which you can request here. 
We provide technical support to researchers by assisting with system access issues and troubleshooting computational problems on our systems.  To receive online technical support, researchers can submit a ticket.  We also have weekly  virtual office hours where researchers can speak to Research Computing staff.  Training and Technical Support are provided at no charge.

    {{< button button-url=""/form/support-request/"" button-class=""primary"" button-text=""Submit a Ticket"" >}}

Consultations: Advising

We provide guidance and recommend best practices on our systems in service areas such as data storage and code improvement, and can also discuss project feasibility. While a consultation does not include any hands-on work by Research Computing personnel, the outcome of a consultation may include a recommendation for a hands-on collaboration.  Consultations are provided at no charge.

    {{< button button-url=""/form/support-request/"" button-class=""primary"" button-text=""Request a Consultation"" >}}

Collaborations: Expertise and Custom Solutions

Research Computing has a team of computational experts specializing in a variety of fields, such as High-Performance Computing (HPC), bioinformatics, artificial intelligence (AI), image processing, and more. Whether you need assistance with project analysis or grant-related tasks, our experts are ready to collaborate and develop tailor-made solutions for your research needs.
Basic Collaborations
Our Basic Collaborations are designed for tasks that do not require significant time commitment from our team. For example, we may leverage our expertise to efficiently debug codes or rewrite scripts that parallelize your job. Typically, these collaborations involve less than 20 hours of work, with no more than 8 hours of effort required in any given week. We are pleased to offer this service at no charge to the researchers.
In-Depth Collaborations
We also offer comprehensive collaboration services, where our experts thoroughly examine your data, suggest appropriate analyses, streamline workflows, or create custom programs for data analysis. In special cases, we may design architectures to facilitate your workflows. These tailored solutions typically involve over 20 hours of effort from our team members. Because of the extensive time and specialized expertise required, we charge a fee depending on the skills and efforts required.

    {{< button button-url=""/form/support-request/"" button-class=""primary"" button-text=""Request a Collaboration"" >}}
"
rc-website-fork/content/service/bioinformatics.md,"+++
author = ""RC Staff""
description = """"
title = ""Bioinformatics & Genomics""
date = ""2023-02-23T09:48:06-05:00""
draft = false
tags = [""bioinformatics"",""genomics""]
categories = [""services""]
images = [""""]
+++
UVA Research Computing (RC) can help with your bioinformatics project.

Next-generation sequence data analysis
RC staff can help you start to use popular bioinformatics software for functions such as

Genome assembly, reference-based and/or de-novo
Whole-Genome/Exome sequence analysis for variant calling/annotation
RNA-Seq data analysis to quantify, discover and profile RNAs
Mircobiome data analysis, including 16S rRNA surveys, OTU clustering, microbial profiling, taxonomic and functional analysis from whole shotgun metagenomic/metatranscriptomic datasets
Epigenetic analysis from BSAS/ChIP-Seq/ATAC-Seq


Computing Platforms
UVA has three computing facilities available to researchers: Rivanna and Afton, for non-sensitive data, and Ivy, for sensitive data. In addition, cloud-based services offer a computing environment for running flexible, scalable on-demand applications. RC can work with your team to determine the computing platform best suited for your research project.  


Rivanna and Afton
High-performance Computing Clusters

    Rivanna and Afton are the university's high-performance computing systems for high-throughput multithreaded jobs, parallel jobs, and memory intensive large-scale data analyses. The architecture is specifically suited for large scale distributed genomic data analysis, with many 100+ bioinformatics software packages installed and ready to use.   
    
Learn more





Ivy
High-Security / HIPAA Computing

    Ivy is a HIPAA and CUI compliant cluster at UVA for working with sensitive data. Researchers have access to a group of bioinformatics software on Ivy Linux VMs.  
    
Learn more





Cloud Computing

    We can explore the possibility of using cloud infrastructure (AWS/GCP) for your bioinformatics analysis and data storage. For certain applications, the 'elasticity' of cloud computing may prove beneficial for saving time and reducing costs of data analysis and sharing. The RC team is available for consultation on your project needs.      
    
Learn more




Consulting
We can assist with choosing the right package, developing a workflow, porting it to one of our computing platforms, and running the jobs.
Request a Consultation
{{% top-of-page %}}"
rc-website-fork/content/service/cloud.md,"+++
categories = [""architecture""]
tags = [
  ""aws"",
  ""google"",
  ""cloud"",
  ""iot"",
  ""containers""
]
draft = false
date = ""2023-02-23T14:18:18-05:00""
title = ""Cloud Solutions""
description = """"
author = ""Staff""
images = [
  ""/2016/10/image.jpg"",
]
aliases = [""/cloud""]
+++

Run your



Cloud computing is ideal for running flexible, scalable applications on demand, in periodic bursts, or for fixed periods of time. UVA Research Computing works alongside researchers to design and run research applications and datasets into Amazon Web Services, the leader among public cloud vendors. This means that server, storage, and database needs do not have to be estimated or purchased beforehand ‚Äì they can be scaled larger and smaller with your needs, or programmed to scale dynamically with your application.

Service Oriented Architecture
A key advantage of the cloud is that for many services you do not need to build or maintain the servers that support the service -- you simply use it.
Here are some of the building blocks available using cloud infrastructure:



¬†Compute
¬†Storage
 ¬†Databases
 Containers / Docker
 Analytics / Data Management
 ¬†Continuous Integration




 ¬† Sensor / IoT Data Streaming
 Message Queues / Brokers
 SMS / Push Integration
 ¬† Alexa Skills / Speech Integration
 Serverless Computing
 ¬† Code Build / Validation



Researchers Using the Cloud



Serverless Web

UVA faculty and researchers can share data, findings, tools and other resources from static HTML
content published to object storage. This simple method for publishing can cost only a few dollars a month and requires
no server management. 
      


Data Lakes

A new paradigm in data storage and processing, data lakes help researchers by providing a central repository for both
structured and unstructured data, of any type or size. These data can then be siphoned off for processing, either in
real-time streams or in queues for later analysis.
      


Services in Support of HPC

Users of HPC usually have more than enough computing power to run their jobs. But what if you
need a relational or NoSQL database, a messaging service, or offsite storage? Researchers have begun integrating the cloud 
into their HPC jobs to create, use, and manage external services like these.
      


Workflows & Pipeline Management


Researchers need flexibility for where they run their data pipelines -- it might be on a personal computer, a lab server,
an HPC cluster, or a cloud instance. We are working with faculty to extend some commonly-used pipeline tools so that they
can create and push jobs to cloud-based resources, regardless of the cloud vendor. 
      


Long-term Cold Storage

AWS Glacier and Google Nearline/Coldline offer researchers ""cold"" offsite storage for long-term backups of infrequently-accessed data. 
Many researchers use Glacier to store terabytes of source data to fulfill grants and federal research project compliance. 
      



Other Common Use Cases


Proofs of concept - To verify a system or design works, to benchmark processing speeds, we may use short-lived instances to learn from before building a production system.


Test / Development environments - For installing test packages, trying new ideas, and testing design patterns.


Dynamic / flexible / scaling application stacks - When future traffic or load cannot be determined beforehand, deploying into a dynamic environment means the infrastructure is not locked into any set type of CPU/RAM or scale.


Short-term or fast deployment projects - For almost immediate computing needs, existing users can create new instances as needed.


Container deployments - Run microservices (such as Docker containers) in an environment that can load-balance their traffic and maintain container health.



Reference Architecture
To get an idea of how public or private cloud resources are used in real-world and research scenarios, visit one of these Solution Architecture References:

AWS Architecture Center.
Google Cloud Solutions Architecture Reference | GCP Builder Tutorials.
Azure Solution Architecture | Azure Reference Architectures.

Some examples from AWS:









Batch Processing Build auto-scalable batch processing systems like video/image/datastream processing pipelines. 










Large Scale Processing and Huge Data sets Build high-performance computing systems that involve Big Data. 










Time Series and Streaming Data Processing Build elastic systems that process chronological data.
    



Cloud Services at UVA
Request an Account
Researchers or labs who would like to use AWS for their computing infrastructure should request an account through UVA Information Technology Services (ITS).
They currently support deployments in Amazon Web Services and Microsoft Azure, and offer both managed and self-service options.
ITS Cloud Solutions
Training & Implementation
With an AWS account in hand, you will need some training. ITS can provide you with links to self-paced training for both AWS and Azure. Research Computing
If you need help with how to design your project in a cloud environment, or thinking through how to migrate your existing projects, contact us for a consultation.
Sensitive Data in the Cloud
If your cloud-based project involves any sensitive data (HIPAA, PHI, etc.) you must request approval from the Information Security office at UVA. You will be required
to verify that your application, infrastructure, and staff can meet all minimum requirements for the secure transfer and handling of sensitive data.

Please note that while sensitive data projects are possible in the cloud, their approval is not automatic nor guaranteed.


Solution Architecture / Consulting
We have experience designing and delivering solutions to the public cloud using industry best practices. 
If you have a project and would like to discuss options, pricing, design, or implementation, we are available for consultation.
Our staff includes an AWS certified solution architect, and the RC team uses AWS for our own internal 
systems and development.
Request a Consultation

Training
We also offer in-person, hands-on workshops and sessions on working with the cloud. Workshops cover a number of topics,
from creating object storage buckets and simple compute instances to more complex data-driven workflows and Docker containers,
If you have an idea for a workshop or would like to schedule training for your lab or group, please contact us. 









{{% top-of-page %}}"
rc-website-fork/content/service/dac.md,"+++
title = ""Data Analytics Center""
description = """"
author = ""RC Staff""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""userinfo"",
]
date = ""2025-03-18""
tags = [
  ""userinfo"",
  ""services"",
  ""consulting"",
  ""collaboration"",
  ""rc"",
  ""drug-discovery"",
]
draft = false
+++ 

Assisting Researchers across Grounds with Big Data Management and Analysis


The DAC offers comprehensive support for projects on Research Computing platforms at every stage, from initial grant proposals and data ingestion to the development of finalized models and manuscript preparation, ensuring effective and efficient use of high-performance computing resources.  Our consultants provide tailored guidance and technical support to researchers to ensure that their projects benefit from the latest methodologies and best practices in big data analytics. 
 


Our team consists of experts with diverse backgrounds, offering a wealth of knowledge and experience in various domains, including bioinformatics, AI and machine learning, drug discovery, statistical analysis, image processing, high-performance computing, and data management and compliance, among others.  Our services and resources range from embedding one of our team members in your project and creating custom workflows to leasing storage and purchasing time on our high-performance cluster.  These services are designed to be flexible and adaptable, allowing us to support researchers across a wide range of disciplines and research questions, and can vary in scope, rate, and time-commitment from DAC team members. PIs may budget for DAC support in their grants and DAC members can be listed as key personnel. 
We are committed to enhancing the quality and impact of research across Grounds by providing exceptional data analytics support using UVA HPC platforms.  If you would like to learn more about how the DAC can assist with your project, or if you have specific needs or questions, please contact us at RC-DAC@virginia.edu. 
Service Areas



Project Preparation


                The computational needs of each project are unique.  To help you prepare, we provide:
                
Project feasibility and best practices advising
Software and compute platform selection and configuration
RC Systems Training




HPC Support


                We assist researchers with effective and efficient use of HPC resources, including: 
                
Technical Support
Best practices advising
Code parallelization, optimization, and debugging




Data Storage and Compliance


                RC provides a variety of Research Data Storage options.  We can help you decide which option best fits your needs and compliance requirements, and assist in transferring your data onto our storage.
            


Data Wrangling and Analysis


                We assist researchers with big data wrangling, analysis, and visualization, including creation of customized data pipelines and data dashboards.
            


Code Improvement


                To assist you with code improvements - such as enhancing efficiency, optimizing performance, and ensuring scalability - we offer services in code debugging, optimization, parallelization, and containerization. 
            


Computational Domain Expertise


                We have a variety of expertise including conducting academic research. We can provide specialized assistance in topics such as:
                
AI and Machine Learning
Image Processing
Bioinformatics
Text Analysis




Grant Proposal Support


                We can help you prepare documents such as: 
                
Data analysis plans
Budget justifications for compute resources
Letters of Support





Services Provided at No Charge

Training in Using RC Systems and Technical Support 
Consultation (does not include hands-on work by DAC personnel)
Basic Collaboration (typically less than 20 hours of hands-on work by DAC personnel)

Services Provided for a Fee
If you are interested in a paid service, please contact us at RC-DAC@virginia.edu for rates and other information.  

In-Depth Collaboration (typically 20 or more hours of hands-on work by DAC personnel)

How to Get Started with DAC Support
If you would like DAC assistance with a project, please contact us at RC-DAC@virginia.edu. Generally, we will start with a consultation to discuss project feasibility.  If your project is approved and meets the basic or in-depth collaboration requirements, a DAC team member will schedule a meeting to discuss staff effort and deliverables.  For an in-depth collaboration, we will create a detailed Statement of Work (SOW) that outlines the support researchers can expect from the DAC, detailing the project scope, budget, technical tasks to be completed, timeline, team member percent effort, and the deliverables to be provided at the end of the project. The SOW ensures all parties have a mutual understanding of the project's objectives and the level of support that will be offered. 

DAC Awards
The DAC has the following awards currently available.  Faculty can apply for these awards by submitting a proposal.  More information and proposal instructions are provided at the corresponding links.  For a list of previous DAC Award winners and corresponding projects, click here.

Small Awards (up to $10,000 in services)
Large Awards (up to $100,000 in services)

Featured Collaborations
We have years of experience with collaborations. To see more examples, visit our Projects page.
{{< dac-featured >}}
Team

Jacalyn Huband, PhD: Assistant Director of the Data Analytics Center
Madeline Erickson: Administrative Support

Consultants
* Angela Boakye Danquah
* Ed Hall, PhD
* Kathryn Linehan, PhD
* Priyanka Prakash, PhD
* Andrew Strumpf
* Deb Triant, PhD"
rc-website-fork/content/service/consult.md,"+++
date = ""2019-05-05T15:26:06-05:00""
categories = [""service""]
images = [""""]
author = ""Staff""
description = """"
title = ""Request a Consultation""
draft = true
tags = [""staff"",""consultation"",""contact""]
sidebar = ""hidden""
+++
Research Computing consulting services can help you take your research to the next level through the use of advanced computational techniques. Our experts are accustomed to working with researchers from across Grounds who are ready to take advantage of the University‚Äôs high-performance resources, but need help conceptualizing and executing their computation-intensive projects.
RC consulting team members provide professional advice, hands-on training, and constructive feedback at every stage of the process. Since our founding in 2008, we have assisted scores of academic researchers looking for better ways to solve complex optimization, parallelization, workflow, and data analysis issues. From concept to code, we‚Äôre here to help.
Our team members forge creative partnerships and tackle complex computing problems which may take days or even months to solve. A majority of RC consulting services are free, but some long-term collaborations require contributions from outside funding.


Name



Email



User ID



Phone



Describe your project



Send
Cancel
"
rc-website-fork/content/service/dtc.md,"+++
author = ""RC Staff""
description = """"
title = ""Digital Technology Core""
date = ""2025-03-20T00:00:00-05:00""
draft = false
tags = [""consulting"",""collaboration"",""rc""]
categories = [""services""]
images = [ ]
+++

Support for research using modern wearables, smartwatches, smartphones, and IoT devices.

The DTC is a one stop shop for researchers conducting experiments with modern wearables, smartwatches, smartphones, and IoT devices. The DTC exists to make it faster, easier, and more affordable to use modern consumer devices in research. Whether you're just getting started with devices/apps in your research or have a long track record, the DTC can meet you where you're at and provide the services you need.





What can the Digital Technology Core do for me?
The DTC understands different teams have different needs. We are open to working with you in the way that is best for you. Below is a list of some of the ways we've worked with teams in the past. If one of these sound like what you're looking for please contact us.
* Create smartphone apps to collect data, share content, and intervene to change behavior
* Collect and act on data from multiple devices (e.g., Oura ring, Whoop band, and Smartphone)
* Utilize a secure cloud where you can safely send data from IoT devices and wearables in real time
* Brainstorm ideas within the digital technology space (we love to help you formulate your ideas)
Isn't building an app time consuming and expensive?
Traditionally, yes, but the DTC offers a model that we call ""hosted"" apps, which can be built faster and more cheaply than traditional apps. A ""hosted"" app is in between building an app from scratch and using someone else's app. If your goal is to run a research study, a ""hosted"" app will do everything you want. If your goal is to eventually have your own stand-alone app, think of a ""hosted"" app as an affordable prototype. The hosted approach has many benefits for you:
* Maintenance costs are minimized due to a shared infrastructure
* New features are available for free due to a shared infrastructure
* App store review and approval processes are avoided
* Updates to your app can be released in a matter of minutes
* Your app will be available on both iOS and Android without any extra work
Can I collect data from device _____?
Probably, most modern wearable device support integration via web APIs. If a device you want to work with provides API integration (e.g., Apple Watch, Android Wear, Whoop, Oura, BioStrap, FitBit) then we can easily add it to our eco-system (if we haven't already). If you aren't sure if we support your device contact us and ask.
What happens to data you collect for me?
All data collected by the Digital Technology Core ends up in the DTC cloud. To access your data you simply need to create an account with the DTC and then you can download your data at any time. The data we collect for you is owned by you and governed by your IRB.
What does it cost to work with the Digital Technology Core?
The exact cost for a project is determined on a per-project basis, though in general costs come from two sources:
1. Paying to add new functionality (this is charged as hours of developer time)
2. Paying to use existing functionality (this is charged monthly to defer maintenance costs)
Do you offer funding grants?
Yes, if you have a research idea but don't yet have grant money, you can apply for DTC funding. We call thse awards ""seed ""grants"". The goal of DTC seed grants is to support you en-route to a larger and more competitive research grant. As such, awards are typically around $10,000, which covers adding one new feature to the DTC ecosystem and completing a study with participants. If you want to apply for a seed grant we encourage you to talk with us first. In our initial conversation we can let you know if your proposal is something the DTC could fund. For eligibility criteria and application template see DTC seed grants.
How do I include the DTC in my grant budget?
PIs may budget for DTC support in their grants.  To ensure that DTC support is included in your upcoming grant proposal, we recommend that you schedule a consultation to explore the ways in which we can assist you.  Our services and resources range from embedding one of our team members in your project to providing access to our existing software infrastructure for mobile data collection.
Ready to get started?
Contact us today using any of the methods on our contact page. If you still aren't sure if the DTC can help you, please reach out anyways. Tech jargon can be hard to follow, and sometimes all you need is to talk it through."
rc-website-fork/content/form/retired/storage-ptao.md,"+++
date = ""2022-05-04T23:59:16-05:00""
tags = [""storage""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Storage Request""
draft = true
type = ""form""
private = true
+++
{{< form-cookies >}}








{{% form-userinfo %}}
  
 Classification *
- Select -FacultyStaffPostdoctoral AssociateOther


Affiliation *

- Select -
College of Arts & Sciences
School of Data Science
School of Engineering and Applied Sciences
School of Medicine
Darden School of Business
UVA Health System
Other





Type of Request *


 ¬† Create new storage share


 ¬† Increase size of existing share


 ¬† Decrease size of existing share


 ¬† Retire existing share




Space (TB) *

The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments.





Storage Platform *


 ¬† Research Project Storage ({{% storage-pricing project %}}/TB/year)


 ¬† Research Standard Storage ({{% storage-pricing standard %}}/TB/year)


 ¬† Ivy Central Storage ({{% storage-pricing ivy %}}/TB/year)





Internal Use / Public DataThis 
storage platform is appropriate for public or internal use data.
Sensitive / Highly Sensitive DataThis storage platform is appropriate for highly sensitive data such as HIPAA, FERPA, CUI, etc.




MyGroup Ownership *

MyGroups name under your Eservices user ID. If you don‚Äôt have one, we can create one for you. You will have access to the MyGroups management and will be able to add/remove users for your project.


Shared Space Name *

This is the name to be applied to your shared storage space. By default, the space will be named according to the MyGroups associated with the storage request. If you would prefer a different identifier, indicate the name for the space.




Project Title 



Project Summary 




Grant Summary

Grant Agency 



Grant Number 




{{% billing %}}
  

Data Agreement *

      The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
    


¬†¬† I understand
  



Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit





"
rc-website-fork/content/form/retired/fdm.md,"+++
date = ""2019-06-30T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""FDM Field Testing""
draft = true
type = ""form""
private = true
+++
Alternate forms with FDM implemented:

SU Allocation Purchase
Storage Purchase

Ivy FDM needs to be implemented in the SERVICES application:

https://services.rc.virginia.edu/
"
rc-website-fork/content/form/retired/allocation-purchase-ptao.md,"+++
date = ""2019-06-30T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Purchase Service Units""
draft = true
type = ""form""
private = true
+++








  {{% form-userinfo %}}
  

Allocation Pricing

      {{< allocation-pricing >}}
    

 Name of PI *


  {{% billing %}}
  
Is the PI of your account a UVA faculty member? *

 
 ¬†Yes

 
 ¬†No
        ¬†(Non-UVA personnel are charged $0.07/SU)
      


 I agree that this allocation will be used for research purposes only *

 
 ¬†Agree

 
 ¬†Disagree



 Title of Award (if applicable) 



 Total number of SUs requested *


 Total amount to be charged to FDM *

$





 SU expiration date (if applicable) 


 Apply this purchase to which allocation *






Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit





"
rc-website-fork/content/form/retired/omero-ptao.md,"+++
date = ""2019-07-22T23:59:16-05:00""
tags = [""omero""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Omero Image Database Service""
draft = true
type = ""form""
private = true
+++
From the microscope to publication, OMERO handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), from the web or from 3rd party software. Over 140 image file formats supported, including all major microscope formats.
Use the form below to request access for your group or lab to manage and analyze data in our OMERO database service.








{{% form-userinfo %}}
  
 Classification *
- Select -FacultyStaffPostdoctoral AssociateOther


Affiliation *

- Select -
College of Arts & Sciences
School of Data Science
School of Engineering and Applied Sciences
School of Medicine
Darden School of Business
UVA Health System
Other





Type of Request *


 ¬† Create new storage share


 ¬† Increase size of existing share


 ¬† Decrease size of existing share


 ¬† Retire existing share




Data Sensitivity


¬† Moderately sensitive / public
        






Space (TB) *

The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments.


MyGroup Ownership *

MyGroups name under your Eservices user ID. If you don‚Äôt have one, we can create one for you. You will have access to the MyGroups management and will be able to add/remove users for your project.




Project Title 



Project Summary 




Grant Summary

Grant Agency 



Grant Number 




PTAO



















Financial Contact 




Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit




"
rc-website-fork/content/form/retired/containers-ptao.md,"+++
date = ""2021-04-10T23:59:16-05:00""
tags = [""microservices"",""containers"",""docker""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Container Service Request""
draft = true
type = ""form""
private = true
+++








{{% form-userinfo %}}
  
 Classification *
- Select -FacultyStaffPostdoctoral AssociateOther


Affiliation *

- Select -
College of Arts & Sciences
School of Data Science
School of Engineering and Applied Sciences
School of Medicine
Darden School of Business
UVA Health System
Other




Project Summary 


Please describe your project and the container images you want to run.




Tier of Service *


 ¬† <= 5 containers ($5/month total)


 ¬† 6 - 15 containers ($10/month total)


 ¬† > 15 containers ($48/month total)




Billing Tiers are selected and paid for by the PI. Submit this form again if you wish to change your tier. Stopped containers do not incur charges, nor does local cluster storage or remote NFS mounts to /project storage. Project storage pricing can be found here.



Storage *


 ¬† No storage required


 ¬† Persistent cluster storage required


 ¬† NFS mount of project storage is required




Storage Capacity (GB)

The size of storage if required. Specify in 1GB increments.




SSL/HTTPS Required *


 ¬† No


 ¬† Yes




Netbadge Authentication *


 ¬† No


 ¬† Yes





PTAO *



















Financial Contact *

Please enter the name and email address of your financial contact.



Data Agreement *

      The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
    


¬†¬† I understand
  


Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit






"
rc-website-fork/content/form/retired/omero.md,"+++
date = ""2019-07-22T23:59:16-05:00""
tags = [""omero""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Omero Image Database Service""
draft = true
type = ""form""
private = true
+++
From the microscope to publication, OMERO handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), from the web or from 3rd party software. Over 140 image file formats supported, including all major microscope formats.
Use the form below to request access for your group or lab to manage and analyze data in our OMERO database service.








{{% form-userinfo %}}
  
 Classification *
- Select -FacultyStaffPostdoctoral AssociateOther


Affiliation *

- Select -
College of Arts & Sciences
School of Data Science
School of Engineering and Applied Sciences
School of Medicine
Darden School of Business
UVA Health System
Other





Type of Request *


 ¬† Create new storage share


 ¬† Increase size of existing share


 ¬† Decrease size of existing share


 ¬† Retire existing share




Data Sensitivity


¬† Moderately sensitive / public
        






Space (TB) *

The size of storage to be created/retired, or the amount of the increase/decrease to your storage. Specify in 1TB increments.


MyGroup Ownership *

MyGroups name under your Eservices user ID. If you don‚Äôt have one, we can create one for you. You will have access to the MyGroups management and will be able to add/remove users for your project.




Project Title 



Project Summary 




Grant Summary

Grant Agency 



Grant Number 



  {{% billing-fdm %}}
  
Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit




"
rc-website-fork/content/form/retired/database.md,"+++
date = ""2021-04-10T23:59:16-05:00""
tags = [""mysql"",""database""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""Database Service Request""
draft = true
type = ""form""
private = true
+++









A Relational Database Service is available for researchers who need such resources in Rivanna or microservice applications. Currently database services are limited to the MySQL RDBMS, and can only be accessed from within the HPC network.
Note that you cannot connect directly to this database service from elsewhere on campus, VPN, etc. but only from Rivanna or other Research Computing systems.
Upon approval of this request you will be given the following:
      
The database host endpoint address
A database username
A database password

Connections can then be made over port 3306 to the endpoint using normal MySQL tools and libraries. A GUI user interface is available at https://phpmyadmin.uvadcos.io/ for convenience of management.
Databases are automatically backed up nightly, and 7 days are retained.
Learn more:

Database Basics
Rivanna Software



{{% form-userinfo %}}
  
 Classification *
- Select -FacultyStaffPostdoctoral AssociateOther


Affiliation *

- Select -
College of Arts & Sciences
School of Data Science
School of Engineering and Applied Sciences
School of Medicine
Darden School of Business
UVA Health System
Other




MySQL Database Use Case 


Please describe your database requirements and the project they are associated with.



Anticipated Storage Capacity (GB)

The size of storage expected over time. Specify in 1GB increments. This should not exceed 20GB.


SDS Capstone Group

The name of your SDS capstone group, if applicable.



Billing
Database Billing is paid for by the PI or SDS Capstone Faculty Advisor. Database services currently cost $5/database/month in addition to any charges for other deployed microservices containers.
  (See DCOS billing tiers for more information.) Seven days of backups are automatically stored for each database.
PTAO *



















Financial Contact *

Please enter the name and email address of your financial contact.



Data Agreement *

      The owner of these services assumes all responsibility for complying with state, federal, and international data retention laws. Researchers may be required to keep data securely stored for years after a project has ended and should plan accordingly. University of Virginia researchers are strongly encouraged to use the University Records Management Application (URMA), a web-based tool that automatically tracks when data can be safely transferred or destroyed.
    


¬†¬† I agree
  


Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit






"
rc-website-fork/content/post/.ipynb_checkpoints/2023-july-scratch-transfer-checkpoint.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2023-07-18T00:00:00-05:00""
title = ""New Scratch System on Rivanna July 18, 2023""
url = ""/maintenance""
draft = false
tags = [""rivanna""]
categories = [""feature""]
+++
During the July 18th maintenance, we installed a new /scratch file storage system on Rivanna. We have created sample scripts and instructions to help you transfer your files from the previous file system to the new one. These instructions are available here"
rc-website-fork/content/about/people/xu.md,"+++
draft = false
date = ""2024-05-30T11:15:10-05:00""
title = ""Xinyue Xu""
job_title = ""Student""
lastname = ""xu""
biolink = true
email = ""syw3ev@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_xinyue_xu.jpg""
subjects = [
  ""python"", ""machine-learning"", ""c/c++"", ""software-development""
]
active = false
+++
Xinyue is currently a second-year UVA master student in computer science. With a diverse background in computer science, she is proficient in Python and C++, and has experience with data science and cloud deployments. Xinyue is familiar with machine learning algorithms, applying them across various fields such as autonomous navigation for ROSbot cars and bioinformatics for cancer survival analysis. Her diverse expertise and passion for technology keep her moving forward.
Education

B.S. Computer Science
M.S. Computer Science
University of Virginia (Expected Graduation: 2025)
"
rc-website-fork/content/about/people/kureishi.md,"+++
draft = false
date = ""2025-02-05T11:15:10-05:00""
title = ""Namai Kureishi""
job_title = ""Student""
lastname = ""Kureishi""
student_year = ""3rd Year""
student_type = ""Undergraduate""
biolink = true
email = ""zap5dt@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_namai_kureishi.jpg""
subjects = [
  ""python"",
  ""java"",
  ""software-development"",
  ""r"",
  ""docker"",
]
active = true
+++
Namai is a rising third-year UVA student majoring in computer science. She is proficient in Python, Java, and JavaScript, and is passionate about software development, cybersecurity,and statistics. At Research Computing, she hopes to apply her skills and learn new things. 
Education

B.A. Computer Science
University of Virginia (Expected Graduation: 2027)
"
rc-website-fork/content/about/people/periyasamy.md,"+++
draft = false
date = ""2023-12-24T11:15:10-05:00""
title = ""Hariprasad Periyasamy""
job_title = ""Student""
student_year = ""4th Year""
student_type = ""Undergraduate""
lastname = ""periyasamy""
biolink = true
email = ""sfb3jj@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_hariprasad_periyasamy.jpeg""
subjects = [
  ""python"", ""software-development"",""api""
]
active = false
+++
Hari is currently a fourth-year UVA student pursuing computer science and applied statistics with a concentration in data science. At Research computing, he has helped out with the maintenance of the website, which is based in Hugo. He has also helped out on the software development side of research computing to create some Flask-based APIs in python. He is currently a software development engineer at Amazon (AWS) focusing on security tools, and is planning to return for the Fall and Spring semester. 
Education

B.A. Computer Science
B.A. Applied Statistics
University of Virginia (Expected Graduation: 2026)
"
rc-website-fork/content/about/people/lin.md,"+++
draft = false
date = ""2025-02-05T11:15:10-05:00""
title = ""Diana Lin""
job_title = ""Student""
lastname = ""lin""
student_year = ""1st Year""
student_type = ""Undergraduate""
biolink = true
email = ""xrc9wg@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_diana_lin.jpg""
subjects = [
  ""python"",
  ""docker"",
  ""machine-learning"",
]
active = true
+++
Diana is currently a first-year UVA student pursuing electrical and computer engineering. She has experience with machine learning and software development, with her favorite languages being Python, JavaScript, and C++. At Research Computing, Diana is excited to support research and gain more experience with high-performance computing. In the future, she is hoping to further explore software and embedded development.
Education

B.S. Computer Engineering
University of Virginia (Expected Graduation: 2027)
"
rc-website-fork/content/about/people/mistele.md,"+++
draft = false
date = ""2025-2-05T11:15:10-05:00""
title = ""Ivey Mistele""
job_title = ""Student""
lastname = ""mistele""
student_year = ""2nd Year""
student_type = ""Undergraduate""
biolink = true
email = ""zyh4up@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_ivey_mistele.jpg"" 
subjects = [
   ""python"", ""r"",""va-whpc""
]
active = true
+++
Ivey is a second-year student in UVA‚Äôs inaugural undergraduate data science class, intending to concentrate in systems. She primarily works with Java, R, and Python. She has experience in data cleaning and visualization using multiple programming languages, including Python, R, and SQL. Outside of UVA HPC, she plays baritone saxophone in UVA‚Äôs jazz ensemble.
Education

B.S. Data Science; University of Virginia (Expected May, 2027)
"
rc-website-fork/content/about/people/khan.md,"+++
draft = false
date = ""2025-06-3T11:15:10-05:00""
title = ""Muneer Khan""
job_title = ""Student""
lastname = ""khan""
student_year = ""2nd Year""
student_type = ""Undergraduate""
biolink = true
email = ""fzc3xs@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_muneer_khan.jpg""
subjects = [
  ""software-development"",
  ""python"",
]
active = true
+++
Muneer is currently a second-year undergraduate student studying computer science. He has experience with full-stack software development, primarily using TypeScript and the Next.js framework with PostgreSQL for database. At Research Computing, Muneer is hoping to learn more about parallel computing and software containers.
Education

B.S. Computer Science; University of Virginia (Expected May, 2027)
"
rc-website-fork/content/about/people/co.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Michele Co, PhD""
lastname = ""co""
job_title = ""HPC Systems Specialist""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
  ""parallel-computing"",
  ""storage"",
]
+++"
rc-website-fork/content/about/people/bobar.md,"+++
draft = false
date = ""2023-01-01T15:25:10-05:00""
title = ""Marcus Bobar""
job_title = ""Computational Scientist""
lastname = ""Bobar""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""imaging"",
  ""data-science"",
  ""image-analysis"",
  ""machine-learning"",
  ""python"",
  ""research""
]
+++"
rc-website-fork/content/about/people/duy.md,"+++
draft = false
date = ""2023-01-01T15:25:10-05:00""
title = ""Camden Duy""
job_title = ""Research Computing Associate""
lastname = ""Duy""
email=""cmd7ag@virginia.edu""
type = ""people""
biolink = true
description = """"
author = ""UVARC Staff""
images = [
 """"
]
subjects = [
  ""python"",
  ""matlab"",
  ""materials-science"",
  ""hpc""
]
+++
Camden is experienced in working in an HPC environment and coding with Python. He works to provide user support for the users of Rivanna. His background includes materials physics and mathematics."
rc-website-fork/content/about/people/quist.md,"+++
draft = false
date = ""2024-6-3T11:15:10-05:00""
title = ""Wright Quist""
job_title = ""Student""
lastname = ""quist""
student_year = ""4th Year""
student_type = ""Undergraduate""
biolink = true
email = ""wru3rm@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_wright_quist.jpg""
subjects = []
active = false
+++
Wright is a 4th-year computer science student at UVA studying computer science and studio art, with a focus on web applications and software engineering.  Outside the classroom, Wright's interests include textile art, hiking, and woodworking.
Education

B.S. Computer Science 
B.A. Studio Art
  University of Virginia SEAS (Expected Graduation: 2025)

"
rc-website-fork/content/about/people/siller.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Karsten Siller, PhD""
job_title = ""user services, Director"" 
lastname = ""siller""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_karsten_siller.png""
email = ""khs3z@virginia.edu""
subjects = [
  ""hpc"",
  ""rivanna"",
  ""image-processing"",
  ""python"",
  ""java"",
  ""containers""
]
+++
Karsten's background is in developmental cell biology, genetics and light microscopy. He leads Research Computing's User Services team and provides technical support, training and project consultation for UVA's centralized compute platforms. His focus areas are HPC workflows, software containers, Python programming and image processing.
Selected Publications


Zhang J, Wallrabe H, Siller K, Mbogo B, Cassidy T, Alam SR, Periasamy A. (2025) Measuring Metabolic Changes in Cancer Cells Using Two‚ÄêPhoton Fluorescence Lifetime Imaging Microscopy and Machine‚ÄêLearning Analysis. J Biophotonics 18 (1), e202400426. doi:10.1002/jbio.202400426


Sun R, Siller K. (2024) HPC Container Management at the University of Virginia. Practice and Experience in Advanced Research Computing 2024, 73, 1-4. doi:10.1145/3626203.3670568


Zimyanin V, Magaj M, Yu CH, Gibney T, Mustafa B, Horton X , Siller K, Cueff L, Bouvrais H, P√©cr√©aux J, Needleman D, Redemann S. (2024) Using 3D Large Scale Tomography to Study Force Generation in the Mitotic Spindle. Microscopy and Microanalysis 30 (Supplement_1), ozae044. 344. doi:10.1093/mam/ozae044.344


Best MN, Lim Y, Ferenc NN, Kim N, Min L, Wang DB, Sharifi K, Wasserman AE, McTavish SA, Siller KH, Jones MK, Jenkins PM, Mandell JW, Bloom GS. (2023) Extracellular Tau Oligomers Damage the Axon Initial Segment. J Alzheimers Dis. 93 (4). doi:10.3233/JAD-221284. 


Alam SR, Wallrabe H, Christopher KG, Siller KH, Periasamy A. (2022) Characterization of mitochondrial dysfunction due to laser damage by 2-photon FLIM microscopy. Sci Rep. 12:11938. doi:10.1038/s41598-022-15639-z


Cao R, Wallrabe H, Siller K, Periasamy A. (2020) Optimization of FLIM imaging, fitting and analysis for auto-fluorescent NAD(P)H and FAD in cells and tissues. Methods Appl. Fluoresc. 8(2):024001. doi:10.1088/2050-6120/ab6f25.


Lauren K Rudenko LK, Wallrabe H, Periasamy A, Siller K, Svindrych Z, Seward ME, Best MN, Bloom GS. (2019) Intraneuronal Tau Misfolding Induced by Extracellular Amyloid-Œ≤ Oligomers. J Alzheimers Dis. 71(4):1125-1138. doi:10.3233/JAD-190226.


Cao R, Wallrabe H, Siller K, Rehman Alam S, Periasamy A. (2019) Single-cell redox states analyzed by fluorescence lifetime metrics and tryptophan FRET interaction with NAD(P)H.
Cytometry A. 95(1):110-121. doi:10.1002/cyto.a.23711.


Wallrabe H, Svindrych Z, Alam SR, Siller KH, Wang T, Kashatus D, Hu S, Periasamy A. (2018) Segmented cell analyses to measure redox states of autofluorescent NAD(P)H, FAD & Trp in cancer cells by FLIM. Sci Rep. 8(1):79. doi:10.1038/s41598-017-18634-x.


Janssens DH, Hamm DC, Anhezini L, Xiao Q, Siller KH, Siegrist SE, Harrison MM, Lee CY. (2017) An Hdac1/Rpd3-Poised Circuit Balances Continual Self-Renewal and Rapid Restriction of Developmental Potential during Asymmetric Stem Cell Division. Dev Cell 40(4):367-380.e7. doi:10.1016/j.devcel.2017.01.014.


Doyle SE, Pahl MC, Siller KH, Ardiff L, Siegrist SE. (2017) Neuroblast niche position is controlled by Phosphoinositide 3-kinase-dependent DE-Cadherin adhesion. Development 144(5):820-829. doi:10.1242/dev.136713.


Siller KH, Doe CQ. (2009) Spindle orientation during asymmetric cell division. Nat Cell Biol. 11(4):365-74. doi:10.1038/ncb0409-365.


Siller KH, Cabernard C, Doe CQ. (2006) The NuMA-related Mud protein binds Pins and regulates spindle orientation in Drosophila neuroblasts. Nat Cell Biol. 8(6):594-600. doi:10.1038/ncb1412.


Siller KH, Serr M, Steward R, Hays TS, Doe CQ. (2005) Live imaging of Drosophila brain neuroblasts reveals a role for Lis1/dynactin in spindle assembly and mitotic checkpoint control. Mol Biol Cell. 16(11):5127-40. doi:10.1091/mbc.e05-04-0338.


See Google Scholar for a complete list of publications."
rc-website-fork/content/about/people/vo.md,"+++
draft = false
date = ""2024-05-30T18:43:42-05:00""
title = ""Jennifer Vo""
job_title = ""Student""
student_year = ""4th Year""
student_type = ""Undergraduate""
lastname = ""vo""
biolink = true
email = ""phb8pt@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_jennifer_vo.jpg""
subjects = [
  ""java"", ""r"", ""va-whpc""
]
active = false
+++
Jennifer is a fourth-year student at UVA pursuing a B.S in Computer Science and a minor in Data Science. She has a strong foundation in programming languages like Python and C, and particularly favors Java. She has experience in making data visualizations and contributing to web and game development projects. Enthused about learning and exploration, she enjoys trying new foods and playing board games, with Catan being her first choice. Jennifer is driven by a desire to help others and making a positive impact by using her skills to innovate and support others in achieving common goals.
"
rc-website-fork/content/about/people/addakula.md,"+++
draft = false
date = ""2022-05-26T15:25:10-05:00""
title = ""Praveen Kumar Addakula""
job_title = ""DevOps Developer""
lastname = ""addakula""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""containers"",
  ""infrastructure"",
  ""kubernetes"",
  ""api"",
]
+++
More about this user goes here."
rc-website-fork/content/about/people/chamakuri.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Ravi Chamakuri, PMP""
job_title = ""DevOps Manager""
lastname = ""chamakuri""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""api"",
  ""databases"",
  ""containers"",
  ""python"",
  ""docker"",
  ""kubernetes"",
  ""software-development""
]
+++
More about this user goes here."
rc-website-fork/content/about/people/triant.md,"+++
draft = false
date = ""2025-04-11T13:25:10-05:00""
title = ""Deb Triant""
job_title = ""Research Computing Scientist""
lastname = ""triant""
email=""dtriant@virginia.edu""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""bioinformatics"",
  ""hpc"",
  ""research""
]
+++
Deb's background is in bioinformatics with an emphasis in animal genetics. Her research has primarily involved the analysis of genomic and transcriptomic sequence datasets. She is also passionate about teaching and training scientists and has taught workshops on computational genomics and programming for biologists."
rc-website-fork/content/about/people/shankar.md,"+++
draft = false
date = ""2023-12-24T11:15:10-05:00""
title = ""Mohan Shankar""
job_title = ""Student""
lastname = ""shankar""
student_year = ""1st Year""
student_type = ""Graduate""
biolink = true
email = ""mjs7eek@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_mohan_s.JPG""
subjects = [
  ""hpc"", ""python"", ""dft"", ""computational-chemistry""
]
active = true
+++
Mohan is currently a first-year graduate student pursuing chemical engineering with a focus on computational catalysis. Primarily, he uses Python, density functional theory (DFT) software like VASP and Gaussian, and machine learning models like FLARE and ALLEGRO for his work.
Education

B.S. Chemistry, Highest Distinction; University of Virginia 
M.S. Chemical Engineering; University of Virginia (Expected May, 2026)
"
rc-website-fork/content/about/people/hongyan.md,"+++
draft = false
date = ""2024-05-31T10:25:10-05:00""
title = ""Hongyan Wu""
job_title = ""student""
student_year = ""1st Year""
student_type = ""Graduate""
lastname = ""Wu""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_hongyan_wu.png""
email = ""hw5uj@virginia.edu""
subjects = [
  ""reinforcement-learning"",
  ""machine-learning"",
  ""python"",
  ""software-development""
]
active = false
+++
Hongyan Wu is a graduate student at the University of Virginia studying Computer Science. He specializes in Machine Learning, Reinforcement Learning and Robotics."
rc-website-fork/content/about/people/strumpf.md,"+++
draft = false
date = ""2025-04-11T15:25:10-05:00""
title = ""Andrew Strumpf""
job_title = ""Data Manager""
lastname = ""strumpf""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
   ""ivy"", 
   ""research"",
   ""sensitive-data"", 
   ""storage""
]
+++
Andrew is a Data Manager with a background in public health and clinical research. He helps users comply with University policies for research data and develop customized data pipelines."
rc-website-fork/content/about/people/galitz.md,"+++
draft = false
date = ""2024-01-02T23:27:10-05:00""
title = ""Matthew Galitz""
job_title = ""Student""
student_year = ""4th Year""
student_type = ""Undergraduate""
lastname = ""Galitz""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_matthew_galitz.jpg""
email = ""ykk6rh@virginia.edu""
subjects = [""python"", ""software-development""]
active = false
+++
Matthew Galitz is an undergraduate student at University of Virginia studying Mathematics and Computer Science. He specializes in Python scripting and software development. His interests include artificial intelligence, brazilian jiu jitsu, and powerlifting.
"
rc-website-fork/content/about/people/jpark.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Juwon Park""
job_title = ""Student""
student_year = ""3rd Year""
student_type = ""Undergraduate""
lastname = ""park""
biolink = true
email = ""bzf9sj@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_juwon_park.png""
subjects = [""data-analysis"",""statistics"",""r"",""python""]
active = false
+++
Juwon is a third year student at the University of Virginia pursuing a major in Applied Statistics and a minor in Data Science. She contributes to the development of a program that rebuilds R libraries for users. She has experience performing data analysis and creating data visualizations in R, Python, and SAS, and is passionate about utilizing data to make meaningful interpretations in the world.
Juwon currently works as a data volunteer for a local non-profit organization that provides essentials for under-resourced families with children. One of her favorite hobbies include collecting and organizing her sock collection."
rc-website-fork/content/about/people/sun.md,"+++
draft = false
date = ""2019-10-27T11:15:10-05:00""
title = ""Ruoshi Sun, PhD""
job_title = ""Lead Scientist, Scientific Computing""
lastname = ""sun""
biolink = true
email = ""rs7wz@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_ruoshi_sun.png""
subjects = [
  ""compiler"",
  ""containers"",
  ""hpc"",
  ""materials-science"",
  ""parallel-computing"",
  ""python"",
]
+++
Ruoshi is experienced in electronic structure codes for computational materials science. He takes part in managing the HPC software stacks, building Apptainer/Docker containers, and providing user support/research consultation.
He is a volunteer pianist at the University Hospital through the ""Music for Healing"" program.
Selected Publications

K Esfarjani, H Stokes, SN Sadeghi, Y Liang, B Timalsina, H Meng, J Shiomi, B Liao, and R Sun, ""ALATDYN: A set of Anharmonic LATtice DYNamics codes to compute thermodynamic and thermal transport properties of crystalline solids,"" Computer Physics Communications 312, 109575 (2025). doi:10.1016/j.cpc.2025.109575
R Sun, M Asta, and A van de Walle, ""First-principles thermal compatibility between Ru-based Re-substitute alloys and Ir coatings,"" Computational Materials Science 170, 109199 (2019). doi:10.1016/j.commatsci.2019.109199
R Sun, C Woodward, and A van de Walle, ""First-principles study on Ni3Al (111) antiphase boundary with Ti and Hf impurities,"" Physical Review B 95, 21412 (2017). doi:10.1103/PhysRevB.95.214121
R Sun and DD Johnson, ""Stability maps to predict anomalous ductility in B2 materials,"" Physical Review B 87, 104107 (2013). doi:10.1103/PhysRevB.87.104107
R Sun, MKY Chan, and G Ceder, ""First-principles electronic structure and relative stability of pyrite and marcasite: Implications for photovoltaic performance,"" Physical Review B 83, 235311 (2011). doi:10.1103/PhysRevB.83.235311

See Google Scholar for a complete list.
Education


Ph.D. Materials Science and Engineering
Massachusetts Institute of Technology (2013)


B.S. Materials Science and Engineering
B.S. Mathematics
University of Illinois at Urbana-Champaign (2008)

"
rc-website-fork/content/about/people/park.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Gisoo Park""
job_title = ""HPC Systems Specialist""
lastname = ""park""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
  ""parallel-computing"",
  ""storage"",
]
+++
More about this user goes here."
rc-website-fork/content/about/people/huband.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Jacalyn Huband, PhD""
job_title = ""Assistant Director of the Data Analytics Center""
lastname = ""huband""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
  ""parallel-computing"",
  ""python"",
  ""r"",
  ""matlab"",
  ""storage"",
  ""data-analysis""
]
+++
Jacalyn (Jackie) Huband, Ph.D. is the Assistant Director of the Data Analytics Center (DAC) within Research Computing at the University of Virginia (UVA). In addition to managing the DAC resources, she is a facilitator to researchers who are transitioning their codes onto a high-performance computing (HPC) platform. She also has been instrumental in creating learning opportunities within Research Computing, including Workshops, Brown Bag Lunches, and Journal Clubs.  Prior to her work at UVA, Jackie has been an Associate Professor in Computer Science and a programmer in industry. She has experience programming in R, Python, Matlab, C/C++, and Fortran."
rc-website-fork/content/about/people/baller.md,"+++
draft = false
date = ""2023-01-02T15:25:10-05:00""
title = ""Josh Baller, PhD""
job_title = ""Associate Vice President""
lastname = ""Baller""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""research"",
  ""hpc""
]
+++
More about this user goes here."
rc-website-fork/content/about/people/prakash.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Priyanka Prakash, PhD""
job_title = ""Research Computing Scientist""
lastname = ""Prakash""
type = ""people""
biolink = true
email = ""gpd6kn@virginia.edu""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""computational-chemistry"",
  ""computational-biophysics"",
  ""data-analysis"",
  ""drug-discovery"",
  ""amber"", 
  ""namd"",
  ""rosetta"",
  ""docking"",
  ""python"",
  ""hpc"",
  ""rivanna"",
  ""va-whpc""
]
+++
Priyanka (Pri) Prakash joined the Research Computing (RC) Data Analytics Center as a Research Computing Scientist in Jan 2024. She joins us from MD Anderson Cancer Center, Houston, TX where she worked as an Institute Research Investigator. Prior to that she was a computational scientist at the Frederick National Laboratory for Cancer Research (FNLCR, Frederick, MD). She has MS in organic chemistry, and she completed her PhD in Biological Sciences and Bioengineering from the Indian Institute of Technology Kanpur, India  (IITK). She has extensively worked in the field of computational biophysics, computer-aided drug design (CADD) and in integrating AI/ML and molecular simulation approaches in CADD. She has published 30+ original peer-reviewed research articles in journals including Cell, PNAS, Biophys J, Scientific Rep, in the field of her domain expertise and served as an inventor on investigational new drug applications and patents. Her domain expertise is drug discovery, cancer biology, computational biophysics and computational chemistry and she has been a user of HPC environment (TACC, XSEDE, Biowulf) for more than a decade. She has served as co-PI on two awarded grants for compute time on NSF-funded ACCESS systems. She has designed and developed innovative computational strategies including workflows/computational pipelines in CADD and computational biophysics/chemistry with tremendous positive impact in academia as well as pharma-oriented industry setups. She is highly experienced in scientific communication and grant-writing and has served as a co-I on an awarded NIH R01."
rc-website-fork/content/about/people/siadaty.md,"+++
draft = false
date = ""2024-05-31T11:15:10-05:00""
title = ""Steven Siadaty""
job_title = ""Student""
lastname = ""Siadaty""
student_year = ""2nd Year""
student_type = ""Undergraduate""
biolink = true
email = ""vth3bk@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_steven_siadaty.jpg""
subjects = [""python"", ""machine-learning"", ""r"", ""bioinformatics""]
active = true
+++
Steven is a second-year in the College of Arts and Sciences studying computer science and statistics. He has experience working with Python, Java, and C#. His interests focus on deep learning and AI integration, particularly involving bioinformatics and genetic sequencing. At Research computing Steven hopes to expand his understanding of the software development workflow, and to make high performance computing resources more available to UVA students and staff."
rc-website-fork/content/about/people/linehan.md,"+++
draft = false
date = ""2024-01-25T12:55:18-05:00""
title = ""Kathryn Linehan, PhD""
job_title = ""Computational Research Scientist""
lastname = ""linehan""
biolink = true
email = ""kjl5t@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_kathryn_linehan.jpg""
subjects = [
  ""hpc"",
  ""rivanna"",
  ""python"",
  ""r"",
  ""data-science"",
  ""machine-learning""
]
+++
Kathryn is an applied mathematician with the Data Analytics Center. Her current research interests include numerical linear algebra, low-rank matrix approximation and applications, and scientific computing. Prior to joining RC, she was a Research Scientist with the Social and Decision Analytics Division of UVA's Biocomplexity Institute where she specialized in natural language processing and machine learning for social science applications.  She has experience with HPC, general purpose GPU computing, and academic programming with University of Maryland faculty from the Math, Computer Science, and Chemistry/Biochemistry departments. 
Education


Ph.D. Applied Mathematics & Statistics, and Scientific Computation
University of Maryland, College Park (May 2024) 


Certificate in Data Science 
Georgetown University, School of Continuing Studies (2018)


M.S. Applied Mathematics & Statistics, and Scientific Computation
University of Maryland, College Park (2009)


B.A. Mathematics
Hood College (2006)

"
rc-website-fork/content/about/people/kim.md,"+++
draft = false
date = ""2024-05-31T11:15:10-05:00""
title = ""Jinwoo Kim""
job_title = ""Student""
student_year = ""2nd Year""
student_type = ""Undergraduate""
lastname = ""kim""
biolink = true
email = ""eer6vt@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_jinwoo_kim.jpeg""
subjects = [
  ""python"", ""software-development""
]
active = false
+++
Jinwoo is currently a second-year UVA student pursuing a major in computer science. He has experience with Python, Java, and C, ranging from software development and web development. At Research computing, he hopes to be able to help out with the RC website using Hugo, and learning more about HPC. In the future, he is hoping to pursue software development as his main focus.
Education
B.S. Computer Science
University of Virginia (Expected Graduation: 2026)
"
rc-website-fork/content/about/people/parece.md,"+++
draft = false
date = ""2023-12-14T13:25:10-05:00""
title = ""Hana Parece""
job_title = ""Research Computing Associate""
lastname = ""parece""
email=""xve5kj@virginia.edu""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""python"",
  ""matlab"",
  ""c/c++""
]
+++
Hana is a Computer Scientist with a strong background in a variety of programming languages. She is most experienced with Python and C/C++ and is passionate about teaching others about those languages. She helps to provide user support for the users of Rivanna and is seeking to learn more about GPU computing to better help those in an HPC environment."
rc-website-fork/content/about/people/losen.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Steve Losen""
job_title = ""HPC Systems Specialist""
lastname = ""losen""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
  ""parallel-computing"",
  ""storage"",
]
+++
More about this user goes here."
rc-website-fork/content/about/people/wang.md,"+++
draft = false
date = ""2023-01-02T15:25:10-05:00""
title = ""Xu Wang, PhD""
lastname = ""wang""
job_title = ""HPC Systems Specialist""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
]
+++"
rc-website-fork/content/about/people/garrevenkata.md,"+++
draft = false
date = ""2025-2-05T11:50:12-05:00""
title = ""Bhargav Garre Venkata""
job_title = ""Student""
student_year = ""2nd Year""
student_type = ""Undergraduate""
lastname = ""garrevenkata""
biolink = true
email = ""kks9hk@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_bhargav_garrevenkata.jpg""
subjects = [
    ""software-development"", ""api""
]
active = true
+++
Bhargav is a Computer Science student in the School of Engineering with experience in web development, Python, Java, and Linux, as well as video and tutorial production. He is passionate about creating websites and writing scripts to enhance efficiency. At Research Computing, Bhargav aims to deepen his knowledge of high-performance computing while making the University‚Äôs extensive computing resources more accessible to UVA students and staff. Outside of technology, Bhargav‚Äôs hobbies include playing quizbowl and watching stand-up."
rc-website-fork/content/about/people/ptak.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Alex Ptak""
job_title = ""HPC / Cloud Infrastructure Engineer""
lastname = ""ptak""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""ivy"",
  ""sensitive-data"",
  ""hpc"",
  ""rivanna""
]
+++"
rc-website-fork/content/about/people/eubanks.md,"+++
draft = false
date = ""2025-02-03T10:25:10-05:00""
title = ""Adam Eubanks""
job_title = ""student""
student_year = ""2nd Year""
student_type = ""Undergraduate""
lastname = ""Eubanks""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_adam_eubanks.jpeg""
email = ""ucn5xj@virginia.edu""
subjects = [
  ""machine-learning"",
  ""ood"",
  ""docker""
]
active = false
+++
Adam Eubanks is an undergraduate student at Brigham Young University studying Applied Math. He specializes in Open OnDemand app development and machine learning with HPC, as well as containerization."
rc-website-fork/content/about/people/orndorff.md,"+++
draft = false
date = ""2023-06-22T01:10:10-05:00""
title = ""Paul Orndorff, PhD""
job_title = ""Computational Scientist""
lastname = ""Orndorff""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""data-analysis"",
  ""computational-chemistry"",
  ""python"",
  ""research""
]
+++"
rc-website-fork/content/about/people/alabi.md,"+++
draft = false
date = ""2022-05-26T15:25:10-05:00""
title = ""Mubarak Olamide Alabi""
job_title = ""DevOps Engineer""
lastname = ""alabi""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""containers"",
  ""k8s"",
  ""infrastructure"",
  ""kubernetes"",
  ""api"",
]
+++
More about this user goes here."
rc-website-fork/content/about/people/sheikhzada.md,"+++
draft = false
date = ""2022-06-05T15:25:10-05:00""
title = ""Ahmad Sheikhzada, PhD""
job_title = ""Research Computing Technical Support Manager""
lastname = ""sheikhzada""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""parallel-computing"",
  ""data-analysis"",
  ""python""
]
+++"
rc-website-fork/content/about/people/boakyedanquah.md,"+++
draft = false
date = ""2023-03-06T07:30:10-05:00""
title = ""Angela Boakye Danquah""
job_title = ""Research Computing Scientist""
lastname = ""boakye danquah""
biolink = true
email = ""aab5zd@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_angela_boakyedanquah.png""
subjects = [
  ""machine-learning"",
  ""deep-learning"",
  ""hpc"",
  ""r"",
  ""parallel-computing"",
  ""python"",
]
+++
Angela is a computational scientist in the Data Analytics Center at RC. She supports faculty and community members' machine and deep learning research. Angela is experienced in working in HPC and Cloud computing environments. 
Education


M.S. Data Science
University of Virginia (2023)


B.A. Statistics, Economics
University of Virginia (2021)

"
rc-website-fork/content/about/people/vu.md,"+++
draft = false
date = ""2025-02-05T11:15:10-05:00""
title = ""Megan Vu""
job_title = ""Student""
lastname = ""vu""
student_year = ""1st Year""
student_type = ""Undergraduate""
biolink = true
email = ""zrv4jz@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_megan_vu.jpg""
subjects = [
  ""python"", ""java""
]
active = true
+++
Megan is a current 2nd-year student double majoring in Computer Science and Cognitive Science. She has experience coding in Python, Java, and SQL. Megan is an active member of Women in Computing Sciences, Student Game Developers, and the Cavalier Symphony Orchestra. In her free time, Megan likes to draw, play video games, and doom scroll on Wikipedia."
rc-website-fork/content/about/people/hussein.md,"+++
draft = false
date = ""2024-06-06T13:41:00""
title = ""Fadumo Hussein""
job_title = ""student""
student_year = ""2nd Year""
student_type = ""Graduate""
lastname = ""Huseein""
biolink = true
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_hussein_fadumo.png""
email = ""fmh7pvuj@virginia.edu""
subjects = [""r"",""python"", ""sql"", ""machine learning""]
+++
Fadumo Hussein is a graduate student at the University of Virginia studying Data Science. Equipped with experience in Python, R, SQL, and SAS, Fadumo excels in conducting data analysis, model development, and visualization, all aimed at supporting organizational objectives and fostering positive outcomes. Outside of Research Computing, she enjoys spending time outdoors in nature and indulging in reading."
rc-website-fork/content/about/people/holcomb.md,"+++
draft = false
date = ""2019-05-19T15:25:10-05:00""
title = ""Katherine Holcomb, PhD""
job_title = ""Senior Research Systems Consultant""
lastname = ""holcomb""
biolink = false
type = ""people""
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
  ""parallel-computing"",
  ""compiler"",
  ""mpi"",
  ""fortran"",
  ""c/c++"",
  ""python""
]
+++
More about this user goes here."
rc-website-fork/content/about/people/hall.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Ed Hall, PhD""
job_title = ""Computational Research Consultant""
lastname = ""hall""
type = ""people""
biolink = false
description = """"
author = ""UVARC Staff""
images = [
  """"
]
subjects = [
  ""hpc"",
  ""rivanna"",
  ""parallel-computing"",
  ""matlab"",
]
+++
More about this user goes here."
rc-website-fork/content/about/people/_index.md,"+++
tags = [
  ""rc"",
  ""staff"",
]
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Research Computing Staff""
type = ""people""
description = """"
author = ""RC Staff""
images = [
  """"
]
categories = [
  ""staff"",
  ""about""
]
+++"
rc-website-fork/content/about/people/tran.md,"+++
draft = false
date = ""2025-02-05T11:15:10-05:00""
title = ""Dominic Tran""
job_title = ""Student""
lastname = ""tran""
student_year = ""3rd Year""
student_type = ""Undergraduate""
biolink = true
email = ""tfe2zz@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_dominic.jpeg""
subjects = [
]
active = true
+++
Dominic is a 3rd year in the School of Engineering studying computer science. He's interested in systems programming and uses C and Python.
Education

B.S. Computer Science
University of Virginia (Expected Graduation: 2027)
"
rc-website-fork/content/about/people/andino.md,"+++
draft = false
date = ""2019-05-17T15:25:10-05:00""
title = ""Gladys Andino, PhD""
job_title = ""Strategic Services and Education Manager""
lastname = ""Andino""
type = ""people""
biolink = true
email = ""gka6a@virginia.edu""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_gladys_andino.png""
subjects = [
  ""hpc"",
  ""bioinformatics"",
  ""rivanna"",
  ""va-whpc""
]
+++
  Gladys  is the Strategic Services and Education Manager for ITS-Research Computing at UVA. She 
leads strategic planning for researcher training and outreach initiatives coordinating workshops and tutorials to support UVA‚Äôs computational research community. She adeptly manages a team of student workers, optimizing ITS-Research Computing‚Äôs efficiency by delegating routine tasks, thereby enabling staff to concentrate on domain-specific research challenges, all while grooming the next generation of HPC and scientific computing professionals. 

With a strong background in biological sciences and expertise in bioinformatics, Gladys supports computational biology research, particularly in genomic data analysis and high-performance computing applications. She is also part of the Data Analytics Center (DAC) consulting team at UVA, advising on bioinformatics workflows and computational methodologies. 

Since 2021, Gladys has served as the Founder and Chair of Virginia Women in HPC, a program she has successfully expanded across seven Virginia institutions. She organizes virtual events featuring distinguished technical speakers to champion women and allies in HPC and technology.


Selected Publications


Gladys Andino, Scott L. Delinger, Jacob Fosso Tande, Timothy Middelkoop, Claire Mizumoto, David P. Reddy, and Michael D. Weiner. Onboarding Research Computing and Data Professionals. in Practice and Experience in Advanced Research Computing (PEARC ‚Äô24), July 21‚Äì25, 2024, Providence, RI, USA. ACM, New York, NY, USA, 2024. https://doi.org/10.1145/3626203.3670582


Gladys Andino, Scott L. Delinger, Jacob Fosso Tande, Timothy Middelkoop, Claire Mizumoto, David P. Reddy, and Michael D. Weiner. An Onboarding Checklist for Research Computing and Data Professionals, Zenodo, 2024. https://doi.org/10.5281/zenodo.11074226


Harrell SL, Brazil M, Younts A, Dietz DT, Smith P, Gough E, Zhu X, Andino GK. Mentoring Undergraduates into Cyber-Facilitator Roles. PEARC '18: Proceedings of the Practice and Experience on Advanced Research Computing. 2018 July 22; 70:1-7. DOI:
10.1145/3219104.3219138


Andino GK, Marisa B, Gribskov M, Smith P. Advancing the Representation of Women in HPC at Purdue University. PEARC '17: Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact. 2017 July; (56):1-3. DOI:
10.1145/3093338.3104175


Andino GK, Gribskov M, Anderson DL, Evans JD, Hunt GJ. Differential gene expression in Varroa jacobsoni mites following a host shift to European honey bees (Apis mellifera). BMC Genomics. 2016 Nov 16;17(1):926. PubMed Central PMCID: PMC5112721


Hunt GJ, Given K, Tsuruda JT, Andino GK. Breeding mite-biting bees to control Varroa. Bee Culture: The Magazine of American Beekeeping. 2016 March 23; :41-47. Available from: https://www.beeculture.com/breeding-mite-biting-bees-to-control-varroa/


Krupke CH, Hunt GJ, Eitzer BD, Andino G, Given K. Multiple routes of pesticide exposure for honey bees living near agricultural fields. PLoS One. 2012;7(1):e29268. PubMed Central PMCID: PMC3250423


Andino GK, Hunt GJ. A scientific note on a new assay to measure honeybee mite-grooming behavior. Apidologie. 2011; (42):481-484. DOI: 10.1007/s13592-011-0004-1


Education
Ph.D. Entomology
Purdue University, (2014) 
B.S. Science in Agriculture
Zamorano Pan-American Agricultural School (2002)"
rc-website-fork/content/about/people/tomar.md,"+++
draft = false
date = ""2023-12-21T11:15:10-05:00""
title = ""Diya Tomar""
job_title = ""Student""
lastname = ""tomar""
student_year = ""2nd Year""
student_type = ""Undergraduate""
biolink = true
email = ""mhe3ft@virginia.edu""
type = ""people""
description = """"
author = ""UVARC Staff""
image = ""/images/profiles/profile_diya_tomar.jpg""
subjects = [
  ""python"",
  ""software-development""
]
active = false
+++
Diya is a 2nd year student at the University of Virginia and is pursuing a degree in Computer Science with a minor in Data Science. She has been programing since 7th grade and has been involved in robotics during middle and high school. She has experience in languages like Python and C++, and has worked on web development projects.
As a Research Computing Student Worker, Diya has mainly focused on converting workshop materials into easily digestible markdowns. She has also been involved in providing feedback to improve the RC websites and has helped document workshop attendance through Jira ticket submissions. Diya is looking forward to working on machine learning projects in the future.
Education

B.A. Computer Science, Minor in Data Science
  University of Virginia (Expected 2026)
"
rc-website-fork/content/userinfo/ivy/_index.md,"+++
description = """"
title = ""Ivy and Rio Secure Environment""
draft = false
date = ""2025-01-15T11:45:12-05:00""
tags = [""ivy"",""vm"",""hipaa"",""linux"",""windows"",""security"",""jupyter"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
author = ""RC Staff""
aliases = [ ""/ivy"" ]
layout = ""single""
+++

The UVA secure environment consists of Ivy virtual machines (Linux and Windows) and Rio HPC. Researchers can use Ivy and Rio to process and store sensitive data with the confidence that the environment is secure and meets requirements for HIPAA, FERPA, and certain controlled-access data (e.g. dbGaP, NIMH NDA, etc.). However, projects involving CUI or ITAR data cannot access Rio at this time. To access the High security Rio HPC, researchers need to request an Ivy Linux VM which serves as a login node.
{{< systems-boilerplate >}}
Overview
Ivy provides virtual computing environments (virtual machines) specifically designed for interactive and small-scale analysis of highly sensitive data. Ivy Linux VMs can also act as a frontend for accessing the Rio HPC environment, which is optimized for large-scale analysis of sensitive data. Project-specific storage volumes are seamlessly mounted to the VMs and made accessible on the HPC system, facilitating smooth transitions between tasks performed on the VM and the HPC environment. In order to obtain access to either system, users must 

Submit an account request,
Complete the Information Security Awareness Training, and

Ensure their personal computer meets all High Security VPN requirements.


Requesting Access

Security Training
High Security VPN
Storage
Virtual Machines
Using the Rio HPC System
Data Transfer In/Out of Ivy



Requesting Access

{{% highlight %}}
  {{% pi-eligibility %}}
{{% /highlight %}}
Access to Ivy resources is project-based, limited to PIs and their designees, and requires approval. Once a project is approved a PI and her/his researchers must sign a RUDA (one for every researcher on each project).
Request an Ivy Account

Security Training {#training}
In order to use Ivy, researchers must complete the High Security Awareness Training (HSAT). This training takes approximately 10 minutes to complete.
Please complete the training at the following link: https://in.virginia.edu/hsat-training.

High Security VPN
The High Security VPN (HSVPN) allows researchers to connect to Ivy securely both on and off grounds. In order to use the HSVPN, users must ensure that their personal machines meet the following requirements. More information on HSVPN compliance can be found on the ITS website: https://in.virginia.edu/vpncheck


Install the Cisco AnyConnect Secure Mobility Client.
    This can be found at the UVA ITS Software Gateway. Be sure to install the version of VPN Client HS 4.6 that is compatible with your personal computer's operating system. More detailed instructions for installing the VPN client can be found on the ITS website.


Install Opswat.
    Opswat checks if your computer is compliant with HSVPN requirements. Opswat can be downloaded from the UVA ITS Software Gateway.


{{% callout %}}
If your personal machine's operating system is no longer supported and does not allow for disk encryption, having OPSWAT installed will not resolve the issue. The recommended solution is to upgrade the operating system or acquire a device with an updated OS that meets these security requirements.
{{% /callout %}}

Install Anti-malware software (Windows Defender recommended).
    Anti-malware software must be installed on your machine. Windows Defender is behavioral-based antimalware software and meets UVA's HSVPN requirements. Windows Defender can be downloaded from the UVA ITS Software Gateway.



Connecting and Signing In
1 Authentication

You will sign in to all Ivy resources using your UVA computing ID and Eservices password. Because of Ivy's high security requirements, your Eservices password must be changed every 60 days.
Need help resetting your Eservices password?
Reset Your Password
If you are working from a secure Health Systems workstation you are ready to connect. If you are working from elsewhere on or off Grounds you will need Duo MFA and a High Security VPN connection.

2 Duo MFA


To connect to the Ivy environment with VPN you will need to install the Duo Mobile multi-factor authentication (MFA) app on your smartphone.

Get Duo for iPhone in the App Store
Get Duo for Android on Google Play

In the context of Ivy, Duo allows you two ways to provide a second factor of authentication beyond your password: via a random 6-digit key, or via a push message direct to your phone.
Set Up Duo

3 High Security VPN

With your UVA computing ID, Eservices password, and Duo Mobile in hand, you must run the Cisco AnyConnect software to start a UVA High Security VPN connection every time you use any Ivy resource. AnyConnect will authenticate to the UVA network using a digital certificate installed on your workstation. 
More information on VPN from ITS:

High Security VPN installation and connection instructions.
    How to create, install, and use digital certificates.
  
Learn More about UVA VPN

Once you have completed these three steps, you will be connected to the secure Ivy network. From there you can connect to a Virtual Machine, or use a web browser to access JupyterLab.

Storage
Ivy VM and Rio HPC have a pool of over 2 petabytes of Network Attached Storage shared amongst users. A PI specifies the storage space s/he would like to have when requesting access to either of these environments. Virtual machines do not come with any significant disk storage of their own. 
Virtual Machines
A virtual machine (VM) is a computing instance dedicated to your project. Multiple users can sign in to a single VM.
Virtual machines come in two platforms, Rocky 8 Linux and Windows Server 2019. Each platform is available in numerous instance types. Refer to the grid below for specifics.
{{< pricing ivy >}}
Once created, your instance will be assigned a private IP address that you will use to connect to it (in the format 10.xx.xx.xx). VMs exist in a private, secure network and cannot
reach outside resources on the Internet. Most inbound and outbound data transfer is managed through the Data Transfer Node (see below).
Connecting to your VM
Before connecting to your VM, you must run the High Security VPN. Make sure that you have the VPN client installed on your laptop/desktop.
Next, you will need to know two pieces of information:
* The type of VM that you have, either Windows or Linux;
* The IP address of your VM (e.g., 10.xxx.xxx.xxx).
The steps for connecting to the VM will depend on the type of VM and, to a lesser extent, the operating system of your laptop/desktop (i.e., MacOS or Windows).
To connect to a Windows VM from a Mac, you will need the Microsoft Remote Destop application which you can  download here .
Windows laptops/desktops already have the Remote Desktop Connection application installed.
STEPS TO CONNECT TO YOUR VM
Follow the steps below for the type of VM that you have:




Connecting to a Windows VM



 Start the High Security VPN
           Run the Remote Desktop application (see comment above for installing this application on Macs)
           Enter the IP address for your VM
           Sign in with your Eservices password and your computing ID prefixed by ESERVICES as the username (i.e. ESERVICES\mst3k)







Connecting to a Linux VM



 Start the High Security VPN
           Open a web browser and enter the IP address for your VM (e.g., https://10.xxx.xxx.xxx) 
           If you get a warning message, you may need to click on Advanced Settings and/or a Connent Anyway option, depending on your web browser
           Use your Netbadge credentials to log in
        




In addition to connecting to a Linux VM through a web browser, you have the option of connecting with an ssh client. To do this, follow these steps:

Start the High Security VPN
Open the ssh client on your laptop/desktop (Terminal application on a Mac or Command Prompt on a Windows PC) and type:  ssh mst3k@10.xxx.xxx.xxx, where mst3k is replaced with your user ID.
When prompted for a password, use your Eservices password.

Software
Every virtual machine (Linux or Windows) comes with a base installation of software by default. These help researchers by providing the basic tools for data processing and manipulation. Additional software packages are pre-approved and available for installation upon request. See the lists below for options.
Preinstalled Software
{{< rawhtml >}}




PREINSTALLED Linux Software


Click on each for details:

            {{% ivy-approved-software os=""Linux"" installation=""preinstalled"" category=""all"" %}}
        






PREINSTALLED Windows Software


Click on each for details:

            {{% ivy-approved-software os=""Windows"" installation=""preinstalled"" category=""all"" %}}
        




{{< /rawhtml >}}
Python/R Packages - Anaconda Python and R packages are available to users through the normal pip, conda, and CRAN and library installation methods.
ADDITIONAL APPROVED SOFTWARE (Available by Request)
If you require additional software not listed, you must submit a request. Requests are reviewed by the UVA ISPRO office for security and regulatory compliance and, if approved, will be installed for you.




ADDITIONAL Linux Groups


Click on each for more information:


All Packages
Bioinformatics
Data Analysis
Database Software
Image Processing


Software Details for Linux






ADDITIONAL Windows Groups


Click on each for more information:


All Packages
Bioinformatics
Data Analysis
Database Software
Image Processing


Software Details for Windows




To request installation of optional software packages, please use the web request form provided through this link:
Request Ivy Software
Installing Python Packages on Your VM
CREATING CONDA ENVIRONEMENT
Researchers often require Python packages that are not included in the base installation of Anaconda. Users can install additional Python packages on their VMs using conda environments. Conda environments allows users to install packages in isolated environments to avoid version conflicts with other users on the VM.
Windows


Launch ""Anaconda Prompt"" from the Start Menu.


From the prompt, issue the command:
conda create -n my_env package1 package2
where my_env is the name you wish to give your new conda environment, and package1 and package2 are the names of the Python packages you want to install.


To activate and use your new environment, issue the command:
conda activate my_env


Linux


Log into your VM via SSH or log in through your web browser and launch the Terminal.


From the prompt, issue the command:
conda create -n my_env package1 package2
where my_env is the name you wish to give your new conda environment, and package1 and package2 are the names of the Python packages you want to install.


To activate and use your new environment, issue the command:
conda activate my_env


Creating a Conda Environment with a Specific Python Version
If you require a specific version of Python, you can create a new conda environment with:
conda create -n my_env python=2.7
INSTALLING PACKAGES
After creating your conda environment, you can install additional libraries with pip and conda.
Installing Packages with pip


Use pip from the command line to install individual packages:
pip install numpy


You can search for a package:
pip search panda


To see which packages you have installed already:
pip list


You can install packages listed in a requirements.txt file (one package per line): 
pip -r requirements.txt


To save a list of your currently installed packages in a requirements.txt file:
pip freeze > requirements.txt


Installing packages with conda
conda works similarly to pip.


To install a package:
conda install scipy


To search for a package:
conda search scipy


And to list all packages in your environment:
conda list


Once installed on your VM, packages will persist and you will not need to install them again. You will only need to import them again in your code.
Scheduled Maintenance
Beginning Sunday, April 14, your Ivy virtual machine (VM) will be rebooted on the 2nd Sunday of each month between 5:00 a.m. and 6:00 a.m. EST while RC engineers install security updates. Any sessions running during this period will be terminated. Windows and Linux VMs will be rebooted at the same time.
If you have any questions or problems with your software applications after the security updates have been installed, you may contact our user services team.

JupyterLab Notebooks
{{% callout %}}

JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular: write plugins that add new components and integrate with existing ones.

Learn more about Jupyter
{{% /callout %}}

Using the Rio HPC System
Access
Access to the Rio HPC requires an Ivy Linux VM to serve as a login node. Similar to other Ivy VMs, access to the Rio HPC is project-based. For details on requesting an Ivy Linux VM and accessing it, please refer to the instructions provided above. Please note that PIs must specifically request for their associated Linux VM to be provisioned as a frontend to Rio. Access to Rio from the VM is not granted by default.
As outlined above, VMs are available in various sizes. Please request a VM that is appropriately sized for your specific workflow. For larger groups or projects involving computationally intensive tasks, we recommend selecting a larger VM, with a preference for Small or above. 
Request Ivy VM for Rio
System Details
HARDWARE CONFIGURATION
Currently, Rio comprises 39 compute nodes, providing a total of 1,560 x86 64-bit compute cores. Each HPC node is equipped with 375 GB of RAM to accommodate memory-intensive applications. Rio also includes an NVIDIA HGX H200 GPU, and additional GPU nodes designed to support AI and machine learning workloads will be integrated in the near future.
JOB QUEUES
Similar to our clusters Rivanna and Afton in standard security zone, Rio is a managed resource. Users must submit jobs to queues controlled by a resource manager, also known as a queueing system. The manager in use on Rio is Slurm. Slurm refers to queues as partitions because they divide the machine into sets of resources. There is no default partition and each job must request a specific partition. Partitions and access policies are subject to change, but the following table shows the current structure. Detailed information on Slurm and instructions for submitting jobs to the HPC can be found here. 
For an introduction to the Rio HPC system, please see our tutorial.
Data Transfer In/Out of Ivy/Rio {#data-transfer-in-out-of-ivy}
Moving sensitive data into the Ivy VM platform (and Rio) is possible through a secure Globus DTN (data transfer node). The Ivy DTN is connected to a pool of secure storage called ‚ÄúHigh-Security Research Standard Storage‚Äù, which in turn is connected to Ivy VMs. Only active research projects using Ivy virtual machines can use this service.

How to Connect to the DTN and Transfer Files
Before transferring files to Ivy, you will need Globus installed on the computer you are transferring data from. Globus can be downloaded from https://www.globus.org/globus-connect-personal.


Ensure that you are NOT connected to the HSVPN. Data transfer will not work if you are connected to the HSVPN.


Open Globus in your web browser: https://app.globus.org/file-manager. When logging in, select University of Virginia and log in with Netbadge.


Once you are in the Globus File Manager, select the two-panel view by clicking the two-panel button beside the Panels button in the top-right corner of the page. This should open a second panel on the page, so that you have two side by side.


In one panel, click on the Collections field and select your computer. You can then click to the directory that contains the data you want to move, or type the path to the directory in the Path field. Click the files or folders you want to transfer to select them.


In the remaining panel, click on the Collections field and search for and select the UVA IVY-DTN. Select the storage share to which you want to transfer data. (Unless you are part of multiple Ivy projects, you should only see one storage folder.)


Click the Start button beneath the first panel (should be highlighted) to begin the data transfer.


Once the data transfer is complete, you will be able to access the data in your VM by clicking the High-Security Research Standard Storage shortcut on your VM's desktop.




"
rc-website-fork/content/userinfo/faq/storage-faq.md,"+++
description = """"
title = ""Storage FAQs""
draft = false
date = ""2020-02-20T15:15:12-05:00""
tags = [""storage"",""faqs"",""data""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""storage""
toc = true
+++

Accidental File Deletions
Why Lease Storage
Leased Storage Options
File Transfer with Globus
Permission Denied Error with Globus


Help! I deleted my files accidentally! What can I do?

For your home storage, the directory is /home/.snapshots . Snapshots are created once per day. Find the date you wish to find the snapshot for and navigate to your computing id. 
For GPFS Research Project (leased) storage, the directory is /gpfs/gpfs0/project/.snapshots.
Neither Research Standard (leased) nor scratch storage is backed up in any way.

Why should I lease storage?
Leasing storage from Research Computing means that you do not have to run your own data server or backup system.  You can lease storage for lab data without using any of the Research Computing computational resources, or you can lease storage for use with our computing facilities.
What are my options for leased storage?
Research Computing offers two tiers of leased storage, Research Standard and Research Project. Please see our storage page for details.
Where can I learn more about the Globus file transfer tools?
Globus maintains a well documented FAQ webpage that answers common questions related to security, file transfer and sharing, Globus endpoints and the command line interface (CLI) tools.
I keep getting a 'Permission Denied' error when trying to transfer my files through Globus. What can I do?
If you are certain that you have write permissions in the target directory and read permissions in the origin directory, you may be experiencing a common error with hidden files such as .AppleDouble or Thumb.db. You can resolve this issue by opening the Transfer and Timer Options menu at the center of the Globus screen (between the two blue ""Start"" buttons). Then check the box for Skip files on source with errors to tell Globus to ignore files that trigger a 'file not found' or 'permission denied' error."
rc-website-fork/content/userinfo/faq/rivanna-faq.md,"+++
description = """"
title = ""Rivanna and Afton FAQs""
draft = false
date = ""2024-12-03T01:45:12-05:00""
tags = [""hpc"",""rivanna"",""faqs"",""supercomputer""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++

General Usage
Allocations
Research Software
Job Management
Storage Management
Data Transfer
Downloading Files
Other Questions


General Usage
How do I gain access to Rivanna/Afton?
A faculty member must first request an allocation on the HPC system. Full details can be found here.
How do I log on to Rivanna/Afton?
Use an SSH client from a campus-connected machine and connect to login.hpc.virginia.edu. Instructions for using ssh and other login tools, as well as recommended clients for different operating systems, are here. You can also access the HPC system through our Web-based interface Open OnDemand or FastX.
Please note that the old Domain Name System (DNS) entries for logging into Rivanna/Afton HPC have been removed. Please refer to the table below for the updated login names.
|Old|New|
|---|---|
|rivanna.hpc.virginia.edu -> | login.hpc.virginia.edu|
|rivanna-desktop.hpc.virginia.edu -> | fastx.hpc.virginia.edu|
|rivanna-portal.hpc.virginia.edu -> | ood.hpc.virginia.edu|
{{% off-campus %}}
How do I reset my current password / obtain a new password? {#how-do-i-reset-my-current-password-obtain-a-new-password}
Access to the HPC cluster requires a valid ITS (Netbadge) password. If you are unable to log in, you should first try resetting your ITS password here.  If the problem persists, contact ITS through their online Helpdesk.  Keep in mind that ITS requires annual resetting of your password.  If you see a ""password expired"" message, you will need to change it through ITS.
What happens to my account when I leave UVA?
ITS controls access to the University‚Äôs computing resources, so when you or your students leave, you/they may lose access to many of these resources. Sponsored accounts allow people who work or volunteer at UVA, but who are not paid by UVA, to access the University‚Äôs computing resources. Researchers with sponsored accounts cannot request RC services but they are allowed to use the systems we manage as members of a Grouper (requires VPN connection) group controlled by a UVA Principal Investigator (PI). Details on sponsored accounts are posted on the ITS sponsored accounts page.
Why am I seeing WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED when I log in?
Some users logging in through ssh may encounter this error message. If you receive this message, please see our instructions on how to clear this error.
When I try to log in with ssh, nothing happens when I type my password!
When you type your password, the ssh program does not echo your typing or move your cursor.  This is normal behavior.
When running Firefox on the HPC system, I get : ""Firefox is already running, but is not responding. To open a new window, you must first close the existing Firefox process, or restart your system."" What can I do?
From a terminal in your home directory on the HPC system, run the commands:
rm -rf ~/.mozilla/firefox/*.default/.parentlock
rm -rf ~/.mozilla/firefox/*.default/lock
When should I use FastX Web, when should I use an Open OnDemand Desktop session?
Both allow you to run applications with graphical user interfaces in a Linux Desktop environment.
Open OnDemand Desktop:
* Runs your session on allocated resources on a compute node.
* Ideal for running compute-intensive single-node applications with graphical user interface.
* Does not require a VPN connection from off-Grounds locations.
* Recommended practice for running interactive jobs (particularly for coursework with a reservation).
FastX Web:
* Runs all users' sessions on a single frontend node.
* Good for light-weight file management, script editing.
* Requires a VPN connection from off-Grounds locations.
How can I view .pdf or .csv files on Rivanna/Afton?
For .pdf files, run the command:
atril filename.pdf
You can also open Atril from a FastX or Open OnDemand desktop environment from the Applications‚Üíffice menu.
The atril command can also be used to display image files, e.g. .png and .jpg files. Or you may use eom FILE (Eye of MATE) from a terminal.  Alternatively, you can open Eye of MATE from the MATE Desktop menu Applications‚ÜíGraphics.
For .csv files, run the command:
oocalc filename.csv
where filename is a placeholder for the specific filename. The oocalc command invokes the LibreOffice spreadsheet program ""Calc.""  If logged on to a FastX or Open OnDemand Desktop, use the menu Applications‚ÜíOffice to access it.
Why does it hang on log in? Why do OpenOnDemand interactive apps give conflicting package errors?
It could be that your .bashrc file is loading too many or conflicting modules respectively. See our Modules page on how to load modules within best practices. If your .bashrc file is getting too crowded, you should replace it with the default here:
```
Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
    fi
PS1=""\s-\v\$""
alias vi='vim'
```

Dedicated Computing
Service Unit Allocation or Dedicated Computing--What is the right HPC service for me?
The following table might help you decide which model suits you the best.




SU Allocations
Dedicated Computing




Description
Access to variety number of cores and node types
Access to a [set of] dedicated node[s] of prespecified hardware with fixed number of cores


Lifetime
Standard alloc: 1 year; paid alloc: unlimited
5 year (expected hardware EOL)


Queue times
System load dependent; standard alloc: default priority; paid alloc: shorter than standard allocation; no preemption
None(assuming no contention for the dedicated resources by other group members)


Max walltime
3-7d
Typically 1 to 5 years


Ideal workload
Even or bursty
Even


GPU or largemem
Yes
If that node type was purchased (can be expensive!)



To learn about RC's service models and how to request and access each, plese refer to here.
Can I still get access to HPC allocations without having to pay?
Yes, standard and instructional allocations remain available free of charge.
What will happen to my unused SUs that I purchased before Jan 7, 2025?
The service unit balance of your paid allocation will carry forward as is. Please be aware of the new service unit consumption rates which are more directly tied to the hardware type number of cpu cores, memory, and specialty hardware (e.g. GPUs) requested. 
What hardware options are available under the Dedicated Computing services?
See here.
Can the node I purchased under the Dedicated Computing model be configured as my personal login node to the HPC system?
No. Dedicated computing hardware is configured as compute nodes only. 

Allocations
What is an allocation?
Time on the HPC system is allocated as Service Units (SUs). One SU corresponds to one core-hour. Multiple SUs make up what is called an allocation (e.g., a new allocation = 100K SUs). Allocations are managed through Grouper (requires VPN connection) groups. These groups must be created by the Principal Investigators (PIs) prior to submitting an allocation request. Full details can be found here.
How can I request an allocation?
The different Service Unit (SU) allocation types are explained in this article. It includes links to our allocation request webforms.
How do I check my allocation status on Rivanna/Afton? {#how-do-i-check-my-allocation-status-on-rivanna}
Run the allocations command.  The output may look like this:
```
Name           Balance  Reserved Effective Available

rivanna_alloc  9885.811 1000.000  8885.811  8885.811
for more information about a specific allocation,
 run: 'allocations -a '
```
The Balance column shows the total of unused service units (SUs); the Reserved column shows the number of SUs held for current active jobs (pending or running). The Effective and Available columns show the difference of Balance and Reserved, i.e. the amount of SUs available for future jobs. After a job completes, the SUs actually consumed will be deducted from the allocation Balance and any SUs unused by that job will be released from the Reserved pool.
In all cases you can only submit additional jobs if the available SU
amount is sufficient to cover the full SU request for the jobs.
You do not need any allocation service units to access the frontend or files in
your directories as long as your account is active.
If you don't see your allocation, it may mean that you've been removed from the allocation group or that your allocation has expired.
How do I check an allocation's expiration date?
To check an allocation's expiration date run allocations -a <allocation group> command.  Alternatively, run mam-list-allocations.
Only Standard Allocations and Instructional Allocations have an expiration date. PIs may request renewal of their expired allocation. Purchased Allocations never expire.
How are Service Units Reserved?
When a job is submitted the account manager calculates the required maximum amount of Service Units (SUs) using the assumption that the job will run the full amount of time requested. These SUs are held in reserve as a ""lien"" against the allocation charged for the job.  When the job completes the lien is released and the actual SUs consumed
are deducted from the allocation balance. See How do I check my allocation status on Rivanna/Afton? for specifics.
How are Service Units charged for specialty hardware, e.g. GPU and large memory nodes?
Service Units (SUs) serve as a general single currency on the HPC system. SUs in a given allocation account can be used freely to run jobs on nodes in the standard, parallel, gpu and interactive queues.  Please note that the SU charge rate is different for some of the specialty hardware, e.g. the GPU nodes, as listed here.
How is my ""Fairshare"" impacted by the changed SU charge rates?
Your Fairshare value is driven by your SU consumption and affects the priority of jobs that you submit. This is true for both the standard and purchased allocations. If the changes to the SU consumption rates increases your SU consumption you will see a proportional impact on your Fairshare value.
How do I create a group or manage members in my allocations?
You must use the Grouper (requires VPN connection) interface to create the group, and you must have administrative access to the group. New groups will require two owners who hold active roles at UVA, as well as a third departmental owner. Group owners will be required to perform an annual attestation of group membership. If group owners do not complete attesting to the validity of their group, the members will be automatically removed from the group. Note that If you need to set up a new group or modify a group that was created after November 28th, 2023, go to Grouper. Legacy MyGroups groups created before November 28th, 2023, can be accessed through the ""Legacy MyGroups"" folder on  Grouper.
How do I check allocation usage of individual group members?
Please visit here to see how to generate an allocation usage report.
How can I estimate the expected SU consumption for a new job?
We have developed a utility in Open OnDemand (OOD) called the Slurm Script Generator. This tool generates a Slurm script based on the parameters specified by the user. Additionally, it estimates the number of Service Units (SUs) that will be billed based on the time requested in the script.
I submitted a job and received an error ‚ÄúInvalid account or account/partition combination specified‚Äù. What should I do?
All resource requests through the Open OnDemand interactive apps or through slurm batch jobs require you to specify an allocation for your job. If you do not input an allocation name, you will get this error.
If you are experiencing this error and you have input an allocation, verify what allocations you are a part of as described here. Verify that you are inputting the allocation name exactly as you see it all in lowercase.
I submitted a job and receive an error ""Insufficient balance. Applying funds failure for JobId="".  What should I do?
The error indicates that your allocation group does not have enough service units to execute the job. Check your allocation status as described here. Also verify that your allocation has not expired, see here.
Only Standard Allocations, and Instructional Allocations have an expiration date. PIs may request renewal of their expired allocation. Purchased Allocations never expire.

Research Software
How do I use research software that's already installed?
We use the lmod system for managing software environments. Learn more about how to use lmod.
Does RC install research software?
Our staff will install software onto the HPC system if it is of wide applicability to the user community. Software used by one group should be installed by the group members, ideally onto leased storage for the group.  We can provide assistance for individual installations.
For help installing research software on your PC, please contact Research Software Support at res-consult@virginia.edu.
Is there any other way to install research software that I need?
Some groups and departments have installed a bundle of software they need into shared space.  Please see your departmental IT support personnel if your department has its own bundle.
Can I run this Docker container on Rivanna/Afton?
We do not run Docker on the HPC system.  Instead we use Apptainer.  Apptainer can run Docker images directly, or you can convert a Docker image to an Apptainer image.  To import existing Docker images, use the apptainer pull command.
module load apptainer
apptainer pull docker://account/image
Software images built by Research Computing are hosted on Docker Hub. For example, to pull our PyTorch 1.5.1 image, run:
apptainer pull docker://uvarc/pytorch:1.5.1
Please visit this page for more details.
Can I run application/container X on a GPU?
Please check the user manual for your application/container before running on a GPU. For instance, scikit-learn does not have GPU support; hence using GPUs for scikit-learn will not help with your job performance but will only cost you more service units (see SU charge rate here) and prevent other users from using the GPUs.
https://scikit-learn.org/stable/faq.html#will-you-add-gpu-support
How can I make my Jupyter notebook from JupyterLab to run as a batch job on Rivanna/Afton?


Capture the information that you use to start up a JupyterLab session.  It helps to take a screenshot of the web form where you enter the partition, number of cores, amount of memory, etc.  You will need that information for requesting resources on a compute node.


Note which kernel is used to run your notebook.  This information will be needed later.


Convert the notebook to a regular script.  To do this, go into the notebook that you want to convert.  In the upper left corner, click on File > Export Notebook As > Export Notebook to Executable Script .  This will download the script onto your laptop.   On my computer, this leaves a blank window on my screen.  But, if I close that tab on my browser, the tab with the notebook returns.  I‚Äôm now down with the notebook and can terminate the session.


Upload the ‚Äúexecutable script‚Äù to the HPC system. In Open onDemand dashboard view, on the black ribbon across the top, click on Files > Home Directory.  This will open a page that shows the files that you have in your home directory on Rivanna.  At the top of the page, toward the right, is a button labelled ‚ÄúUpload‚Äù.  Click on that button.  In the dialog box that appears, click on ‚ÄúChoose File‚Äù.  This will allow you to go to the downloaded file and select it.


Create a Slurm script to run your code.  The Slurm script list the resources and instructions that are needed to run your ‚Äúexecutable script‚Äù.   See the following link:
https://www.rc.virginia.edu/userinfo/hpc/slurm/


Open a terminal window on the HPC system, and move to the location where your scripts are.  We recommend using the web-based FastX application (see below). Once in a terminal window, type sbatch followed my the name of your Slurm script.
https://www.rc.virginia.edu/userinfo/hpc/login/#remote-desktop-access



Job Management
How do I submit jobs?
You submit jobs by writing a Slurm script and submitting it with the  sbatch command.  Please see our Slurm documentation.
If you would like assistance in generating Slurm scripts, please check out our Slurm Script Generator. Simply input the parameters of your job to get a fully-working Slurm script.
How do I submit an interactive job?
If you wish to run a program that requires a graphical user interface or generates other graphics for display, such as a plot or chemical model, use one of the Open OnDemand interactive apps.  Several are available, but if you one you wish to use isn't in the list, submit an interactive Desktop request.
If you will be using the command line for your interactive job you may use the locally-written ijob command. The minimum required options are -A and -c  for allocation and number of cores. Run ijob -h for a list of all options.
For more information see the documentation.
What queues can I use?
After logging in, run the command qlist to see a list of queues and their availability.  Run qlimits for the restrictions on submitting to each queue.
How do I choose which queue to use?
Queues are set up to emphasize one-core (serial or threaded), multi-node parallel, and specialty hardware including large-memory nodes and GPUs.  

Serial jobs requiring only 1 compute node: standard
Parallel jobs requiring up to 50 compute notes: parallel
Jobs requiring the use of GPUs: gpu
Jobs for interactive sessions or quick tests of code:  interactive

More information about queue policy is at the HPC homepage.
How do I use the interactive queue?
The interactive queue is ideal for code development or other short interactive jobs that require active monitoring. Examples include Slurm ijobs and OOD interactive apps like JupyterLab, RStudio Server, MATLAB, etc. The interactive queue has a time limit of 12 hours per job, and users can request up to a maximum of 24 CPU cores or 2 GPUs and up to 216G of CPU memory across all jobs. For example, you can run 24 serial jobs or one 24-core job. To request a GPU on with OOD apps, you'll be asked if you want to use a GPU. If yes is selected you can choose one or two GPUs. In slurm, the user must specify the --gres=gpu flag for GPU access. If two GPUs are desired in Slurm, you can specify --gres=gpu:2.
How do I check the status of my jobs?
From a terminal, run the command squeue -u computingID. Replace computingID with your specific UVA computing ID.  From Open OnDemand, use the Job Viewer and select ""Your Jobs"" as the filter.
If reporting a problem to us about a particular job, please let us know the JobID for the job that you are having a problem with.
Why is my job not starting?
Several things can cause jobs to wait in the queue. Paid allocations have priority over standard allocations. If members of your group under the same PI using the same quality of service level (i.e. paid or standard) have consumed a large amount of compute time in the recent past, the ‚Äúfair share‚Äù algorithm will give other users outside of your group higher priority access ahead of you. Finally, the queue you requested may simply be very busy. If your job is pending there will be another field with the reason; if it is ‚ÄúResources‚Äù that means that the resource you requested isn‚Äôt available. If the reason is ‚ÄúPriority‚Äù it means that a job with higher priority than yours is running. Your job will rise in priority as it waits so it will start eventually. 
How can I check when my job will start?
To request an estimate from the queueing system of your start time, run 
squeue -u $USER --start
for all your jobs, or
squeue -j <jobid> --start
for a specific job. Slurm will provide an estimate of the day and time your job will start.
Why was my job killed?
Usually this is because you inadvertently submitted the job to run in a location that the compute nodes can't access or is temporarily unavailable.  If your jobs exit immediately this is usually why.  Other common reasons include using too much memory, too many cores, or running past a job's time limit.
You can run sacct:
```
[mst3k@udc-ba36-27:/root] sacct
       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode

159637       ompi_char+   parallel  hpc_admin         80  COMPLETED      0:0
159637.batch      batch             hpc_admin          1  COMPLETED      0:0
159637.0          orted             hpc_admin          3  COMPLETED      0:0
159638       ompi_char+   parallel  hpc_admin        400    TIMEOUT      0:1
159638.batch      batch             hpc_admin          1  CANCELLED     0:15
159638.0          orted             hpc_admin         19  CANCELLED  255:126
```
If it's still not clear why your job was killed, please contact us and send us the output from sacct.
Why can't I submit jobs anymore?
In order to be allowed to submit jobs, you must not be overallocated with your /scratch usage and you must have some remaining service units. There is a limit of 10 TB of space used per user in each /scratch directory and if you exceed either of those limits, you will not be able to run jobs until you clean up.  To check whether this is the case, run
hdquota -s
If you have not exceeded the limits on /scratch, check whether your account has allocation units remaining by running
allocations
Why do I get sbatch error: Batch script contains DOS line breaks
If you use a Windows editor to create Slurm batch scripts, when you try to run them you may encounter an error
sbatch: error: Batch script contains DOS line breaks (\r\n)
sbatch: error: instead of expected UNIX line breaks (\n).
Windows and Linux use different conventions to mark the end of each line.  Many applications on the HPC system, such as compilers, Matlab, etc., understand Windows end-of-line markers, but the shell does not.  This is easy to fix by running the dos2unix command
dos2unix myscript.slurm
It will not hurt to run dos2unix on a file that doesn't need it. Sometimes you get {^M} character at the end of every line when the file was imported from Windows environment. dos2unix usually takes care of the problem, but not 100% all the time.
How do I check how much SU's my job has burnt?
To find out how many Service Units (SUs) a specific job has consumed, users can run the following command. Here the value under the Amount column shows the amount of SUs consumed. The time-frame can be controlled using the -s(starting time) and -e(end time) flags.
$  mam-list-transactions -a <allocation-name> -s 2024-11-01 -e 2024-12-03  # -s:starting date   -e: end date
How do I check the efficiency of my completed jobs?
Run the command seff on the Slurm job ID:
udc-ba34-36-deepLearning$seff 40330441
Job ID: 40330441
Cluster: shen
User/Group: teh1m/users
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 2
CPU Utilized: 00:15:14
CPU Efficiency: 89.08% of 00:17:06 core-walltime
Job Wall-clock time: 00:08:33
Memory Utilized: 6.89 GB
Memory Efficiency: 58.76% of 11.72 GB
udc-ba34-36-deepLearning$
The output of this command is also contained in the email sent by Slurm once your job completes.
My jobs are failing due to an incorrect environment setup, but I am loading my modules and/or conda environments correctly in my job script. What is wrong?
When submitting jobs using sbatch, Slurm will remember the environment that you were working in. This means that loaded modules, activated conda environments, and generally all the environment variables set in the terminal prior to job submission will follow through into your job. A way to avoid this issue from happening is to include the following line into your Slurm script:
```
SBATCH --export=NONE
```
This makes it so that Slurm does not carry over any environment variables into your running job. Be sure to include the necessary module load or conda activate commands in your script to run your code. If you are using srun in your Slurm script, see an example script here

Storage Management
What storage options are available to me to use on Rivanna/Afton?
All users are provided a 200-GB home directory for longer-term storage.  This directory provides ""snapshots"" though it is not backed up.  Each user also is provided 10TB of temporary ""scratch"" storage accessible as /scratch/$USER where $USER will stand for your ID.  Scratch storage is fast but is not backed up in any way.
If the free storage is not sufficient, you need snapshots of your files, or you wish to share space among a research group, the group should lease storage.
Why should I use /scratch storage?
Scratch storage is fast and provides a large quantity of free space.  However, there are limits on the number of files and the amount of space you may use.  This is to maintain the stability and performance of the system.  Please review our scratch filesystem policy for details. If you use or expect to use a large number of files please contact us.
How do I obtain leased storage?
Research Computing offers two tiers of leased storage, Research Standard and Research Project. Please see our storage page for details.
How do I check my disk usage?
Run hdquota on a HPC frontend.
How do I check my /scratch usage on Rivanna/Afton?
Run the command hdquota -s:
hdquota -s
If you have used up too much space, created too many files, or have ""old"" files you may be regarded as ""overallocated"". Please note that if you are overallocated, you won't be able to submit any new jobs until you clean up your /scratch folder.
If I'm over my disk quota in either in my /home directory or my /scratch directory, how can I determine my disk usage?
You can run the following command from your /home or /scratch directory to see how your disk usage is distributed
across subdirectories, and where you need to remove files. You can increase max-depth to go further down in the directory structure.
du . -h  --max-depth=1|sort -h -r
If I'm over my file limit in /scratch, how can I determine where all the files are located?
From your /scratch directory, run the following command to determine where you need to remove files.
find . -type f | cut -d/ -f2 | sort | uniq -c
How long can I store files in /scratch?
/scratch is designed to serve as fast, temporary storage for running jobs, and is not long-term storage. For this reason, files are periodically marked for deletion from all /scratch directories. Please review the /scratch filesystem policy for more details.  Store longer-term files in your home directory or purchased storage.
How do I share data in my /scratch or leased storage with a colleague?
To share data from your /scratch directly with any other user, use Globus sharing.  If your colleague also has an account on UVA HPC, he or she does not need to set up a personal endpoint but can simply log into the uva#main-DTN endpoint and navigate to his or her /scratch directory to transfer the files.
If you wish to share data in leased space with a member of your group, be sure that permissions are set so that the group member can access your subdirectory.  The college can then simply use the data directly, or copy it elsewhere.  If you wish to share data from your leased storage to a colleague who is not a member of the group, use Globus sharing in the same manner as sharing /scratch.

Data Transfer
How do I transfer data from UVA Box to my /scratch directory on Rivanna/Afton?
Log into UVA HPC using the web-based FastX and launch the MATE Desktop interface. Then from the top menu bar, open firefox through the FastX desktop, in the upper right hand corner of the browser window you should see 3 horizontal bars. Click on that and then select Preferences from the drop-down window. In the new window scroll down until you see Downloads and select ‚ÄòAlways ask you where to save files‚Äô.  Then when you go to Box to download, a new window will pop up and if you click on ‚ÄòOther locations‚Äô, you can navigate to your scratch directory.
How do I transfer data from my /scratch directory on Rivanna/Afton to my UVA Box account?
Log into UVA HPC using the web-based FastX and launch the MATE Desktop interface. Then from the top menu bar, open firefox through the FastX desktop and log into your UVA Box account. Once logged in to box, click on the New + button (upper right) to upload a file/folder. In the left sidebar of the new window, select Other Locations/Computer/scratch/ to navigate to your scratch directory and select the files/folders you want to upload to your box account.
What Linux commands can I use to transfer files to/from Rivanna/Afton?
Smaller files can be transferred to/from Rivanna/Afton using scp, sftp, and rsync as well as standard FTP tools.
Larger files should be moved using Globus.
Read more about data transfer.
I need to push and commit code changes from Rivanna/Afton to my GitHub account. How do I set that up?
You must first generate an ssh key and then copy it to your git repository. Here are the instructions for generating the ssh key and what to do on your git page:


To generate an ssh key, see the following link: ssh key generation


Click on the drop-down menu next to my Git profile picture in the upper right corner; Select Settings; Click on SSH and GPG keys in the left column; Click on the New SSH Key button and followed the directions to upload your ssh key.
Make sure that the ssh key is in your authorized_keys file in your .ssh directory on Rivanna/Afton.


The next step is to clone the repository using the ssh link. If you have already cloned the repository using the http link and made a number of changes to your files, you won‚Äôt want to redo them.  Rename the directory that was created when you first cloned the repository. Then, re-clone the repository using the ssh link and copy all of the files you had changed to the new directory. Finally, push those changes back to the repository.


How do I add external or mapped network drives onto the Globus Path
When you first set up Globus, it only has access to certain folders of your local drive. You can add additional locations such as mapped network drives or external hard drives in the Globus Options/Preferences menu.
Windows: Right click the Globus icon > Options > + to add a new folder
Mac: Globus icon > Preferences > Access > + to add a new folder
Click the Up button in the Globus File Manager to navigate to higher level folders.
Downloading Files
What command-line tools are available on Rivanna/Afton for downloading files from web?
wget
wget can be used to download files over HTTP,HTTPS and FTP protocols. You can use wget to download files from a single URL or multiple URLs. For example to download a file from a website you can use the following command:
bash
wget https://example.com/file.zip
curl
In addition to what mentioned for wget, curl can be used to upload files to a server as well. To download a file from a website, you can use the following command:
bash
curl -O https://example.com/file.zip
axel
axel not only downloads files over different protocols, but accelerates the process by using multiple connections to retrieve files from the destination. Axel is available on Rivanna/Afton through module load axel.
The syntax for using axel over 10 connections is as follows:
bash
axel -n 10 http://example.com/file.zip
wget, curl or axel?
For rather small files of size <1GB, it might be easier to use wget or curl since module loading is not necessary. For large files it is recommended to use axel on a compute node. Below is a simple comparison between the download rate of these tools on a single core compute node on Rivanna/Afton:
| tool | 100MB | 1GB |
|------|------|------|
| wget | ~5s | 36s |
| curl | ~5s | 35s |
| axel | ~2s | 8s |  
Other Questions
What if my question doesn't appear here? Take a look at our User Guide.  If your answer isn't there, contact us."
rc-website-fork/content/userinfo/faq/_index.md,"+++
title = ""Frequently Asked Questions""
description = """"
author = ""RC Staff""
images = [
  """",
]
date = ""2020-02-16T09:55:56-05:00""
categories = [""userinfo""]
tags = [
  ""hpc"",
  ""ivy"",
  ""storage"",
  ""cloud"",
]
draft = false
quell_footer = true
layout = ""single""
+++








General
Do you have a general computing question?

Read our FAQ‚Ä∫












Rivanna and Afton
High Performance Computing Platforms

Read our FAQ ‚Ä∫














Ivy
Secure Data Computing Platform

Read our FAQ ‚Ä∫












Storage
Research Data Storage & Transfer

Read our FAQ ‚Ä∫





"
rc-website-fork/content/userinfo/k8s/deployments.md,"+++
author = ""RC Staff""
description = """"
title = ""Microservice Deployments""
date = ""2023-03-04T23:59:16-05:00""
draft = false
tags = [""compute"",""containers"",""infrastructure"",""docker"",""kubernetes"",""api"",""k8s""]
categories = [""userinfo"",""containers""]
images = [""""]
+++


  Kubernetes is a container orchestrator for both short-running (such as workflow/pipeline stages) jobs and long-running (such as web and 
  database servers) services. Containerized applications running in the UVARC Kubernetes cluster are visible to UVA Research networks (and 
  therefore from Rivanna, Afton, Skyline, etc.). Web applications can be made visible to the UVA campus or the public Internet.

Kubernetes
Research Computing runs microservices in a Kubernetes cluster that automates the deployment of many containers, making their
management easy and scalable. This cluster will eventually consist of several dozen instances, >2000 cores and >2TB of memory allocated to 
running containerized services. It will also have over 300TB of cluster storage and can attach to both project and 
standard storage.
{{% highlight-danger %}}
The research Kubernetes cluster is hosted in the standard security zone. It is suitable for processing standard sensitivity or internal 
use data. Highly sensitive data (PHI, FERPA, etc.) are not permitted on this platform. 
{{% /highlight-danger %}}

Design Principles
Deployments within the UVARC Kubernetes cluster are configured with a ""Desired State"" architecture. This means that deployments are
described in code (k8s YAML, Helm charts, Jsonnet & Kustomize files) and the cluster maintains the described state all the time. This is
often called the ""GitOps"" model, since the deployment files are code that can be tracked and versioned in a Git repository. As researchers
iterate on their application and build+test their containers, deployments themselves can be managed separately with new container versions.
This model has some distinct advantages for both research users and engineers:

Deployments should be defined in code. Hand-built deployments are as brittle and unreproducible as hand-made Docker containers. This helps maintain the state of applications as well as for disaster recovery.
We do not grant users command-line access to the K8S API. kubectl and helm require the overhead of user authentication, roles, permissions, and network connectivity to the control plane that are unnecessary.
Permissions in the GitOps model are granted via the deployment's Git repository, not at the cluster level.

Deployment Lifecycle
The lifecycle of applications themselves is different from, and should be independent from, various deployments of that application. 
Running your containerized application in Kubernetes requires you to think of two separate activities: (1) developing, testing, and building your 
application and its dependencies; and (2) deploying your application stack in the cluster.


Development - The first activity is generally well understood by researchers who may write their apps in Python, R, or other languages. As the 
app evolves, it is containerized using a Dockerfile and tested. Finally, more advanced projects will use automation tools such as GitHub Actions or 
Jenkins to build, test, and publish the application container(s) image to a container registry such as Docker Hub or GitHub Container Registry (GHCR).


Delivery - The second activity is less understood by researchers, since running docker locally for testing is different from cluster 
deployments. It is our belief that researchers should not be required to learn kubectl or other cluster management commands, instead simply defining their
deployment in code and letting automation tools take it from there. We suggest you use a separate repository for all your deployment files, so
that these two activities remain entirely separate.


This lifecycle is known to engineers and developers as CI/CD, or Continuous Integration / Continuous Delivery, as it describes how modern applications
are built, packaged, delivered, and deployed - each of which may take more than one form. 


Continuous Integration is the process of developers 
continually iterating on the features, logic, and inner-workings of their application stack. This may be as small as bug fixes and as large as a 
complete redesign. The final product of the CI stage is a deliverable that can be run in test, user acceptance, or production modes. CI tools help automate the packaging
and publication of that deliverable. In the case of microservices this is most often a container image that is ready to use. 


Continuous Delivery is
the process of taking that deliverable and running it in an environment such as a public cloud, a server, etc. However, the CD process is normally
more elegant than stopping the existing version of an application and replacing it. CD tools attempt to gently roll new versions into operation
without any service disruption. And, using normal performance health checks, if the new version does not gain a healthy state, the CD orchestrator will 
roll back the container version.


ArgoCD is UVARC's choice for a Kubernetes-based CD tool as it offers the ""desired state"" model described above, accepts a number of deployment formats, 
and is robust enough for distributed production clusters.
Here's a brief explanation of ArgoCD and the entire CI/CD lifecycle:
{{< youtube ""MeU5_k9ssrs"" >}}


Launching Your Application
The following ingredients come together to run your application


Namespace -
Service launches generally require the creation of a namespace if users do not already have one. Namespaces serve as logical 
and organizational dividers, keeping the management of the services of User A isolated from those of User B. Namespaces also
allow administrators to monitor or limit the maximum consumable resources (i.e. CPU and memory) for all deployments within the
namespace. 


Deployment -
The basic unit of your running application. This defines what container image to use, what it should be named,
how many replicas should run, what storage should be mounted and where, as well as resource limits.


Service -
A running deployment that expects incoming requests must be exposed as a service, over a port. This allows Kubernetes
to route traffic to the container(s). Some deployments, such as a recurring task or cron job, may not need a service 
or ingress definition.


Ingress -
For service deployments such as web servers that expect incoming requests, the ingress definition maps a hostname (example.pods.uvarc.io)
with a service. The UVARC cluster runs multiple ingress controllers to handle incoming traffic.


Secrets / env Variables -
The best practice for passing sensitive information (usernames, passwords, keys, tokens, credentials) into a running
container is to pass them in as encrypted environment variables called secrets. We use kubeseal for encrypting secrets 
into plaintext. This text can be added and committed to public repositories since their decryption key is stored privately 
in the cluster. Secrets can be consumed as env variables or as files mapped within the file hierarchy of the container.
Normal env variables can also be passed to the container by defining them within the deployment spec file or as a
config map that stores several key-value env vars.


Storage -
We offer the ability to mount from three pools of persistent storage:

Research Value Storage
Research Project Storage
Local Cluster Storage - priced by TB increments like Project Storage.




Observability
Observability is the process of debugging, monitoring, or determining the state of your applications
behind the scenes. We provide three levels of access into your microservices:

ArgoCD - A GUI to check the state of your deployments within Kubernetes.
Lens - A GUI to view pods, logs, shell into your pods, view storage and secrets, etc. Download Lens here
kubectl - Programmatic CLI access to the same resources as Lens.

Connecting Services
Kubernetes offers two simple ways to connect your microservices:


Inter-pod communication - When launching more than one container in your deployment specification, the
containers can communicate by name, without a service definition. Your containers would launch in the same
pod, which means they run on the same physical server and have immediate access to each other.


Service-based communication - Running pods are assigned an arbitrary internal IP address within the
cluster, and exposed internally within a namespace through service definitions (see above). Fortunately,
containers within a namespace are provided all other service addresses within that namespace
as env variables. This means that one of your pods, (e.g. MY_API) will be exposed via the name defined
in its specification (in all caps, e.g. MY_API_SERVICE), that can be consumed by other pods running within 
that namespace, simply by referring to that variable.


Division of Responsibilities
What we take care of:

Underlying physical infrastructure: Servers, networks, cabling, power, cooling.
Underlying hosts: Operating system, patching, mounts to cluster/remote storage.
Kubernetes API: Core k8s services across the cluster, high availability.
Kubernetes Ingress: The ability to map traffic to pods. With SSL as necessary.
Observability Tools: K8S Dashboard, ArgoCD Dashboard, Lens GUI to monitor and inspect your deployments.
Deployment templates: To help you get started.

What you take care of

Creation, versioning and maintenance of container image(s).
Maintenance of your deployment's scale, version, env variables and secrets.
Debugging your application as necessary.


Cluster Status
{{% k8s-grafana-embed %}}

Next Steps
Have a containerized application ready for launch? Or want a consultation to discuss your microservice implementation?
Request Access ¬†¬† {{< consult-button >}}"
rc-website-fork/content/userinfo/accord/environments.md,"+++
title = ""ACCORD Environments""
draft = false
date = ""2021-10-11""
images = [""""]
tags = [
    ""python"", 
    ""R"", 
    ""c/c++"",
]
author = ""Staff""
+++

Back to Overview
After creating a project and logging into the ACCORD platform, you will next choose an environment. The environments currently available on ACCORD are listed below. We welcome your suggestions for additional environments to be included in the future.



RStudio


                                RStudio is the standard IDE for research using the R programming language. 
            


JupyterLab


                                Jupyter Lab allows for interactive, notebook-based analysis of data. A good choice for pulling quick results or refining your code in numerous languages including Python, R, Julia, bash, and others.
            


Theia Python


                                Theia Python is a rich IDE that allows researchers to manage their files and data, write code with an intelligent editor, and execute code within a terminal session.
            

Theia C++


                                Theia C++ is a rich IDE that allows researchers to compile their own code.
                        



Runtimes & Limitations

Computing environments can be used for both short-term and long-term jobs.
For extended runs, you may close your browser tab and return later.
Outbound access to the Internet is restricted to package and library mirrors such as pip, PyPi, CPAN, CRAN, and others.
"
rc-website-fork/content/userinfo/accord/userguide.md,"+++
title = ""ACCORD User Guide""
draft = false
date = ""2021-10-11""
images = [""""]
author = ""Staff""
+++

Back to Overview
Projects
Request Access

A PI can request access using an online form. Please include a description of the project data, level of sensitivity, the anticipated scope of computing for the project, and any supplemental information such as IRB approval.  

Request Access to ACCORD

Your Project
A new project creates the following:

1TB project storage 
50GB home directory

Add or Remove Team Members
Additional team members can be added or removed by request using the following online form. Please include the researcher's full name, email, and home institution.
Add or remove researchers

Transfer Data
Details on Globus data transfer coming soon

Environments
Create an environment
{{< button button-class=""primary"" button-text=""Open the ACCORD Platform"" button-url=""https://accord.uvarc.io/"" >}}

From the ACCORD console, select the project you want to work with and the desired resource tier. We currently offer the following resource tier options:

Small (2 cores & 16 GB RAM)
Medium (4 cores & 32 GB RAM)
Large (8 cores, 64 GB RAM)
X-Large ( 16 cores & 64 GB RAM)

Next, select an environment. Your container should be running within a few seconds and will appear under ""Running Environments"" on the same page.
To learn more about your choices of environments, see Environments
Connect to an environment
Once you have created an environment, click on the ""CONNECT"" button for the appropriate environment and a new browser tab will open.
Terminate an environment
When you are finished with your environment, please terminate it. Using the ""Running Environments"" section of the ACCORD console, find the environment you wish to terminate.
On the far right will be a red ""Terminate"" button. Clicking this will terminate your environment.
Note that your saved files and storage are never terminated or destroyed in this process. 
Terminated environments cannot be recovered. However, they can be replicated.
Replicate an environment
To replicate an environment you used before, scroll down on the ACCORD console to see a list of
environments you have run before. Click the orange ""Run"" button next to an environment you want to reuse.

Software and data
Software requirements

A modern web browser such as Chrome, Firefox, Safari, or Edge.
Access to your institution's VPN
Install and register OPSWAT, a posture-checking client.

{{< button button-class=""primary"" button-text=""Learn More About OPSWAT"" button-url=""https://www.opswat.com/"" >}}
Data retention
PIs may suspend a project at any time using the console. Project data is stored for 6 months and then
automatically removed. Suspended projects can be reinstated by request before this time."
rc-website-fork/content/userinfo/accord/accord-support.md,"+++
date = ""2021-06-10T23:59:16-05:00""
tags = [""search""]
categories = [""forms""]
images = [""""]
author = ""Staff""
description = """"
title = ""ACCORD Support Request""
draft = true
type = ""form""
private = true
+++









Name *




E-mail *




University/Institution *



Department/Organization *



Brief description of your request *



Details of your request * 








Please submit the form only once. If you receive an error message after submitting this request, please check your email to confirm that the submission completed.
Submit



"
rc-website-fork/content/userinfo/accord/rstudio.md,"+++
description = """"
title = ""ACCORD: RStudio""
draft = false
date = ""2020-06-25T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""supercomputer"",""allocations"",""queues"",""storage"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""accord""
+++
Back to Overview
RStudio is the standard IDE for research using the R programming language.

Learn more about RStudio"
rc-website-fork/content/userinfo/accord/jupyter.md,"+++
description = """"
title = ""ACCORD: Jupyter Lab""
draft = false
date = ""2020-06-25T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""supercomputer"",""allocations"",""queues"",""storage"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""accord""
+++
Back to Overview
Jupyter Lab allows for interactive, notebook-based analysis of data. A good choice for pulling quick results or refining your code in numerous languages including Python, R, Julia, bash, and others.

Learn more about Jupyter Lab"
rc-website-fork/content/userinfo/accord/faq.md,"+++
title = ""ACCORD FAQs""
draft = false
date = ""2021-10-11""
images = [""""]
author = ""Staff""
+++

Back to Overview


What is the link to the accord portal?

{{< button button-class=""primary"" button-text=""Open the ACCORD Platform"" button-url=""https://accord.uvarc.io/"" >}}





Who can use ACCORD?

Researchers from public universities across the state of Virginia are invited to request access. ACCORD is also open to researchers from other states who are working on COVID related projects





How much does ACCORD cost?

ACCORD is free with 1TB of storage





How much storage does ACCORD support?

ACCORD gives each project a free 1TB of storage. Additional storage can be purchased





How is data transferred in/out of ACCORD?

ACCORD uses Globus to transfer data





How do I learn more about ACCORD?

{{< button button-class=""primary"" button-text=""Learn more about ACCORD"" button-url=""https://www.rc.virginia.edu/userinfo/accord/about/"" >}}


"
rc-website-fork/content/userinfo/accord/projects.md,"+++
title = ""ACCORD Projects""
draft = false
date = ""2021-10-11""
images = [""""]
author = ""Staff""
+++

Back to Overview

The fundamental organizing unit for your work in ACCORD is a project. A project consists of:

Researchers - a group of approved collaborators.  
Storage - import and store 1TB (more upon request) of data for your project. 

Researchers
All Principal Investigators (PI) are carefully vetted and approved by the ACCORD team before they are given access to the ACCORD computing platform. PIs manage personnel for their projects, transfer code and data, and control their storage.
Co-investigators are also vetted before they are given access to the ACCORD platform. Once a co-investigator has been onboarded, they can be added to any project by that project‚Äôs PI.
Data

Data can be transferred using the Globus federated GridFTP platform. The ACCORD DTN (data transfer node) endpoint
address can be found once you sign in to the ACCORD Console.
ACCORD user access is non-hierarchical and makes no distinction between various project personnel when granting permissions. 
All project members have equal access to project data, i.e. there is no privileged user or access for any given project.
Storage
ACCORD users automatically receive 1TB of free storage with each project. Additional storage can be purchased for a fee."
rc-website-fork/content/userinfo/accord/theia.md,"+++
description = """"
title = ""ACCORD: Theia IDE""
draft = false
date = ""2020-06-25T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""supercomputer"",""allocations"",""queues"",""storage"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""accord""
+++
Back to Overview
Theia Python is a rich IDE that allows researchers to manage their files and data, write code with an intelligent editor, and execute code within a terminal session.

Learn more about the Theia Python IDE"
rc-website-fork/content/userinfo/accord/about.md,"+++
title = ""About ACCORD""
draft = false
date = ""2021-10-11""
images = [""""]
author = ""Staff""
tags = [""nsf"",""accord""]
+++

Back to Overview


What is it
ACCORD (Assuring Controls Compliance of Research Data) gives researchers from public universities across the state of Virginia access to a multicore computing system capable of processing and storing de-identified sensitive data. ACCORD is appropriate for de-identified PHI, FERPA, business confidential, and other types of de-identified sensitive data. This is especially important for schools that lack the financial, staffing, or technical resources for such systems. ACCORD is designed to encourage collaborative research partnerships across institutions by managing the risks associated with sensitive data sharing.
ACCORD is project-based, which means use of the platform centers on individual projects. 
A principal investigator requests access to the platform (specifying a project title and level of sensitivity), populates the project with co-investigators,  and imports data. Each of the investigators can create and use computing environments to run analysis on the data. When done with the analysis, the investigators can spin down the pla/tform to free the resources.  
Who can use it
This platform is open for approved academic research on sensitive data. Researchers from
public universities across the state of Virginia are invited to request access. ACCORD is also open to researchers from other states who are working on COVID related projects.
ACCORD is appropriate for de-identified PHI, FERPA, de-identified HIPAA, business confidential, and other types of sensitive data. More
restrictive levels, such as CUI, FISMA, iTar, and PCI cannot be satisfied.
How do I use it
ACCORD is entirely web-based, which means it can be easily accessed using a modern browser such as Chrome, Firefox, Edge or Safari. Once you have created a project in ACCORD, you can perform computational research using one of the application environments. Currently we offer RStudio, JupyterLab, Theia C++, and Theia Python. More detailed information can be found in our User Guide
ACCORD offers no SSH, FTP, Remote Desktop, or VNC access.


Who runs it
The ACCORD project was developed by the Research Computing Group at the University of Virginia and funded through an NSF grant awarded to Ron Hutchins who serves as the Principal Investigator."
rc-website-fork/content/userinfo/accord/_index.md,"+++
title = ""ACCORD""
draft = false
date = ""2022-08-11""
images = [""""]
author = ""Staff""
categories = [""accord""]
tags = [""accord"",""python"",""r"",""hipaa"",""sensitive-data""]
layout = ""single""
+++




Welcome to ACCORD (Assuring Controls Compliance of Research Data), a web-based platform which allows researchers from public universities across the state of Virginia to analyze and store their sensitive data in a central location. 
ACCORD is appropriate for de-identified PII, FERPA, de-identified HIPAA, business confidential, and other types of de-identified sensitive data
Thanks to funding provided by the National Science Foundation (Award #: 1919667), ACCORD is available at no cost to researchers in the state of Virginia.

Partners
Listed below are our partner universities for ACCORD:




































Get Started



About
Learn about ACCORD.
Learn More




Support Ticket
Open a support ticket for ACCORD
Open a Support Ticket






Launch
Open the ACCORD Portal and get to work.
Launch




Projects
Learn how ACCORD manages projects
Learn More






Environments
Learn what environments ACCORD supports
Learn More




Security
Learn about ACCORD's security
Learn More






User Guide
Learn how to get started and work on ACCORD
Learn More




FAQs
FAQs about ACCORD
Learn More


"
rc-website-fork/content/userinfo/accord/security.md,"+++
title = ""ACCORD Security""
draft = false
date = ""2022-08-11""
images = [""""]
author = ""Staff""
categories = [""accord""]
tags = [""accord"",""security"",""nsf"",""hipaa"",""sensitive-data""]
+++

Back to Overview
ACCORD is appropriate for de-identified PII, FERPA, de-identified HIPAA, business confidential, and other types of de-identified sensitive data. ACCORD cannot be used to process highly-restricted data such as CUI, FISMA, iTAR, and PCI data.

Authentication
ACCORD does not have its own user identity store but instead relies upon authentication via your home institution's single sign-on tool.
Authorization
All members of a project have equal access to the data storage for that project, without sudo or root privileges. 
Closed Environments
ACCORD environments have no outbound connectivity to the Internet other than approved library and tool 
repositories (PyPi, CPAN, CRAN, etc.). Connections to tools such as GitHub and external APIs are not allowed.
Encryption
All connectivity to ACCORD environments is encrypted using SSL over HTTPS. 
Data transfers in/out via the Globus DTN meet FIPS 140-2 compliance.
Isolation
ACCORD environments cannot have any access to other environments. Environments run within isolated Kubernetes pods and their
network connectivity is isolated and encrypted.
Private Environment URLs
When you request an ACCORD environment, a unique HTTPS endpoint is created for you and 
can only be used by you. For example:
https://jupyter-notebook-1a2b3c4d5e-mst3k.uvarc.io/

These environments cannot be shared.
Logging
All user interactions with ACCORD are logged including account creation, approval, project creation, changes in group membership, the creation of/changes to environments, and file uploads/downloads using a browser or the Globus DTN.
Client Posture-Checks
Access to ACCORD is restricted to computers that are sufficiently updated and meet minimum security requirements. To verify this, ACCORD uses OPSWAT, a small piece of software that users install on their local computers.
Step 1: Install the VPN Assessment Application (Opswat)
Opswat will be installed during the onboarding process for ACCORD.
Step 2: Resolve Security Requirement Issues
Requirement 1: Operating System





Update Operating System for Mac (version 10.14.0 or higher)







Open System Preferences
Click on Software Update
Click Update Now


Note: Updating the Operating System may take up to a couple of hours. Do not shut down your computer or allow it to run out of battery during the update process. A restart of your computer may occur after the updates are complete.
        







Update Operating System for Windows 10







Open Windows Update by clicking the Start button in the lower left corner. In the search box, type ""Update"", and then, in the list of results, click either Windows Update or Check for updates.
Click the Check for updates button and then wait while Windows looks for the latest updates for your computer.
If you see a message telling you that important updates are available, or telling you to review important updates, click the message to view and select the important updates to install.
In the list, click the important updates for more information. Select the checkboxes for any updates that you want to install, and then click ""OK"".
Click Install updates.


Note: Updating the Operating System may take up to a couple of hours. Do not shut down your computer or allow it to run out of battery during the update process. A restart of your computer may occur after the updates are complete. If you encounter issues while trying to update your Windows computer, visit the Fix Windows Update Issues Windows Support webpage
        




Requirement 2: Host-Based Firewall
Host-based firewall software must be installed and enabled.





Enable Firewall for macOS (All Versions)







Open System Preferences
Select Security and Privacy
Select Firewall
Click the lock in the lower-left corner and enter your credentials.
Select Turn On Firewall
Close System Preferences









Enable Firewall for Windows 10







Select the Start button, then select Settings (the gear icon).
Select Windows Security from the menu on the left.
Select Firewall & network protection.
You may then see several networks (i.e., Domain network, Private network). Select each network one at a time and set the Windows Defender Firewall to On.






Requirement 3: Antivirus / Antimalware Software
At least one antimalware software must be installed and enabled. We recommend the following:



Antivirus for Mac

        We recommend using either Gatekeeper or Microsoft Defender for Endpoint for Macs.
      


Antivirus for Windows

        We recommend using either Microsoft Defender or Microsoft Defender for Endpoint for Windows.
      



Most common antivirus software is acceptable, except those made by Kaspersky Labs.
Requirement 4: Device Password
The device must be password protected, and it must lock automatically if there is no activity detected for at least 10 minutes. Configure your device to require a password to log in. Also, set your device‚Äôs screensaver or security settings to automatically lock after 10 minutes of no activity.
Requirement 5: Whole-Disk Encryption
Whole-disk encryption software must be installed and enabled. Accepted applications include BitLocker, Dell Data Protection, and FileVault."
rc-website-fork/content/userinfo/hpc/storage.md,"+++
description = """"
title = ""Rivanna Storage""
draft = false
date = ""2022-05-04T23:59:16-05:00""
tags = [""storage"",""scratch"",""hpc"",""rivanna"",""project"",""qumulo""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
There are a variety of options for storing large-scale research data at UVa. Public and Internal Use data storage systems can be accessed from the Rivanna and Afton high performance computing systems.

Storage Directories



Name
Quota
Price
Data Protection
Accessible from
Best Practices


/home
200GB
Free
{{% backup-policy rivanna_home %}}
Rivanna/Afton
/home is best used as a working directory when using Rivanna/Afton interactively. Slurm jobs run against /home will be slower than those run against /scratch. The /home directory is a personal storage space that is not shareable with other users.


/scratch
10TB
Free
{{% backup-policy rivanna_scratch %}}, Data removed 90 days after last file access time
Rivanna/Afton
/scratch is a high performance parallel filesystem that is suitable for large scale computational work. Data should be moved from /scratch for long-term storage. The /scratch directory is for personal use and not shared with other users.



Scratch Cleanup Policy
{{% scratch-policy %}}

Request Additional Storage
Researchers can lease additional storage, Research Standard or Research Project storage, for sharing public or internal use data within a research group. Research Standard and Research Project storage volumes are mounted on Rivanna/Afton and can also be accessed from local workstations. Learn more about our storage offerings.
Storage requests can be placed through this form:
Storage Requests"
rc-website-fork/content/userinfo/hpc/slurm-script-generator.md,"+++
categories = [""userinfo""]
type = ""full-width""
date = ""2024-08-01T00:00:00-05:00""
tags = [
    ""hpc"",""rivanna"",""software"",""queues""
]
draft = false
title = ""Slurm Script Generator""
description = """"
author = ""RC Staff""
+++

"
rc-website-fork/content/userinfo/hpc/basepod.md,"+++
categories = [""userinfo""]
type = ""rivanna""
date = ""2025-03-28T00:00:00-05:00""
tags = [
    ""hpc"",""rivanna"",""parallel-computing"",""software"",""containers""
]
draft = false
title = ""HGX H200 Nodes and the NVIDIA DGX BasePOD‚Ñ¢""
description = """"
author = ""RC Staff""
+++
HGX H200 Nodes
May 1, 2025
We‚Äôre excited to announce the release of our newest high-performance compute node featuring the NVIDIA HGX H200 platform. This node is built on the Dell PowerEdge XE9680 server model, and we currently have one server available.
The node is equipped with dual Intel Xeon Platinum 8468 CPUs, offering a total of 96 cores running at 2.1GHz, and comes with a massive 2TB of memory. This addition features eight NVIDIA HGX H200 GPUs based on the NVIDIA Hopper architecture.
Each H200 GPU includes 141 GB of memory and delivers up to 4.8 TB/s of memory bandwidth. High-speed communication between GPUs is supported via 900 GB/s NVLink, while connectivity between CPU and GPU uses PCIe Gen5, providing up to 128 GB/s bi-directional bandwidth. More information can be found here.
This node is now integrated into our SLURM scheduler and can be accessed using the gpu partition. However, to maximize the efficiency of this high-demand resource, it is excluded from access via the Open OnDemand platform and is only available through batch job requests. Information for accounting and scheduling purposes can be found here. We invite users with GPU-intensive workflows to take advantage of these powerful nodes. 
Introducing the NVIDIA DGX BasePOD‚Ñ¢
May 30, 2023
As artificial intelligence (AI) and machine learning (ML) continue to change how academic research is conducted, the NVIDIA DGX BasePOD, or BasePOD, brings new AI and ML functionality UVA's High-Performance Computing (HPC) system. The BasePOD is a cluster of high-performance GPUs that allows large deep-learning models to be created and utilized at UVA. 
The NVIDIA DGX BasePOD‚Ñ¢ on Rivanna and Afton, hereafter referred to as the POD, is comprised of:
- 18 DGX A100 nodes with
  - 2TB of RAM memory per node
  - 80 GB GPU memory per GPU device
Compared to the regular GPU nodes, the POD contains advanced features such as:
- NVLink for fast multi-GPU communication
- GPUDirect RDMA Peer Memory for fast multi-node multi-GPU communication
- GPUDirect Storage with 200 TB IBM ESS3200 (NVMe) SpectrumScale storage array
which makes it ideal for the following types of jobs:
- The job needs multiple GPUs on a single node or even multiple nodes.
- The job (can be single- or multi-GPU) is I/O intensive.
- The job (can be single- or multi-GPU) requires more than 40 GB GPU memory. (The non-POD nodes with the highest GPU memory are the regular A100 nodes with 40 GB GPU memory.)
Detailed specs can be found in the official document (Chapter 3.1).
Accessing the POD
The POD nodes are contained in the gpu partition with a specific Slurm constraint, requested with -C or --constraint=.
Slurm script
```bash
SBATCH -p gpu
SBATCH --gres=gpu:a100:N # replace N with the number of GPUs per node requested
SBATCH -C gpupod
```
Open OnDemand
Select NVIDIA A100 in the GPU type dropdown.  Select the number requested in the appropriate textbox.  Select Yes for Show Additional Options. Into the h ‚ÄúOptional: Slurm Option‚Äù textbox type:
-Cgpupod
Remarks

Before running on multiple nodes, please make sure the job can scale well to 8 GPUs on a single node.
Multi-node jobs on the POD should request all GPUs on the nodes, i.e. --gres=gpu:a100:8.
You may have already used the POD by simply requesting an A100 node without the constraint, since 18 out of the total 20 A100 nodes are POD nodes.
As we expand our infrastructure, there could be changes to the Slurm directives and job resource limitations in the future. Please keep an eye out for our announcements and documentation.

Usage examples
Deep learning
As of October 3, 2023 we are migrating toward NVIDIA‚Äôs NGC containers for deep learning frameworks such as PyTorch (2.0+) and TensorFlow (2.13+), as they have been heavily optimized to achieve excellent multi-GPU performance.
{{< alert-green >}} Warning: Distributed training is not automatic! Your code must be parallelizable. If you are not familiar with this concept, please visit: TensorFlow distributed , PyTorch DDP.
{{< /alert-green >}}
MPI codes
Please check the manual of your code regarding the relationship between the number of MPI ranks and the number of GPUs. For computational chemistry codes (e.g. VASP, QuantumEspresso, LAMMPS) the two are oftentimes equal, e.g.
```bash
SBATCH --gres=gpu:a100:8
SBATCH --ntasks-per-node=8
```
If you are building your own code, please load the modules nvhpc and cuda which provide NVIDIA compilers and CUDA libraries. The compute capability of the POD A100 is 8.0.
For documentation and demos, refer to the ‚ÄúResources‚Äù section at the bottom of this page.
GPU-enabled modules
A complete list of GPU-enabled modules on the HPC system can be found here. Please refer to their respective pages and/or module load messages (if any) for usage instructions."
rc-website-fork/content/userinfo/hpc/login.md,"+++
description = """"
title = ""Logging in to the UVA HPC systems""
draft = false
date = ""2019-05-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""supercomputer"",""login"",""ssh"",""openondemand""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
The UVA HPC systems (Rivanna and Afton) are accessible through a web portal, secure shell terminals, or a remote desktop environment.  For of all of these access points, your login is your UVA computing ID and your password is your Eservices password.  If you do not know your Eservices password you must change it through ITS.
{{< off-campus >}}
Web-based Access

Open OnDemand is a graphical user interface that allows access to HPC via a web browser.  The Open OnDemand access point is ood.hpc.virginia.edu.  Within the Open OnDemand environment users have access to a file explorer; interactive applications like JupyterLab, RStudio Server & FastX Web; a command line interface; and a job composer and job monitor to submit jobs to the Rivanna and Afton clusters.  Detailed instructions can be found on our Open OnDemand documentation page.
Launch Open OnDemand
Learn more about Open OnDemand

Secure Shell Access (SSH)
UVA HPC is accessible through ssh (Secure Shell) connections using the hostname login.hpc.virginia.edu.
 Windows
Windows users must install an ssh client application. We recommend MobaXterm, but you may also use other clients such as PuTTY.
Install MobaXterm
 Mac OSX and Linux
OSX and Linux users may connect through a terminal using the command
ssh -Y mst3k@login.hpc.virginia.edu
SSH key authentication is also permissible. 
 Using X11 Applications with ssh
X11 applications can be run via an ssh connection as long as it is configured correctly.  The -Y option specifies this for the command-line application run in a terminal.
Windows users who install MobaXterm do not need to add -Y in an ssh session since this is the default for MobaXterm.  Other clients such as PuTTY must be configured to allow X11 packets to be transferred. 
Mac users must install XQuartz in order to be able to run graphical (X11) applications locally.  
Graphical X11 applications may be slow through a standard ssh login. For extensive use of graphical applications we recommend FastX.
For more details and for troubleshooting information, please see our ssh page.

Remote Desktop Access
Users who wish to run X11 graphical applications may prefer the FastX remote desktop web interface.  The FastX web client is accessible at fastx.hpc.virginia.edu. Your login credentials are your UVA computing ID and your Eservices password.
Connect to FastX via Web
Learn more about FastX Web"
rc-website-fork/content/userinfo/hpc/slurm.md,"+++
description = """"
title = ""Slurm Job Manager""
draft = false
date = ""2019-05-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""supercomputer"",""allocations"",""queues"",""storage""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++







SLURM


Would you like to take an interactive SLURM quiz? y/N¬†|






Overview

UVA HPC is a multi-user, managed environment.  It is divided into login nodes (also called frontends), which are directly accessible by users, and compute nodes, which must be accessed through the resource manager.  Users prepare their computational workloads, called jobs, on the login nodes and submit them to the job controller, a component of the resource manager that runs on login nodes and is responsible for scheduling jobs and monitoring the status of the compute nodes.
We use Slurm, an open-source tool that manages jobs for Linux clusters. Jobs are submitted to the Slurm controller, which queues them until the system is ready to run them. The controller selects which jobs to run, when to run them, and how to place them on the compute node or nodes, according to a predetermined site policy meant to balance competing user needs and to maximize efficient use of cluster resources. Slurm divides a cluster into logical units called partitions (generally known as queues in other systems). Different partitions may contain different nodes, or they may overlap; they may also impose different resource limitations. The UVA HPC environment provides several partitions and there is no default; each job must request a partition. To determine which queues are available, log in to the HPC System and type 
qlist
at a Linux command-line prompt.  For the memory and core limits on each queue, use the command
qlimits
Local Queue Configuration
Several queues (partitions) are available for different types of jobs.  One queue is restricted to single-node (serial or threaded) jobs; another for multinode parallel programs, and others are for access to specialty hardware such as large-memory nodes or nodes offering GPUs.  For the current queue configuration and policies on UVA HPC please see its homepage. 
Slurm Architecture
Slurm has a controller process (called a daemon) on a head node and a worker daemon on each of the compute nodes. The controller is responsible for queueing jobs, monitoring the state of each node, and allocating resources. The worker daemon gathers information about its node and returns that information to the controller. When assigned a user job by the controller, the worker daemon initiates and manages the job. Slurm provides the interface between the user and the cluster. Slurm performs three primary tasks:

Manage the queue(s) of jobs and settles contentions for resources;
Allocate a subset of nodes or cores for a set amount of time to a submitted job;
Provide a framework for starting and monitoring jobs on the subset of nodes/cores.

To submit a job to the cluster, you must request the appropriate resources and specify what you want to run with a Slurm Job Command File. In most cases, this batch job file is simply a bash or other shell script containing directives that specify the resource requirements (e.g. the number of cores, the maximum runtime, partition specification, etc.) that your job is requesting along with the set of commands required to execute your workflow on a subset of cluster compute nodes.  Batch job scripts are submitted to the Slurm Controller to be run on the cluster. When the script is submitted to the resource manager, the controller reads the directives, ignoring the rest of the script, and uses them to determine the overall resource request.  It then assigns a priority to the job and places it into the queue.  Once the job is assigned to a worker, the job script is run as an ordinary shell script on the ""master"" node, in which case the directives are treated as comments.  For this reason it is important to follow the format for directives exactly.
The remainder of this tutorial will focus on the Slurm command line interface. More detailed information about using Slurm can be found in the official Slurm documentation.
Job Scripts
Jobs are submitted through a job script, which is a shell script (usually written in bash).  Since it is a shell script it must begin with a ""shebang""
```
!/bin/bash
```
This is followed by a preamble describing the resource requests the job is making.  Each request begins with #SBATCH followed by an option.
```
SBATCH --time=12:00:00
SBATCH -A myaccount
```
After all resource requests have been established through #SBATCH, the script must describe exactly how to run the job.  
module purge
module load gcc
./mycode
In our installation of Slurm, the default starting directory is the directory from which the batch job was submitted, so it may not be necessary to change directories.

Configurable Options in Slurm
Slurm refers to processes as ""tasks.""  A task may be envisioned as an independent, running process.  Slurm also refers to cores as ""cpus"" even though modern cpus contain several to many cores.  If your program uses only one core it is a single, sequential task.  If it can use multiple cores on the same node, it is generally regarded as a single task, with multiple cores assigned to that task.  If it is a distributed code that can run on multiple nodes, each process would be a task.
Options
Note that most Slurm options have two forms, a short (single-letter) form that is preceded by a single hyphen and followed by a space, and a longer form preceded by a double hyphen and followed by an equals sign.  In a job script these options are preceded by a pseudocomment #SBATCH.  They may also be used as command-line options on their own.
| Option | Usage |
|---|---|
| Number of nodes  | -N <n> or --nodes=<n> |
| Number of tasks  | -n <n> or --ntasks=<n> |
| Number of processes (tasks) per node  | --ntasks-per-node=<n> |
| Total memory per node in megabytes  | --mem=<M> |
| Memory per core in megabytes  | --mem-per-cpu=<M> |
| Wallclock time  | -t d-hh:mm:ss or --time=d-hh:mm:ss |
| Partition (queue) requested  | -p <part> or --partition <part> |
| Account to be charged  | -A <account> or --account=<allocation> |

{{< highlight >}}
   The --mem and --mem-per-cpu options are mutually exclusive.  Job scripts should specify one or the other but not both.
{{< /highlight >}}

Environment variables
These are the most basic; there are many more.  By default Slurm changes to the directory from which the job was submitted, so the SLURM_SUBMIT_DIR environment variable is usually not needed.
SLURM_JOB_ID
SLURM_SUBMIT_DIR
SLURM_JOB_PARTITION
SLURM_JOB_NODELIST
Slurm passes all environment variables from the shell in which the sbatch or salloc (or ijob) command was run. To override this behavior in an sbatch script, add
```
SBATCH --export=NONE
```
To export specific variables use
```
SBATCH --export=var1,var2,var3
```
Optionally export and set with
```
SBATCH --export=variable1=value1,variable2=value2
```
Standard Output and Standard Error
By default, Slurm combines standard output and standard error into a single file, which will be named slurm-\<jobid>.out.  You may rename standard output with
```
SBATCH -o  or --output=
```
To separate standard error from standard output, you must rename both.
```
SBATCH -e  or --error=
```
Slurm Script Generator
If you would like assistance in generating Slurm scripts, please check out our Slurm Script Generator. Simply input the parameters of your job to get a fully-working Slurm script.
Submitting a Job
Job scripts are submitted with the sbatch command, e.g.:
$ sbatch hello.slurm
The job identification number is returned when you submit the job, e.g.:
$ sbatch hello.slurm
Submitted batch job 18341
Submitting an Interactive Job
If you wish to run a job directly from the shell, you can run an interactive job.
If you are using any kind of graphical user interface (GUI) you should use one of the Open OnDemand interactive apps.  This offers direct access to Jupyterlab, VSCode Server, RStudio Server, the MATLAB desktop, and others.  For graphical applications not available through one of the dedicated apps, such as the Totalview debugger or some bioinformatics packages, use the Open OnDemand Desktop app. From the Desktop you can open a terminal window, load modules, and start any application you wish.  Please note that a few GUI applications require a GPU so you must request that partition in the online form.
If you wish to run an interactive job from the command line, you can use our local command ijob to obtain a login shell on a compute node.
$ ijob <options>
ijob is a wrapper around the Slurm commands salloc and srun, set up to start a bash shell on the remote node.  The options are the same as the options to salloc, so most commands that can be used with #SBATCH can be used with ijob.  The request will be placed into the queue specified:
$ ijob -c 1 -A mygroup -p standard --time=1-00:00:00
salloc: Pending job allocation 25394
salloc: job 25394 queued and waiting for resources
There may be some delay for the resource to become available.
salloc: job 25394 has been allocated resources
salloc: Granted job allocation 25394
For all interactive jobs, the allocated node(s) will remain reserved as long as the terminal session is open, up to the walltime limit, so it is extremely important that users exit their interactive sessions as soon as their work is done sothat the user is not charged for unused time.  If you are using an interactive app, be sure to delete session if you exit the session before time runs out.  In the case of a command-line ijob, the job will be terminated if you exit your shell for any reason, including shutting down the local computer from which your login originates.
$ exit 
salloc: Relinquishing job allocation 25394
A full list of options is available from SchedMD's salloc documentation, or you may run
ijob --help
at the command line.
Displaying Job Status
The squeue command is used to obtain status information about all jobs submitted to all queues. Without any specified options, the squeue command provides a display which is similar to the following:
```
JOBID     PARTITION     NAME       USER    ST  TIME      NODES    NODELIST(REASON)

12345     parallel      myHello    mst3k   R   5:31:21   4        udc-ba33-4a, udc-ba33-4b, uds-ba35-22d, udc-ba39-16a
12346     standard      bash       mst3k   R   2:44      1        udc-ba30-5
```
The fields of the display are clearly labeled, and most are self-explanatory. The TIME field indicates the elapsed walltime (hrs:min:secs) that the job has been running. Note that JOBID 12346 has the name bash, which indicates it is an interactive job. In that case, the TIME field provides the amount of walltime during which the interactive session has to be open (and resources have been allocated). The ST field lists a code which indicates the state of the job. Commonly listed states include:

PD PENDING: Job is waiting for resources;
R RUNNING: Job has the allocated resources and is running;
S SUSPENDED: Job has the allocated resources, but execution has been suspended.

A complete list of job state codes is available here.
To check on your job's status
$ sstat <jobid>
For detailed information
$ scontrol show job <jobid>
To request an estimate of when your pending job will run
$ squeue --start -j <jobid>
Canceling a Job
Slurm provides the scancel command for deleting jobs from the system using the job identification number:
$ scancel 18341
If you did not note the job identification number (JOBID) when it was submitted, you can use squeue to retrieve it.
```
$ squeue -u mst3k
JOBID     PARTITION      NAME         USER     ST    TIME   NODES    NODELIST(REASON)
18341     standard          myHello      mst3k    R     0:01   1        udc-ba30-5
```
To cancel all of your jobs
$ scancel -u mst3k
To cancel all of your pending jobs
$ scancel -t PENDING -u mst3k
To cancel all jobs with a specified name
$ scancel --name myjob
To restart (cancel and rerun)
$ scontrol requeue <jobid>
For further information about the squeue command, type man squeue on the cluster front-end machine or see the Slurm Documentation.
Job Arrays
A large number of jobs can be submitted through one request if all the files used follow a strict pattern.  For example, if input files are named input_1.dat, ... , input_1000.dat, we could write a job script requesting the appropriate resources for a single one of these jobs with
{{< pull-code file=""/static/scripts/job_array.slurm"" lang=""no-highlight"" >}}
In the output file name, %a is the placeholder for the array ID.  We submit with
$ sbatch --array=1-1000 myjob.sh
The system automatically submits 1000 jobs, which will all appear under a single job ID with separate array IDs.  The SLURM_ARRAY_TASK_ID environment variable can be used in your command lines to label individual subjobs.
The placeholder %A stands for the overall job ID number in the #SBATCH preamble lines, while %a represents the individual task number.  These variables can be used with the --output option.  In the body of the script you can use the regular environment variable SLURM_TASK_ID if you wish to differentiate different job IDs and SLURM_ARRAY_TASK_ID for the jobs within the array.
To submit a range of task IDs with an interval
$ sbatch --array=1-1000:2
To submit a list of task IDs
$ sbatch --array=1,3,9,11,22
Using Files with Job Arrays
For more complex commands, you can prepare a file containing the text you wish to use. Your job script can read the file line by line.  In the following example, you must number your subtasks starting from 1 sequentially.  You must prepare the options_file.txt in advance and each line must be the options you wish to pass to your program.  
{{< pull-code file=""/static/scripts/job_array_files.slurm"" lang=""no-hightlight"" >}}
The double quotes and curly braces are required.
Canceling Individual Tasks in an Array
One task
$ scancel <jobid>_<taskid>
A range of tasks
$ scancel <jobid>_[<taskid1>-<taskid2>]
A list of tasks
$ scancel <jobid>_[<taskid1>,<taskid2>,<taskid3>]
Specifying Job Dependencies
With the sbatch command, you can invoke options that prevent a job from starting until a previous job has finished. This constraint is especially useful when a job requires an output file from another job in order to perform its tasks. The --dependency option allows for the specification of additional job attributes. For example, suppose that we have two jobs where job_2 must run after job_1 has completed. Using the corresponding Slurm command files, we can submit the jobs as follows:
sbatch job_1.slurm 
Submitted batch job 18375 
sbatch --dependency=afterok:18375 job_2.slurm
Notice that the --dependency has its own condition, in this case afterok. We want job_2 to start only after the job with id 18375 has completed successfully. The afterok condition specifies that dependency. Other commonly-used conditions include the following:

after: The dependent job is started after the specified job_id starts running;
afterany: The dependent job is started after the specified job_id terminates either successfully or with a failure;
afternotok: The dependent job is started only if the specified job_id terminates with a failure.

More options for arguments of the dependency condition are detailed in the manual pages for sbatch found here or by typing man sbatch at the Linux command prompt.
We also are able to see that a job dependency exists when we view the job status listing, although the explicit dependency is not stated, e.g.:
```
% squeue
JOBID    PARTITION     NAME      USER     ST     TIME    NODES   NODELIST(REASON)

18375    standard      job_2.sl  mst3k    PD     0:00    1       (Dependency)
18374    standard      job_1.sl  ms53k    R      0:09    1       udc-ba30-5
```
Job Accounting Data
When submitting a job to the cluster for the first time, the walltime requirement should be overestimated to ensure that Slurm does not terminate the job prematurely. After the job completes, you can use sacct to get the total time that the job took. Without any specified options, the sacct command provides a display which is similar to the following:
```
JobID    JobName        Partition    Account    AllocCPUS   State       ExitCode

18347    hello2.sl+     standard     default    1           COMPLETED    0:0
18347    .batch         standard     default    1           COMPLETED    0:0
18348    hello2.sl+     standard     default    1           COMPLETED    0:0
18348    .batch         standard     default    1           COMPLETED    0:0
```
To include the total time, you will need to customize the output by using the format options. For example, the command
% sacct --format=jobID --format=jobname --format=Elapsed --format=state
yields the following display:
```
JobID         JobName     Elapsed    State

18347         hello2.sl+  00:54:59   COMPLETED
18347.batch   batch       00:54:59   COMPLETED
18347.0       orted       00:54:59   COMPLETED
18348         hello2.sl+  00:54:74   COMPLETED
18348.batch   batch       00:54:74   COMPLETED
```
The Elapsed time is given in hours, minutes, and seconds, with the default format of hh:mm:ss. The Elapsed time can be used as an estimate for the amount of time that you request in future runs; however, there can be differences in timing for a job that is run several times. In the above example, the job called python took 21 minutes, 27 seconds to run the first time (JobID 18352.0) and 16 minutes, 8 seconds the last time (JobID 18353.2). Because the same job can take varying amounts of time to run, it would be prudent to increase Elapsed time by 10% to 25% for future walltime requests. Requesting a little extra time will help to ensure that the time does not expire before a job completes
Sample Slurm Command Scripts
In this section are a selection of sample Slurm command files for different types of jobs.  For more details on running specific software packages, please see the software pages.
Basic Serial Program
This example is for running your own serial (single-core) program.  It assumes your program is in the same folder from which your job script was submitted.
{{< pull-code file=""/static/scripts/simple_serial_job.slurm"" lang=""no-highlight"" >}}
MATLAB
This example is for a serial (one core) Matlab job.
{{< pull-code file=""/static/scripts/simple_matlab_job.slurm"" lang=""no-highlight"" >}}
Python
This script runs a Python program.
{{< pull-code file=""/static/scripts/simple_python_job.slurm"" lang=""no-highlight"" >}}
R
This is a Slurm job command file to run a serial R batch job.
{{< pull-code file=""/static/scripts/simple_R_job.slurm"" lang=""no-highlight"" >}}
Job Scripts for Parallel Programs {#jobs-using-a-gpu}
Distributed Memory Jobs
If the executable is a parallel program using the Message Passing Interface (MPI), then it will require multiple processors of the cluster to run. This information is specified in the Slurm nodes resource requirement. The script mpiexec is used to invoke the parallel executable. This example is a Slurm job command file to run a parallel (MPI) job using the OpenMPI implementation:
{{< pull-code file=""/static/scripts/mpi_job.slurm"" lang=""no-highlight"" >}}
In this example, the Slurm job file is requesting two nodes with sixteen tasks per node (for a total of thirty-two processors).  Both OpenMPI and IntelMPI are able to obtain the number of processes and the host list from Slurm, so these are not specified.  In general, MPI jobs should use all of a node so we'd recommend ntasks-per-node=40 on the parallel partition, but some codes cannot be distributed in that manner so we are showing a more general example here.
Slurm can also place the job freely if the directives specify only the number of tasks. In this case do not specify a node count.  This is not generally recommended, however, as it can have a significant negative impact on performance.
{{< pull-code file=""/static/scripts/mpi_job_free_placement.slurm"" lang=""no-highlight"" >}}
MPI over an odd number of tasks
{{< pull-code file=""/static/scripts/mpi_job_odd_number.slurm"" lang=""no-highlight"" >}}
Threaded Jobs (OpenMP or pthreads)
Slurm considers a task to correspond to a process.  Specifying a number of cpus (cores) per node ensures that they are on the same node.  Slurm does not set standard environment variables such as OMP_NUM_THREADS or NTHREADS, so the script must transfer that information explicitly.  This example is for OpenMP:
{{< pull-code file=""/static/scripts/openmp_job.slurm"" lang=""no-highlight"" >}}
Hybrid
The following example runs a total of 32 MPI processes, 8 on each node, with each task using 5 cores for threading.  The total number of cores utilized is thus 160.
{{< pull-code file=""/static/scripts/hybrid_job.slurm"" lang=""no-highlight"" >}}
GPU Computations {#gpu-intensive-computation}
The gpu queue provides access to compute nodes equipped with RTX2080Ti, RTX3090, A6000, V100, A100, and H200 NVIDIA GPU devices.
{{< highlight >}}
   In order to use GPU devices, the jobs must to be submitted to the gpu partition and must include the --gres=gpu option.
{{< /highlight >}}
{{< pull-code file=""/static/scripts/gpu_job.slurm"" lang=""no-highlight"" >}}
The second argument to gres can be rtx2080, rtx3090, v100, a100, or h200 for the different GPU architectures.  The third argument to gres specifies the number of devices to be requested.  If unspecified, the job will run on the first available GPU node with a single GPU device regardless of architecture.
Two models of NVIDIA A100 GPUs are available; 2 nodes with 40GB of GPU memory per GPU, and 18 nodes with 80GB of memory per GPU. To make a specific request for an 80GB A100 node, please add a constraint to the Slurm script:
```nohighlight
SBATCH --constraint=a100_80gb
``
This is in addition to requesting ana100in thegres` option.
HGX H200 GPUs and NVIDIA GPU BasePOD‚Ñ¢ for Rivanna and Afton Users
As artificial intelligence (AI) and machine learning (ML) continue to change how academic research is conducted, the NVIDIA DGX BasePOD, or BasePOD, brings new AI and ML functionality to Rivanna and Afton, UVA's High-Performance Computing (HPC) systems. The BasePOD is a cluster of high-performance GPUs that allows large deep-learning models to be created and utilized at UVA. In addition, new HGX H200 GPU nodes have been added to the cluster, further expanding UVA‚Äôs capabilities for cutting-edge AI research.
Learn More ¬†¬†
CPU and Memory Usage
Sometimes it is important to determine if you used all cores effectively and if enough memory was allocated to the job. There are separate Slurm commands for running jobs and completed jobs.
Running job
Use sstat to get CPU and memory usage for a running job.
```
$ sstat -j  --format=AveCPU,MaxRSS
    AveCPU     MaxRSS

76-07:19:+  95534696K
```
The CPU efficiency can be calculated by dividing the total core time in AveCPU by the number of requested cores and run time. The above example was obtained for a job that had been running for about 4 days on 20 cores. Therefore, the CPU efficiency is:
(76 + 7/24)
----------- = 95%
  4 * 20
which indicates good usage of all 20 cores.
The maximum memory used is given by MaxRSS (about 91 GB).
For more options, please read the manual man sstat.
Completed job
The seff command reports the CPU and memory usage of a completed job. 
Please use this for completed jobs only - efficiency statistics may be misleading for running jobs.
$ seff <jobid>
Job ID: <jobid>
Cluster: shen
User/Group: mst3k/users
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 4-03:27:36
CPU Efficiency: 84.11% of 4-22:15:20 core-walltime
Job Wall-clock time: 11:49:32
Memory Utilized: 3.65 GB
Memory Efficiency: 4.16% of 87.89 GB
If you need a more detailed analysis of CPU/memory usage, please contact us for help.
Usage Report
PI
The PI of an allocation account can see SU usage of all group members via the following command:
list-group-usage -A <your_allocation> [-S YYYY-MM-DD] [-E YYYY-MM-DD]
The -S and -E flags for the start date and end date are optional. The default values are, respectively, the beginning of the current month and now. You will see a report as follows:
```


Includes fund 000 (mst3klab)
Generated on Mon Aug 31 00:00:00 2020.
Reporting fund activity from 2020-08-01 to Now.


Beginning Balance:           100000.000
Ending Balance:               99000.000
######################### Debit Summary
User   Jobs   SUs

mst1k  500      500.000
mst2k  200      300.000
mst3k  100      200.000
######################### End of Report
```
Non-PI
Regular users can run the previous command, but it will only show your own usage. You may use the sreport command for the total CPU time of your group members:
$ sreport cluster UserUtilizationByAccount Start=2020-08-01 Accounts=<your_allocation> -t Hours
Note that this may be different from the actual allocation usage, since sreport is unaware of our SU charge policy, but can serve as an estimate.
To find out how many Service Units (SUs) a specific job has consumed, users can run the following command. Here the value under the Amount column shows the amount of SUs consumed. The time-frame can be controlled using the -s(starting time) and -e(end time) flags.
$  mam-list-transactions -a <allocation-name> -s 2024-11-01 -e 2024-12-03  # -s:starting date   -e: end date
Documentation"
rc-website-fork/content/userinfo/hpc/access.md,"+++
description = """"
title = ""Access to HPC Resources""
draft = false
date = ""2024-12-03T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""supercomputer"",""allocations"", ""afton"", ""dedicated-computing""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
summary = """"""


Compute time on Rivanna/Afton is available through two service models.
Service Unit (SU) Allocations. One SU corresponds to one core-hour. Multiple SUs make up what is called an SU allocation (e.g., a new allocation = 1M SUs).
  Dedicated Computing. This model allows researchers to lease hardware managed by Research Computing (RC) as an alternative to purchasing their own equipment. It provides dedicated access to HPC resources with no wait times.

""""""
+++
{{< form-cookies >}}



Compute time on Rivanna/Afton is available through two service models.

Service Unit (SU) Allocations. One SU corresponds to one core-hour. Multiple SUs make up what is called an SU allocation (e.g., a new allocation = 1M SUs).

Dedicated Computing. This model allows researchers to lease hardware managed by Research Computing (RC) as an alternative to purchasing their own equipment. It provides dedicated access to HPC resources with no wait times.

Below, you‚Äôll find information on eligibility for access, account creation, and the various types of SU allocations along with their pricing.
PI Eligibility
{{% pi-eligibility %}}
Account Creation
Each PI should create his/her own Grouper group using the ITS Group Management Service. New groups will require two owners who hold active roles at UVA, as well as a third departmental owner. The PI may designate one or more group administrators but must remain a member of the group. Collaborators with UVA Eservices accounts, regardless of status, can be added to the Grouper group once it has been created. (Collaborators outside of UVA must request a temporary, sponsored Eservices account.) Grouper group names should consist of lowercase letters, digits, or underscores only and must begin with a letter. Please do not use spaces in the group name.
{{% highlight %}}
Whether you need to set up a new group, modify a group or access the legacy MyGroups groups, go to Grouper which requires VPN connection. For new groups, specify ""This group will be used for Rivanna/Afton access"" in the description section of the Service Now request form to expedite group creation. Please add yourself as a member to the group in order for us to fulfill any allocation request related to this group.
{{% /highlight %}}
Each PI is ultimately responsible for managing the roster of users in his/her group although PIs may delegate day-to-day management to one or more other members. When users are added or deleted, accounts are automatically created. Group owners will be required to perform an annual attestation of group membership. If group owners do not complete attesting to the validity of their group, the members will be automatically removed from the group.
Manage Grouper

SU Allocations
Pricing
{{< pricing allocations>}}

Types {#allocation-types}
Standard Allocations
Standard allocations require a brief summary of the research project along with an explanation of the computations to be performed. Standard allocations must be renewed annually along with a synopsis of results from the original allocation. There cannot be more than 1 PI per Grouper group. Standard allocations expire 12 months after they are disbursed. 
Available to: Eligible PIs
Request New / Renew Standard Allocation

Allocation Purchases
Time on Rivanna and Afton can also be purchased using an FDM. Purchasers are given a higher priority in the queue and their SUs never expire.
iAs an alternative to purchasing SU's, RC offers dedicated computing which allows researchers to request exclusive access to a subset of HPC nodes for extended periods. See below for more information.
Available to: Eligible PIs who need priority access and premium service.

Purchase an Allocation


Instructional Allocations
Instructional allocations provide limited access to Rivanna and Afton and are available to UVA instructors who are teaching a class or leading a training session. Faculty who wish to request an instructional allocation should choose a Grouper account name using the class rubric, e.g. cs5014. Service units will be automatically purged 2 weeks after the class ends unless the instructor requests an extension. Instructors are required to submit a fresh instructional allocation request‚Äîeither a new request or a renewal request‚Äîat the start of each semester.
Available to: Faculty who intend to use HPC resources in their class. 
Read the full policy and guide for instructors.

Request an Instructional Allocation

Dedicated Computing {#dedicated-computing}
Dedicated computing is an alternative to self-managed lab systems and condominium nodes. This option provides researchers with exclusive access to HPC resources without wait times, eliminating the need for RC to manage the lifecycle of hardware purchased by researchers. Dedicated Computing involves nodes that RC has procured as part of its large-scale HPC acquisitions being ‚Äúleased‚Äù to researchers for a term of one year or longer. These leased nodes are configured with the same system image as the primary HPC environment, ensuring consistency and minimizing support overhead. Once the lease term ends, dedicated nodes are returned to the public queues, making them available for general HPC use.
Available to: Eligible PIs who need exclusive access to a subset of HPC nodes for extended periods.

Request Dedicated Computing

Pricing
{{< pricing dedicated_computing >}}"
rc-website-fork/content/userinfo/hpc/_index.md,"+++
description = """"
title = ""Using UVA‚Äôs High-Performance Computing Systems""
draft = false
date = ""2024-12-03T00:00:00-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""supercomputer"",""allocations"",""queues"",""storage"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
layout = ""single""
aliases = [ ""/rivanna"" ]
+++

{{% callout %}}
  
  {{< get_allocation_blurb name=""Afton"" >}}
{{% /callout %}}
{{% callout %}}
  {{< get_allocation_blurb name=""Rivanna"" >}}
{{% /callout %}}
{{< systems-boilerplate >}}
{{< lead >}}
The sections below contain important information for new and existing Rivanna and Afton users. Please read each carefully.
{{< /lead >}}
{{< lead >}}
New users are invited to attend one of our free orientation sessions (""Introduction to HPC"") held throughout the year.
{{< /lead >}}

Sign up for an ""Intro to HPC"" session


Get Started



Access / Allocations
Learn how to request an allocation and add collaborators.
Request an Allocation




Logging In
Log in through a Web browser or a command-line tool.
Learn More






File Transfer
Moving files between Rivanna/Afton and other systems.
Learn More




Software
See a listing of available software.
Learn More






Storage
Options for free short-term and leased long-term storage
Learn More




Running Jobs in Slurm
Submitting jobs to Rivanna/Afton through the Slurm resource manager
Learn More






Job Queues
Determine the best queue (or ‚Äúpartition‚Äù) for running your jobs.
Learn More




Usage Policies
Understand the terms and conditions for using Rivanna/Afton.
Learn More






FAQs
Frequently Asked Questions.
Rivanna and Afton FAQ




Overview
A high performance computing cluster is typically made up of at least four service layers:

Login nodes - Where you log in, interact with data and code, and submit jobs.
Compute nodes - Where production jobs are run. On Rivanna and Afton these nodes are heterogeneous; some have more memory, some have GPU devices, and so forth. Partitions are homogeneous so you can select specialty hardware by your partition request, sometimes along with a resource request (gres).
Storage - Where files are stored, accessible by all nodes in the cluster.
Resource Manager - A software system that accepts job requests, schedules the jobs on a node or set of nodes, then manages their execution.

Click on elements of the image to learn more:








System Details
Hardware Configuration {#hardware-configuration}
{{< new-rivanna-specs >}}
{{< systems-boilerplate >}}
Job Queues
Rivanna and Afton are managed resources; users must submit jobs to queues controlled by a resource manager, also known as a queueing system.  The manager in use on Rivanna and Afton is Slurm.  Slurm refers to queues as partitions because they divide the machine into sets of resources.  There is no default partition and each job must request a specific partition.  Partitions and access policies are subject to change, but the following table shows the current structure.  Note that memory may be requested per core or for the overall job.  If the total memory required for the job is greater than the number of cores requested multiplied by the maximum memory per core, the job will be charged for the additional cores whether they are used or not.  In addition, jobs running on more than one core may still require a request of total memory rather than memory per core, since memory per core is enforced by the system but some multicore software packages (ANSYS, for example) may exceed that for a short time even though they never exceed cores x memory/core.
{{< new-rivanna-queue >}}
Remarks

standard maximum aggregate CPU cores allowed for a single user‚Äôs running jobs is 1000.
parallel requires at least 2 nodes and 4 CPU cores.
Slurm's default memory unit is in MB. Different units may be specified, e.g. --mem=100G, where 1G = 1024M.
The gpu partition is dedicated to jobs that can utilize a general purpose graphics processing unit (GPGPU). In Slurm scripts you must request at least one GPU device through --gres=gpu. Jobs that do not utilize any GPUs are not allowed in this partition.  
interactive maximum aggregate CPU cores (GPUs) is 24 (2) for a single user.
The NVIDIA DGX BasePOD and HGX H200 GPU nodes offer high-performance GPUs that bring new AI and ML functionality to support parallel GPU computing and large deep-learning models. Currently, H200 nodes are not accessible through Open OnDemand and can only be utilized via batch job submissions. 
Learn More ¬†¬†


Usage Policies
Research computing resources at the University of Virginia are for use by faculty, staff, and students of the University and their collaborators in academic research projects.  Personal use is not permitted.  Users must comply with all University policies for access and security to University resources.  The HPC system has additional usage policies to ensure that this shared environment is managed fairly to all users. UVA's Research Computing (RC) group reserves the right to enact policy changes at any time without prior notice.
Login Nodes
Exceeding the limits on the login nodes (frontend) will result in the user‚Äôs process(es) being killed. Repeated violations will result in a warning; users who ignore warnings risk losing access privileges.
Scratch Directory
{{% scratch-policy %}}
Software Licenses
Excessive consumption of licenses for commercial software, either in time or number, if determined by system and/or RC staff to be interfering with other users' fair use of the software, will subject the violator's processes or jobs to termination without warning.  Staff will attempt to issue a warning before terminating processes or jobs but inadequate response from the violator will not be grounds for permitting the processes/jobs to continue.
Inappropriate Usage
Any violation of the University‚Äôs security policies, or any behavior that is considered criminal in nature or a legal threat to the University, will result in the immediate termination of access privileges without warning.
Acceptable Use of the University‚Äôs Information Technology Resources
According to UVA policy, users are prohibited from downloading or using applications such as TikTok, WeChat, DeepSeek, and similar software on any RC resources. For more details, please see here.

"
rc-website-fork/content/userinfo/storage/personal-computing.md,"+++
author = ""RC Staff""
description = """"
title = ""Commercial Data Sharing and Archiving Solutions""
date = ""2022-05-04T23:59:16-05:00""
draft = false
tags = [""storage"",""personal computing""]
categories = [""userinfo""]
images = [""""]
+++



Box¬Æ
Non-sensitive cloud storage

            UVA Box is a cloud-based storage and collaboration service that gives eligible members of the University community the ability to access, store, and share up to 1 TB of public / internal use University files securely‚Äîanywhere, anytime, on any device.
        
Read more





DropBox¬Æ/Sookasa¬Æ
Highly Sensitive Data (PHI/PII) storage

            If you plan on storing highly sensitive data such as PHI or PII, UVA Health System offers a secure encrypted storage for Health System affiliated researchers, students, and staff. ""DropBox Sookasa"" is a free cloud-based service hosted on Dropbox that can be accessed over the internet on any device. Highly sensitive data such as a HIPAA-compliant dataset or PHI/PII must be stored in a Sookasa folder. It can be used to share files between Health System users. 
        

Requesting your DropBox account

            You have to make a request to Health System IT in order to gain access to the UVA HS DropBox Sookasa service. 
            
Click here to access the request form on the HIT website
Login using your Health System username and password
Click on the green colored ""Account Request"" icon
Underneath the System menu select ""Dropbox""
Underneath the Role menu select the ""Standard Account""
Fill in your computing ID, and HS email address. You may check ""smart select"" to verify if your ID is properly entered
Click ""Add to Cart""
Click ""Checkout"" after entering any comments about your request


Read more






CrashPlan¬Æ
Non-sensitive cloud desktop backup

        CrashPlan is a cloud-based desktop backup service. It securely backs up your endpoint devices to the cloud. CrashPlan provides:
      


Cloud storage for backup of up to 4 endpoint devices per user
Protection against crypto-ransomware and other malicious software that destroys/encrypts content on end-user‚Äôs devices
Protection of University data on endpoint devices from loss due to hard drive failure, computer failure, etc.



      CrashPlan is currently offered at no cost to the University community until June 30, 2018. After that, the cost model/fee structure will be determined for continued use of the service. During this initial phase, the system has a per-user quota of 250GB. If you need more space and have a valid use case, please contact ITS via the link below.
      
Read more


"
rc-website-fork/content/userinfo/storage/research-standard.md,"+++
title = ""Research Standard Storage""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2022-05-04T17:45:12-05:00""
tags = [
    ""rivanna"", 
    ""public-data"",
    ""internal-use-data"",
    ""storage"",
    ""research""
]
draft = false
+++
Overview
The Research Standard Storage file system provides users with a solution for research data storage and sharing. Public, internal use, and sensitive research data can be stored in Research Standard storage, and UVA Information Security provides details about data sensitivity classifications. Members in the same group have access to a shared directory created by the team lead or PI. Group membership can be defined and managed through Grouper (requires VPN connection). Research Standard storage is mounted on the HPC cluster and can also be accessed on a personal computer with an SMB mount, allowing for point-and-click file manipulation.
As of July, 2024, Each PI with a Research Computing account will have up to 10 TB of Research Standard Storage at no charge.
If you are not a researcher, UVA ITS offers Value storage for long-term storage of large scale data. More information about ITS's various storage options can be found on their website.
Request Research Standard Storage
Research Standard storage can be requested for $45/TB/YR through our Storage Request Form.
Users can specify the size and name of the Research Standard storage directory and the name of an existing Grouper group that can access the space. If the Grouper group does not yet exist, please create one through the ITS Group Management System before filling out the Research Standard storage request form.
When your Research Standard Storage share is created, you will receive an email detailing your NFS mount standard.hpc.virginia.edu:vol###, where ### refers to the specific volume number, and the SMB map \\standard.hpc.virginia.edu\Grouper_group_name. Once the space is available, the PI can grant access to lab member by adding them to the Grouper group. Users in the Grouper group will see the directory (/standard/Shared_space_name)` after logging into UVA HPC.
{{% group_creation_tip %}}
Drive Mapping with Research Standard Storage
Research Standard storage can be drive mapped on a personal computer to enable drag-and-drop file manipulation and transfer between your PC and your value storage share. Detailed instructions for mapping network drives on Windows and Mac machines can be found on the UVa Research Computing How-To pages.
File Manipulation and Navigation with Research Standard Storage
Research Standard storage is based on a Linux file system similar to storage spaces on the cluster, including /home and /scratch. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd).  If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below:

A Gentle Introduction
10 Essential Linux Commands
How To Manage Files From The Linux Terminal


Shell Novice

For more help, please feel free to contact RC to set up a consultation or visit us during office hours."
rc-website-fork/content/userinfo/storage/data-transfer.md,"+++
title = ""Data Transfer""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2019-10-04T17:45:12-05:00""
tags = [
    ""rivanna"",
    ""data-transfer"",
    ""storage"",
    ""ivy""
]
draft = false
+++
Data transfer
Public & Moderately Sensitive Data Transfer

Secure Copy (scp)
scp uses secure shell (SSH) protocol to transfer files between your local machine and a remote host. scp can be used with the following syntax:
scp [source] [destination]
scp SourceFile mst3k@login.hpc.virginia.edu:/scratch/mst3k
scp SourceFile mst3k@login.hpc.virginia.edu:/project/Grouper_group_name
Detailed instructions and examples for using scp are listed here.
Secure File Transfer Protocol (sftp)
sftp is a network protocol for secure file management. Instructions and examples for using sftp are located here.
Graphical File-Transfer Applications
Filezilla and Cyberduck, and MobaXterm are examples of open source SFTP client software for file management through an interactive graphical user interface. Instructions for using these SFTP clients can be found here.
Globus Connect (Large Data Transfer)
Globus provides access to data on local machines and HPC file systems, as well as external institutions and facilities. Globus is well suited for transferring both small files and large amounts of data. More information on Globus data transfer can be found here.
Public & Moderately Sensitive Data Storage Systems
/home, /scratch, and /project storage are based on a Linux file system. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd).  If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below:

A Gentle Introduction
10 Essential Linux Commands
How To Manage Files From The Linux Terminal




Shell Novice
"
rc-website-fork/content/userinfo/storage/research-project.md,"+++
title = ""Research Project Storage""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2019-10-04T17:45:12-05:00""
tags = [
    ""rivanna"",
    ""public-data"",
    ""internal-use-data"",
    ""storage"",
    ""research"",
    ""research-project""
]
draft = false
+++
Overview
The Research Project Storage file system provides users with a collaborative space for data storage and sharing. Public, internal use, and sensitive research data can be stored in Research Project storage, and UVA Information Security provides details about data sensitivity classifications. Members in the same group have access to a shared directory created by the team lead or PI. Group membership can be defined and managed through Grouper (requires VPN connection). /project storage is mounted on the HPC cluster and runs on a new scale-out NAS file system.
If you are not a researcher, UVA ITS offers Value storage for long-term storage of large scale data. More information about ITS's various storage options can be found on their website.
Request Research Project Storage
Research Project storage can be purchased for 70$/TB/YR through our Storage Request Form. When filling out the form, the PI can specify the size and name of the Research Project storage directory and the name of an existing or new Grouper group that can access this space. We recommend choosing a Grouper group name specific to your group or collaboration for the Research Project storage directory. This will reduce confusion in the future if you manage multiple Grouper groups and directories on other storage systems.
Once the request has been submitted, the PI will receive a notification that the /project space has been provisioned within 24 hours. Once the space becomes available, the PI can grant access to lab members by adding them to the Grouper group. Users in the Grouper group will see the directory (/project/Shared_space_name) after logging into UVA HPC.
{{% group_creation_tip %}}
Drive Mapping with Research Project Storage
Research Project storage can be drive mapped on a personal computer to enable drag-and-drop file manipulation and transfer between your PC and your value storage share. Detailed instructions for mapping network drives on Windows and Mac machines can be found on the UVa Research Computing How-To pages.
File Manipulation and Navigation with Research Project Storage
Research Project storage is based on a Linux file system similar to storage spaces on the cluster, including /home and /scratch. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd).  If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below:

A Gentle Introduction
10 Essential Linux Commands
How To Manage Files From The Linux Terminal




Shell Novice
"
rc-website-fork/content/userinfo/storage/storage-options.md,"+++
author = ""UVARC Staff""
description = """"
title = ""Storage for Researchers""
date = ""2019-04-18T10:08:29-05:00""
draft = true
tags = [""storage"",""rivanna"",""nfs"",""lustre"",""backup"",""s3""]
categories = [""userinfo""]
images = [""""]
+++
UVA Research Computing provides multi-tiered storage solutions for your data. From small-scale personal computing options to high-performance parallel file systems for serious computational runs, various systems are available to researchers.




Large-Scale Research Data Storage

        UVa offers a number of institutional solutions for storing and managing large-scale research data. Each of these can serve different use-cases depending on budget and archival needs. All of these systems are mounted and visible to local high-performance computing resources.
        
Learn more







Cloud Storage Solutions

        Long-term and high-capacity storage solutions are also available in Amazon Web Services and Google Cloud Platform. Both provide object storage for files of any type, and low-cost archival storage (comparable to tape backups).
        
Learn more







Non-Research (Enterprise) Data Storage

    UVa ITS provides a tiered catalog of data storage options for data not intended to be analyzed in research projects. Examples of appropriate data could include administrative records or departmental information. 
    
Learn more







Commercial Data Sharing and Archiving Solutions

        There are a number of commercially licensed tools available to UVa researchers for free. These products, including UVa Box, Dropbox (Health System) and CrashPlan, are most suitable for small-scale storage needs.
Box
Dropbox (HS Only)
CrashPlan


"
rc-website-fork/content/userinfo/storage/data-sensitivity.md,"+++
title = ""RC Systems Data Sensitivity""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2025-03-13T17:45:12-05:00""
tags = [
    ""rivanna"",
    ""hpc"",
    ""research"",
    ""storage"",
    ""public-data"",
    ""internal-use-data""
]
draft = false
+++








Standard-Security Zone (SSZ)
High-Security Zone (HSZ)



Storage
Computing Environments
Storage
Computing Environments


Data Classification
Research Project (/project)
Research Standard (/standard)
Rivanna/Afton (/home & /scratch)
High-Security Research Standard 
Ivy VM (/home)
Rio (/home & /scratch)
ACCORD




Public
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ


Internal-Use
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ


Sensitive
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ


Highly-Sensitive
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚úÖ
‚ùå


Limited Dataset  1  
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚úÖ
‚ùå


De-Identified Dataset  2  
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ
‚úÖ


HIPAA  3  
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚úÖ
‚ùå


CUI  4  
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚ùå
‚ùå


Controlled-Access Data   5   
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚úÖ
‚ùå


FERPA  6  
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚úÖ
‚úÖ


ITAR  7  
‚ùå
‚ùå
‚ùå
‚úÖ
‚úÖ
‚ùå
‚ùå


PCI-DSS  8  
‚ùå
‚ùå
‚ùå
‚ùå
‚ùå
‚ùå
‚ùå





 1 Limited datasets have direct identifiers removed, but may contain indirect identifiers including, complete dates, age, city, state, and complete ZIP code.   
 2 De-identified datasets contain no identifiers. Note: identifiers can be recoded such that the source information is anonymized (e.g. date shifting, urban/rural determinations vs. ZIP codes, randomly generated subject identifier, etc.)  
 3 Health Insurance Portability and Accountability Act (HIPAA). Information protected under HIPAA includes any protected health information (PHI) in the medical record that can identify an individual. More information can be found here.  
 4 Controlled Unclassified Information (CUI). CUI data is information the government creates or possesses that requires safeguarding or dissemination controls when handling. More information can be found  here.   
 5 Controlled-access data are protected NIH data whose access is controlled by implementing security measures to verify the identity of requesters and their inteded data use, even if it is de-identified or lacks explicit limitations on subsequent use. This includes controlled-access data downloaded from the following controlled-access data repositories: Database of genotypes and phenotypes (dbGaP), BioData Catalyst, NCI Genomic Data Commons, ‚Äå‚ÄåThe NHGRI Genomic Data Science Analysis, Visualization, and Informatics Lab-Space (AnVIL), National Institute of Mental Health Data Archive (NDA), NIA Genetics of Alzheimer's Disease Data Storage Site (NIAGADS). The full list of controlled-access repositories can be found  here. 

    Projects with a data use agreements approved after 1/25/25 are required to protect controlled-access data acquired from a controlled-access repository in compliance with NIST 800-171 security controls. More information can be found  here.   
 6 Family Educational Rights & Privacy Act (FERPA). FERPA is a federal law that governs access to student education records. This includes personally identifiable information (PII) like name, SSN, date of birth, grades, and course schedules. More information can be found  here.   
 7 International Traffic in Arms Regulations (ITAR). This includes military technology and software, technical data, and services. More information can be found  here.   
 8 Payment Card Industry Data Security Standards (PCI-DSS). PCI-DSS is a set of security standards that define how payment card data should be protected. This includes data containing debit card or credit card information. More details can be found  here.   
"
rc-website-fork/content/userinfo/storage/cloud.md,"+++
author = ""RC Staff""
description = """"
title = ""Cloud Storage Solutions""
date = ""2022-02-20T10:08:29-05:00""
draft = false
tags = [""storage"",""cloud"",""aws"",""gcp"",""s3""]
categories = [""userinfo""]
images = [""""]
+++



Amazon Web Services
Tiered object storage

Amazon S3 and Glacier offer cloud-based, affordable, unlimited capacity for storage from anywhere. Advanced
      features include scalability, lifecycle management, encryption, and sharing. S3 is ideal for static files that
      need to be retrieved from any location (PDFs, images, video, etc.). Glacier is archival storage, perfect for
      grant compliance that requires data retention.
    
How RC can help:

Lower pricing - UVA has an Internet2 discount available for educational use. Contact us to create an account for you or your research project.
          Cost estimates - Cloud storage is not free. Consideration should be made to the size of your files and how often they will be retrieved. We can help estimate storage costs in AWS.
          Security - We can assist you in keeping your files private, or sharing with the appropriate parties. We can also help you understand encryption models available in S3/Glacier.
          Management - Automated lifecycle management is available in either service. We can help you set up policies to move data from S3 to Glacier after N days or years, or to delete after the appropriate length of time.
          Access - S3 and Glacier can be accessed via a web UI, command-line, or other third-party tools. RC can discuss the appropriate tools for your users.
          Training - Attend one of our free cloud workshops, or receive in-person training from our staff to understand additional features available.
        

Learn more






Google Cloud Storage
Tiered data storage

      Google Cloud Storage is a tiered storage service that allows for world-wide storage and retrieval of any amount of data at any time. You can use Google Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download.
    
How RC can help:

Lower pricing - UVA has an Internet2 discount available for educational GCP accounts. Contact us to create an account for you or your research project.
          Cost estimates - Cloud storage is not free. Consideration should be made to the size of your files and how often they will be retrieved. We can help estimate storage costs in AWS.
          Security - We can assist you in keeping your files private, or sharing with the appropriate parties. We can also help you understand encryption models available in Google Cloud Storage.
          Management - Automated lifecycle management is available in Cloud Storage. This allows for the automated movement of files to another tier after N days or years, or to delete after the appropriate length of time.
          Access - Google Cloud Storage can be accessed via a web UI, command-line, or other third-party tools. RC can discuss the appropriate tools for your users.
          Training - Attend one of our free cloud workshops, or receive in-person training from our staff to understand additional features available.
        

Learn more


"
rc-website-fork/content/userinfo/storage/non-sensitive-data.md,"+++
title = ""Public and Moderately Sensitive Data Storage""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2019-10-04T17:45:12-05:00""
tags = [
    ""rivanna"",
    ""research"",
    ""storage"",
    ""public-data"",
    ""internal-use-data""
]
draft = true
+++
/home
/home is a free 50GB space provided to users of the HPC system and is visible from the login and compute nodes. /home is the default working directory when logging on to UVA HPC. Users can also access their home directory at /home/$USER, where $USER is an individual's UVa computing ID.
/scratch
/scratch is a Lustre high performance parallel filesystem accessible via the login and compute nodes.
{{% callout %}}
{{% scratch-policy %}}
{{% /callout %}}
How to request /home and /scratch space
/home and /scratch space can be obtained by requesting an allocation on UVA HPC. The process of getting access to Rivanna is described here.
Research Project Storage
The Research Project Storage file system provides users with a collaborative space for data storage and sharing. Members in the same group have access to a shared directory created by the team lead or PI. Group membership can be defined and managed through the Grouper (requires VPN connection). /project storage is mounted on the HPC cluster and runs on a new scale-out NAS file system.
How to request /project storage space
/project storage can be purchased for {{% storage-pricing project %}}/TB/YR by using this form. When filling out the form, the PI can specify the size of the /project directory and the name of an existing or new Grouper group that can access this space. We recommend choosing a Grouper group name specific to your group or collaboration for the /project directory. This will reduce confusion in the future if you manage multiple Grouper groups and directories on other storage systems.
Once the request has been submitted, the PI will receive a notification that the /project space has been provisioned within 24 hours. Once the space becomes available, the PI can grant access to lab members by adding them to the Grouper group. Users in the Grouper group will see the directory (/project/Grouper_group_name) after logging into UVA HPC. Addition and removal of users is managed by the PI of the group.
{{% group_creation_tip %}}
Public & Moderately Sensitive Data Storage Systems {#public-moderately-sensitive-data-storage}
/home, /scratch, and /project storage are based on a Linux file system. Users can invoke generic Linux commands to manage files and directories (mv, cp, mkdir), manage permissions (chmod, chown) and navigate the file system (cd, ls, pwd).  If you or your collaborators are unfamiliar with some of these commands, we encourage you to take time to review some of the material below:

A Gentle Introduction
10 Essential Linux Commands
How To Manage Files From The Linux Terminal


Shell Novice
"
rc-website-fork/content/userinfo/storage/sensitive-data.md,"+++
title = ""Highly Sensitive Data Storage - Ivy""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2018-04-19T17:45:12-05:00""
tags = [
    ""ivy"", 
    ""sensitive data"",
    ""storage"",
]
draft = false
+++
Overview
Residing within the High Security Zone (HSZ), the Ivy secure computing environment is designed to fit your highly sensitive research computing needs and meets HIPAA, FERPA, CUI and ITAR compliance standards. Within the HSZ, researchers can store their highly sensitive research data in High-Security Research Standard Storage. 
Ivy Central Storage {#ivy-central-storage}
Ivy Central Storage (ICS) was an HSD parking zone and central storage pool with a capacity greater than 1PB. This storage space was available for researchers with highly sensitive data and could be mounted on an Ivy Virtual Machine. 
As of 10/15/24, ICS will be upgraded to High-Security Research Standard Storage.
High-Security Research Standard Storage {#hs-standard-storage}
High-Security Research Standard Storage is an HSD storage area within the HSZ with a capacity greater than 6PB. High-Security Research Standard Storage is similar to Research Standard Storage, however it is integrated with the High-Security Data Transfer Node and mounted on an Ivy virtual machine (VM) to create a highly secure environment. For added security, files stored on High-Security Research Standard Storage are read & write only. Note: snapshots, backup, and replication are not provided. 
Researchers can request space on High-Security Research Standard Storage by first requesting an Ivy account using the Ivy request form. Researchers are granted 1TB of space at no-cost, and additional space can be requested in 1TB increments using our Storage Request form.
Data Transfer to Ivy
To ensure that files are always secure, data can only be transferred to Ivy through the High-Security Data Transfer Node using Globus Connect. Globus provides access to data on local machines and HSZ storage. Data can then be moved between HSZ storage as needed. Globus is well suited for transferring both small files and large amounts of data. More information on Globus data transfer can be found here.

  Sensitive Storage Data Transfer
  High level Overview

"
rc-website-fork/content/userinfo/omero/_index.md,"+++
description = """"
title = ""Image Data Management with OMERO""
draft = true
date = ""2020-02-17T17:45:12-05:00""
tags = [""omero"",""image processing"",""storage"",""infrastructure""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
layout = ""single""
+++

{{% callout %}}

OMERO is a database for management of imaging data. UVA is hosting a centralized OMERO database instance backed by centralized storage that facilitates sharing, processing and annotating images for your research group and invited collaborators.
{{% /callout %}}
Overview
With the advent of high-throughput screening, the need for efficient image management tools is greater than ever. From the microscope to publication, OMERO is a database solution that handles all your images in a secure central repository. You can view, organize, analyze and share your data from anywhere you have internet access. Work with your images from a desktop app (Windows, Mac or Linux), on UVA's high performance computing platform (Rivanna and Afton), from the web, or through 3rd party software like Fiji and ImageJ, Python, and MATLAB. OMERO is able to read over 140 proprietary file formats, including all major microscope formats. 

Getting Access
OMERO accounts can be requested by submitting a OMERO request form. Only faculty members may submit a request.
By default, all group members will be able to view their own and others' 
data. Group members can make annotations on each other's data, but cannot modify or delete another member's data. For details on obtaining more restrictive or flexible permissions for your group members, please read the Group/User Permissions section.
Pricing: {{% storage-pricing omero %}} / TB per year.
Request Access to OMERO

Connecting to the OMERO database
Once you have an OMERO account you can log in and begin importing images. You can log into your OMERO account on your computer using the OMERO.insight desktop client, or you can use the OMERO web interface through your web browser. 


When off Grounds, you have to connect through the UVA VPN in order to access the OMERO database. We recommend to connect to the UVA More Secure Network if available. The UVA Anywhere VPN should only be used if the UVA More Secure Network is not available.

Logging in with OMERO.insight
Set up the Software on your Computer


The UVA server is running OMERO 5.4.10.  In order to connect to that server you need to install the compatible Windows, Mac, or Linux client.


Windows: OMERO.insight-5.4.10-ice36-b105-win.zip

Mac: OMERO.insight-5.4.10-ice36-b105-mac.zip

Linux: OMERO.insight-5.4.10-ice36-b105-linux.zip
Please download the .zip file appropriate for your computer and install the client following these installation instructions.


Open OMERO.insight and follow the configuration instructions to set up the connection to the UVA OMERO server. Under step 3 of the instructions, enter omero.hpc.virginia.edu as the server address. The port address needs to be set to 4064.


To log in, enter your computing ID (e.g. mst3k) in the Username field. For Password, enter the password emailed to you after your initial account request (you will be able to change this after logging in for the first time). The OMERO database password is not the same as your Eservices or Netbadge password. Click Login.


Logging in with OMERO.web


Go to http://omero.hpc.virginia.edu.


Log in with your computing ID (e.g. mst3k) and the password that was emailed to you 
upon your initial account request. The OMERO database password is not the same as your Eservices or Netbadge password.


Important things to note when using the OMERO web client interface:


OMERO.web cannot be used to import images


Tags cannot be created in OMERO.web



Changing your OMERO Database Password
The OMERO database password is not the same as your Eservices or Netbadge password. After logging into OMERO for the first time, it is highly recommended that you change your 
password. You can manage your account settings using either OMERO.insight or OMERO.web. 
Password Change in the OMERO.insight Desktop Client


After logging into OMERO using the desktop app, click the Administration tab in the side 
menu.


Click the arrow next to your lab/group account name, then click your own name to open 
the user account menu.


Enter your new password in the New Password field and click the Change Password 
button.


Password Change in the OMERO Web Client


After logging in to OMERO in your web browser, click your name in the top right corner 
of the screen and then click User settings.


Click the Change Password button. Enter your current password and your desired new 
password. Click OK when complete.


If you cannot remember your password, please contact us through our support request webform. Our system administrators can reset the password for you.

Group/User Permissions
OMERO users can have one of two user roles, Group owner or Group member.
As a Group owner of a lab/group account, you can edit the permissions of other 
users in your group. This can be done from the desktop app OMERO.insight.


In the Administration tab of the sidebar menu, click the name of your group. This 
will open the Group settings menu.


Select your desired permissions and click Save.



There are several permissions settings, which are described below. By default, 
all group permissions are set to Read-Annotate.



Permission Type
Description




Private
The group owner can view data of all group members but cannot add annotations. Regular group members can only view and annotate their own data. This permissions setting does not allow for much collaboration.


Read-Only
The group owner can view data of all group members but cannot add annotations. Regular group members can only view and annotate their own data. This permissions setting does not allow for much collaboration.


Read-Annotate
The group owner and group members can view and annotate each other's data. Regular members cannot modify or remove other members' images. [Default]


Read-Write
The Group owner and Group members can view, annotate, modify, and delete each other's data.




Image Analysis with OMERO
OMERO is compatible with a variety of third-party image processing software packages. Using these OMERO-software bindings, you can import images from OMERO to your software such as Fiji or Python,and then process and analyze them as usual. You can then export any results or preprocessed images back to OMERO.
Using OMERO to serve images to your analysis software has many benefits over more traditional methods of reading imaging data. With OMERO, there is no need to download the images directly to your local machine. 
ImageJ/Fiji
Images managed by OMERO can be imported using the ImageJ/Fiji plugin for OMERO. Detailed 
instructions for installing and using the plugin can be found in OMERO's online documentation: 
https://help.openmicroscopy.org/imagej.html.
An introduction with example scripts that demonstrate the basic concepts of writing Fiji scripts to interact with the OMERO database are described in our tutorial .
MATLAB and Python
You can install packages to connect to OMERO with MATLAB or Python. These packages include 
functions for connecting to the OMERO server and reading and exporting data.
OMERO's online documentation for the OMERO MATLAB language bindings can be found here: 
https://docs.openmicroscopy.org/omero/5.5.0/developers/Matlab.html
More information on the OMERO Python language bindings can be found here: 
https://docs.openmicroscopy.org/omero/5.5.0/developers/Python.html.
More in-depth tutorials and sample scripts will be available on our workshop site soon!"
rc-website-fork/content/userinfo/linux/uva-anywhere-vpn-linux.md,"+++
title=""UVA Anywhere VPN on Linux""
description = """"
date = ""2021-03-22T11:16:27-05:00""
author = ""Staff""
categories = [
  ""userinfo"",
]
images=[""""]
tags = [
  ""login"",
  ""security"",
  ""vpn""
]
draft = false
+++
ITS does not support the UVA Anywhere VPN client on Linux.  These instructions may work but they are provided for user information only.  UVA RC does not support usage of the VPN on any platform.

Setting up the VPN


Install Software Prerequisites
You must install some software using yum,dnf, or apt-get.  Note the slight difference in naming convention between distributions.


Rocy/Alma/RedHat/Fedora
These distributions need the following packages:

openssl
openconnect
NetworkManager-openconnect
NetworkManager-openconnect-gnome



Ubuntu
The packages are the same but the names are different.  Ubuntu 18.04 and up requires an additional package.

openssl
openconnect
network-manager-openconnect
network-manager-gnome
network-manager-openconnect-gnome



It will be necessary for Network Manager to be able to manage the connection.


Obtain a Certificate
Go to this unpublicized Web location to obtain a certificate for n non-specific OS. You will be required to sign in with Netbadge. Once authenticated, fill out the form.

Your passphrase need not be related to your Netbadge password, and it must be 15 characters or fewer.  The MAC address of your system is optional for UVA Anywhere.
Click the link to download the certificate.  You will receive a file ending in .p12.  In this example we will assume it is named mst3k.p12.
Do not click the Next button. Once the download is completed, you may close the tab for the certificate site.


{{< figure src=""/images/linux/download-cert.png"" alt=""download-cert"" width=30% >}}



Configure with Network Manager
Click the network app in your tray, or go to Settings->Network.  Choose VPN and click the + to add a VPN.



Select the Cisco Anyconnect compatible VPN option.


Fill in the blanks for a new VPN.  Please use the More Secure VPN if you have access to it. The gateway is **moresecure-vpn-1.itc.virginia.edu**.  Otherwise, use the UVA Anywhere VPN whose gateway is **uva-anywhere-1.itc.virginia.edu** as shown in the figure below.

NetWork Manager may not recognize the `.p12` format.  You can use the file manager of your desktop system to drag and drop the file into both the ""User Certificate"" and the ""Private Key"" boxes.


Click ""Add.""

In the Details tab, make sure that ""Make available to all users"" is *not* checked.  This should be the default.

Connecting to the VPN
Start the VPN through the Network Manager, either through the applet in the tray (Ubuntu) or in the Notifications section of the taskbar (Rocky/Alma/Fedora).  The state can be controlled through the right arrow.
For the first connection, you may need to go through the Settings application to connect.  After that, log out.  When you log back in, your VPN should appear in the taskbar or tray (the illustration was taken from a Rocky Linux installation).


More Secure VPN

UPDATE: Users of the More Secure VPN will now be required to authenticate through Duo before connecting.  When prompted for a password, enter the word push or PUSH (it is not case-sensitive); you will then receive an approval notification on your mobile device.  After approving the request, the client will connect to the VPN.  Alternatively, you may enter a passcode generated by Duo as the password.
"
rc-website-fork/content/userinfo/howtos/_index.md,"+++
title = ""How To""
description = """"
author = ""RC Staff""
images = [
  """",
]
date = ""2020-02-16T09:55:56-05:00""
categories = [""howto""]
tags = [
  ""howto"",
  ""rivanna"",
  ""ivy"",
  ""storage"",
  ""general"",
  ""cloud"",
]
draft = false
quell_footer = true
layout = ""single""
+++








General
General tips and tricks for computational research.

General HowTos ‚Ä∫












Rivanna and Afton
High Performance Computing platforms

HPC HowTos ‚Ä∫














Ivy
Secure Data Computing Platform

Ivy HowTos ‚Ä∫












Storage
Research Data Storage & Transfer

Storage HowTos ‚Ä∫





"
rc-website-fork/content/userinfo/reference/bowtie.md,"+++
author = ""RC Staff""
description = """"
title = ""Bowtie 2""
draft = true
date = ""2017-03-01T15:28:51-05:00""
tags = [""r"",""python"",""bowtie"",""sequencing""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""Bowtie Project""
projecturl = ""http://bowtie-bio.sourceforge.net/bowtie2/index.shtml""
+++
{{< define Bowtie ""Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes."" >}}
Overview
Bowtie is a part of a suite of tools for pipeline processing:

Bowtie: Ultrafast short read alignment
CloudBurst: Sensitive MapReduce alignment
Contrail: Cloud-based de novo assembly
Crossbow: Genotyping, cloud computing
Cufflinks: Isoform assembly, quantitation
Lighter: Fast error correction
Myrna: Cloud, differential gene expression
Tophat: RNA-Seq splice junction mapper

"
rc-website-fork/content/userinfo/reference/jq.md,"+++
author = ""RC Staff""
description = """"
title = ""JQ (Shell)""
draft = true
date = ""2017-02-08T15:28:51-05:00""
tags = [""json"",""shell"",""jq""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""jq Project""
projecturl = ""https://stedolan.github.io/jq/""
+++

jq
jq is like sed for JSON data - you can use it to slice and filter and map and transform structured data with 
the same ease that sed, awk, grep and friends let you play with text.

Installation
Follow the instructions available on https://stedolan.github.io/jq/ for installing the latest version for your platform. 

Basic Usage
jq is used to parse JSON, which helps with programmatic interaction with many APIs. For example, you can retrieve data from an open API like GitHub:
curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5'

And then pipe that output to jq to begin to parse the results. You can filter down to just the first record [0]:
curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' \
    | jq '.[0]'

And then begin to drill down to specific elements of the response hierarchy, building into a new structure:
curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' \
    | jq '.[0] | {message: .commit.message, name: .commit.committer.name}'

which results in this response:
{
  ""message"": ""Merge pull request #162 from stedolan. Closes #161"",
  ""name"": ""Stephen Dolan""
}

jq is extremely useful alongside the AWS command-line tools.

Or, if you wanted to grab a series of values from all entries you retrieved, you could filter into an array. Here are the sha values for 5 recent commits by the user stedolan:
curl 'https://api.github.com/repos/stedolan/jq/commits?per_page=5' | jq -r .[].sha

which results in this output:
dc679081fa770c260ca9a569a8a4fdbb10bcdc20
597c1f6667746058e88a9f6fb0415f80fe114b18
125071cf005e687d4beba9d5822b1c6a72d7d14c
2fb099e4cfe5a9fedd55a1ace44ae2c5ee02cb12
6f9646a44ff0046126f5a2c3010e92a974da7c48


Real-world examples
Here are some bash snippets for various tasks that combine the AWS CLI with jq:
{{< gist nmagee d13a67b82859fcef53acff568ecb114d >}}

Online Testing
The authors of jq also provide https://jqplay.org/ as a space where you can interactively build and test your jq parsing."
rc-website-fork/content/userinfo/reference/rstudio-docker.md,"+++
author = ""RC Staff""
description = """"
title = ""RStudio Server (Docker)""
draft = true
date = ""2017-03-01T15:28:51-05:00""
tags = [""r"",""docker""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""RStudio Server (Docker)""
projecturl = ""https://hub.docker.com/r/rocker/rstudio/""
+++
{{< define ""RStudio Server"" ""RStudio Server enables you to provide a browser based interface to a version of R running on a remote Linux server, bringing the power and productivity of the RStudio IDE to server-based deployments of R. The instructions below will launch your RStudio environment locally within a Docker container."" >}}
What is Docker?
{{< youtube aLipr7tTuA4 >}}
Install Docker
Click the button below and download the appropriate Docker Edition for your platform. We suggest the CE ""Community Edition.""
Download Docker

Run RStudio Locally
Run these two commands for a web-based deployment of RStudio Server on your local workstation:
docker pull rocker/rstudio
docker run -d -p 8787:8787 rocker/rstudio
More Information

Using the RStudio container
Other related images
Sharing Files between your Computer and the Container
"
rc-website-fork/content/userinfo/reference/aws-bioinformatics.md,"+++
date = ""2017-02-22T15:28:51-05:00""
description = """"
title = ""AWS for Bioinformatics""
draft = true
tags = [""bioinformatics"",""samtools"",""AWS"",""bowtie2"", ""genomics""]
categories = [""reference""]
images = [""""]
author = ""RC Staff""
type = ""reference""
+++
Setting up computational infrastructure on AWS is a well-defined, though somewhat time-consuming process. This introduction is designed to explain some of the terminology, define the steps required to set up AWS, and point the user to some excellent tutorials/resources created for bioinformatics and genomics.  
Setting up a server on AWS involves making decisions on the following broad categories.
Set Up a Computational Server
An excellent tutorial that covers the steps for creating an EC2 (Elastic Compute Cloud) instance (up to logging into your EC2) along with a myriad of questions such as pricing and what kind of computing and storage resources to choose from geared towards bioinformatics and genomics is available here. Broadly speaking, setting up a server (EC2 instance) requires the following steps.


AWS user account: The user account can be your own, or you could be part of a group user account (e.g., your lab‚Äôs account on AWS). You (or the administrator of the account) can set up permissions for you, such as whether you are allowed to stop instances started by other members of the lab. AWS calls it an IAM Role. See here for AWS policies and sign-up procedures for user accounts.


Amazon Machine Image (AMI): AMI is an operating system with or without pre-defined computational power, memory, hard disk, or software tools. If you start from scratch, you will choose all the elements step-by-step. We recommend using Ubuntu as your operating system as quite a few community bioinformatics tools are built for Ubuntu. At this time, NCBI BLAST is available as a community AMI on Amazon, and we expect more to be available over time.  


EC2 Instance: If you are building an AMI from scratch, you‚Äôll have to choose the right processing power, memory, and network capabilities for the analyses required. Each operating system available on AWS can come with slightly different processing and memory options, but for the most part the capabilities are comparable. As a start, an instance with 4 cores and 32 GB of memory should be reasonable for a lot of genomics analyses, though for large scale analyses you could need more computing power and storage space. Depending on your needs, you can buy more computational power from AWS. 


Elastic Block Storage (EBS): This is the equivalent of choosing how much hard disk (SSD) to add to your EC2. You can do that while specifying the EC2 or later on as your storage requirements increase or decrease over time. EBS is one of multiple storage options available with AWS, but is a good first choice (along with S3 for long term storage capability) for a researcher starting off on AWS. 500GB of EBS is probably a good start to try out a few analyses. Please see here for more information on storage options with AWS. 


Setting up security protocols to connect with your AMI. This can include which IP addresses are recognized for inbound and outbound communication, methods of connecting with the AMI, etc.


Logging in using ssh and installing the software you‚Äôd like to use, e.g., Bowtie2, Samtools, or Bedtools.


Installing Software
Once you‚Äôre logged into an EC2 instance, you can install most software using the apt-get command on Ubuntu and yum on Amazon Linux, e.g., 
    apt-get -y install samtools bedtools python-mysqldb
To install MACS, first install pip and then use pip to install MACS:
apt-get -y install python-pip
pip install macs2

To install SRA Toolkit, scroll down to the section SRA Toolkit here, and follow the step-by-step instructions. 
Considerations when using AWS


If you are trying to ssh into your instance and you get ‚Äúoperation timed out‚Äù error, check in the EC2 Dashboard on AWS that the instance is running. Another reason for getting the error could be that the EC2 instance only accepts inbound network connections from a specific IP, so if you change your Wi-Fi networks your laptop‚Äôs IP may not be recognized. Go to EC2 Dashboard, click on Running Instances, click on the instance you are trying to connect to, under Description click on Security groups, click on Inbound, and Edit to add a connection with Source ‚ÄúMyIP.‚Äù 


Some of the bioinformatics software is available on apt-get but not yum. Hence, we recommend using Ubuntu as the operating system over Amazon Linux. 


Your EC2 instance you are setting up with all the tools you will need for your analysis can be saved as an AMI. If you stop and start the instance it will boot up again with all the software you installed, but if you terminate the instance you can use a saved AMI to create the instance again without much work.

"
rc-website-fork/content/userinfo/reference/bowtie-docker.md,"+++
author = ""RC Staff""
description = """"
title = ""Bowtie (Docker)""
draft = true
date = ""2017-03-01T15:28:51-05:00""
tags = [""bioinformatics"",""bowtie"",""docker""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""Bowtie (Docker)""
projecturl = ""https://hub.docker.com/r/biocontainers/bowtie/""
+++
{{< define ""Bowtie"" ""Bowtie is an ultrafast, memory-efficient short read aligner. It aligns short DNA sequences (reads) to the human genome at a rate of over 25 million 35-bp reads per hour. Bowtie indexes the genome with a Burrows-Wheeler index to keep its memory footprint small: typically about 2.2 GB for the human genome (2.9 GB for paired-end). The instructions below will launch your Bowtie environment locally within a Docker container."" >}}
What is Docker?
{{< youtube aLipr7tTuA4 >}}
Install Docker
Click the button below and download the appropriate Docker Edition for your platform. We suggest the CE ""Community Edition.""
Download Docker

Run Bowtie Locally
Run these two commands for a web-based deployment of Bowtie on your local workstation:
docker pull biocontainers/bowtie
docker run -d biocontainers/bowtie
More Information

About the BioContainers Project
BioContainers on GitHub
BioContainers Registry
"
rc-website-fork/content/userinfo/reference/boto3.md,"+++
type = ""reference""
draft = true
date = ""2017-02-02T22:28:18-05:00""
author = ""RC Staff""
description = """"
title = ""Boto3 Library (Python)""
categories = [""reference""]
images = [""""]
object = ""Python""
tags = [""python"",""boto3"",""aws"",""cloud""]
project = ""boto3 Home""
projecturl = ""https://github.com/boto/boto3""
+++

boto3 - the AWS SDK for Python
Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2. You can find the latest, most up to date, documentation at Read the Docs, including a list of services that are supported.
boto3 is unavailable on the Ivy Secure Platform.

Install the boto3 library:
Install using pip on Linux/Mac:
pip install boto3
or for Windows (or the latest release) simply clone from GitHub:
$ git clone git://github.com/boto/boto3.git
$ cd boto3
$ python setup.py install


Authentication
boto can run under AWS authentication granted in a few ways:

Inherited from user environment variables
Using hard-coded AWS credentials in your code (Never in production / Never committed to git)
Using hard-coded AWS credentials in a local config.json file (Never committed to git)
Using AWS IAM roles assigned to the EC2 instance when you created it (Best practice)


Create the client
The client is generally referenced directly by name as a resource:
```
Get the service resource
sns = boto3.resource('sns')
```

Make your request
Here is an example of sending a message to an SQS queue:
{{< gist nmagee f55e6d1c03a673a44333e70d1fa6872c >}}

Errors
Note that error handling and parsing error messages is not built into most boto3 requests. You should use try and catch, and botocore can help you display, log, or act on errors."
rc-website-fork/content/userinfo/reference/s3.md,"+++
date = ""2017-02-24T16:40:37-05:00""
description = """"
title = ""Amazon S3 - Simple Storage Service""
draft = true
tags = [""s3"",""aws"",""storage"",""awscli"",""boto3""]
categories = [""reference""]
images = [""""]
author = ""RC Staff""
type = ""reference""
+++

S3 - Simple Storage Service
S3 is a cloud-based object storage service. It has no size or file type restrictions, 
meaning it is ideal for backups, media files, and remote storage of original data.
Files stored in S3 are automatically private, geographically distributed redundantly,
and are 99.99999999999% resilient (13x9's).

S3 using the AWS CLI (Shell)
To install the AWS CLI for Linux or Windows, refer to this documentation.
List your buckets
aws s3 ls

Make a bucket
aws mb s3://my-bucket

List the contents of a bucket
aws ls s3://my-bucket/                     # Displays the base contents of a bucket
aws ls s3://my-bucket/and-folder/          # Displays the contents of a subdir
aws ls s3://my-bucket/and-folder/*.png     # Displays all PNG files in the subdir

Copy an object into S3
aws s3 cp my-local-file.zip s3://my-bucket/          # Copies a file into a bucket
aws s3 cp my-local-file.zip s3://my-bucket/subdir/   # Copies a file into a subdir of a bucket
aws s3 cp s3://my-bucket/file.zip ./                 # Copies a file from a bucket

Remove a file from S3
aws s3 rm s3://my-bucket/subdir/my-file.zip   # Removes the file (called a 'key') from S3

Synchronize a local folder with S3
aws s3 sync myfolder s3://my-bucket/myfolder/   # Syncs local folder's contents up to S3
aws s3 sync s3://my-bucket/myfolder/ myfolder   # Syncs remote S3 folder down to local folder


S3 using Python boto3
Moving objects in and out of S3 using boto3 is similar in concept to the awscli but not identical. Below are two examples, one for copying a local file
into a bucket, and one for copying a file in S3 to the local computer.
Upload files
{{< gist nmagee c2be9caa4479bb11bb1b6097d7269946 >}}
Download files
{{< gist nmagee a8b42a126235a0366f7472efd4965d18 >}}

S3 Desktop Tools
There are many other tools available for managing files in S3 that are command-line driven, web-based, or standalone applications. 
Linux/MacOSX users might be interested in s3cmd (command-line), Transmit, or Cyberduck (applications).
Windows users should explore CloudBerry, Cyberduck (applications), and S3Express (command-line).

S3 Permissions / Access
To begin with, any S3 bucket that you create (and all files/objects in it) will be completely private to you, the account holder, until you make it otherwise.
If you need to share your files, you have multiple methods of controlling access to objects within your S3 bucket. The three methods are:

ACLs
S3 Bucket Policies
Presigned URLs

ACLs
ACLs are the broadest and bluntest method of securing your buckets and their contents. They are less flexible for setting specific folder and file permissions.

private
public-read
public-read-write
authenticated-read
aws-exec-read
bucket-owner-read
bucket-owner-full-control

S3 Bucket Policies
S3 bucket policies grant various AWS accounts unique permissions to your bucket and objects within it. For instance, you may want to
grant a backup administrator the ability to WRITE new files to a bucket, but -- perhaps for auditing reasons -- you do not want her to have the ability
to DELETE files. This would be accomplished with an S3 bucket policy.
Here is a sample S3 bucket policy that grants read (GET) and write (PUT) access to the root user of another account (identified by the account number ""111122223333""), 
but disallows all other actions.
{
  ""Version"":""2012-10-17"",
  ""Statement"":[
    {
      ""Sid"":""AddCannedAcl"",
      ""Effect"":""Allow"",
      ""Principal"": {""AWS"": [""arn:aws:iam::111122223333:root""]},
      ""Action"":[""s3:GetObject"",""s3:PutObject""],
      ""Resource"":[""arn:aws:s3:::my-bucket/*""],
    }
  ]
}

While their structure is similar, there is a difference between S3 bucket policies and IAM user policies. More about S3 bucket policies can be found here.
Presigned URLs
Presigned URLs are a method of creating a unique, expiring signed URL for a user to retrieve an object in S3 for a specific period of time. Here is an example of
how to create a presigned URL using the awscli that expires in 3600 seconds:
aws s3 presign --expires-in 3600 s3://my-bucket-resources/my-file.zip

The output URL would look something like this:
https://my-bucket-resources.s3.amazonaws.com/my-file.zip?
    \ AWSAccessKeyId=AKIAJH5Z6ORWK3XD6YYQ&Expires=1487978024
    \ &Signature=%2BKtyJlbZ4nxw2xhN7On52KTYGjI%3D


S3 Lifecycle Management
Since it is technically ""object storage"" and not true ""disk storage"", S3 comes with the ability to automatically manage file, folder, or bucket lifecycles.
If, for instance, you had a bucket dedicated to storing your backups and you only wanted to keep 3 months' worth, this would be possible by creating a simple
lifecycle policy for your bucket that deletes files after that period of time.
Alternatively, you can create tiered lifecycle policies that will move your S3 data to Amazon's archival storage service named Glacier for a period of time, and then delete them after another period has passed.
To work with your bucket's lifecycle policies, use the AWS console and go to the properties for your bucket within the S3 service.

S3 Pricing
Current pricing is available here, but generally S3 storage costs $0.023 per GB per month. However, be careful when 
moving very large sets of data as other costs may be incurred:

All data transfer IN to S3 is free.
Data transfer OUT to other AWS services is free.
Data transfer OUT to the Internet (i.e. the UVA campus) costs nothing for the first GB each month, but $0.09 per GB beyond that.

Sample storage costs (approximate, no data transfer costs included):



Size
Monthly
Annually




10GB
$0.23
$2.76


100GB
$2.30
$27.60


1TB
$23.00
$276.00


10TB
$230.00
$2,760.00




Real-world Example
Below is a bash script that backs up a specific MySQL database, compresses it, names it with the date, then ships off to S3.
{{< gist nmagee caa3a3395682e1aeb39033201b096e23 >}}"
rc-website-fork/content/userinfo/reference/google-cloud-sdk.md,"+++
author = ""RC Staff""
description = """"
title = ""Google Cloud SDK (Shell)""
draft = true
date = ""2017-02-08T15:28:51-05:00""
tags = [""json"",""shell"",""google""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""Google Cloud SDK""
projecturl = ""https://cloud.google.com/sdk/""
+++

Google Cloud SDK
The Cloud SDK is a set of tools for Cloud Platform. It contains gcloud, gsutil, and bq, which you can use to access Google Compute Engine, Google Cloud Storage, Google BigQuery, and other products and services from the command-line. You can run these tools interactively or in your automated scripts.


Download, Install, and Setup

Python 2.7 is required to install and use the Google Cloud SDK.
Visit https://cloud.google.com/sdk/ and download the installer for your OS platform.
For Mac/Linux users, move the decompressed google-cloud-sdk folder to an appropriate place, then run the ./install.sh script. Windows users have an .exe wizard that will complete the installation process.
To set up after installation, run gcloud init and you will authenticate (using a web browser) into your Google account. Follow the prompts to create a project, etc.
Services that incur charges will have to be associated with billing information (storage, compute, etc.)


Basic Usage
To get help:
gcloud help

To see information about your installation:
gcloud info

Included with the package is the gsutil tool for Google cloud storage
gsutil mb gs://my-bucket            # Make a bucket
gsutil ls gs://my-bucket/folder/    # List contents of a bucket sub-folder
gsutil cp *.txt gs://my-bucket      # Copies all text files up into bucket


Real-world Examples
A backup script to run nightly and keep two weeks of archives:
{{< gist nmagee fe999280428f15ebed98ca88942fc29f >}}
A snippet to create an expiring, signed URL of an object within a bucket:
gsutil signurl -d 10m <private-key-file> gs://<bucket>/<object>

A snippet to monitor a bucket for changes and send alerts to a web endpoint. This
would trigger a notification every time a new object is added or deleted, or if metadata is
updated:
gsutil notification watchbucket https://example.com/notify \
  gs://example-bucket
"
rc-website-fork/content/userinfo/reference/index.md,"+++
title = ""Reference Guides""
draft = true
tags = [""reference"",""support""]
categories = [""reference"",""support""]
images = [""""]
author = ""RC Staff""
date = ""2017-02-08T10:42:33-05:00""
description = """"
type = ""reference""
+++
By language and platform




        Docker / Containers
      









        Bioinformatics / Genomics
      


AWS for Bioinformatics
Bowtie 2
Genomics and Bioinformatics Pipelines




"
rc-website-fork/content/userinfo/reference/anaconda.md,"+++
author = ""RC Staff""
description = """"
title = ""Anaconda (Python)""
draft = true
date = ""2017-03-01T15:28:51-05:00""
tags = [""anaconda"",""python""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""Anaconda Home""
projecturl = ""https://docs.continuum.io/""
+++
{{< define ""Anaconda"" ""Anaconda is a distribution of Python geared toward data science. It includes a package manager, environment manager, and over 700 supplementary packages. Within the Ivy Secure Environment, Anaconda is available on both Linux and Windows VMs."" >}}
Basic Usage
For both Linux and Windows VMs, both Anaconda 2 and 3 are installed.
For Linux users in Ivy, run Python in one of two ways:
/opt/anaconda2/bin/python2.7   # Ver 2.7
/opt/anaconda3/bin/python3.5   # Ver 3.5

For Windows users in Ivy, 

Installing packages
A full mirror of the Anaconda package repository is available to Ivy users. To browse packages, see the Anaconda package list.
Packages can be installed using the conda utility that ships with Anaconda:
# General format
/opt/anaconda2/bin/conda install <package-name>

# Real examples
/opt/anaconda2/bin/conda install pyyaml
/opt/anaconda2/bin/conda install simplejson

To make this command simpler, depending upon the version of Anaconda you prefer, add an alias to your .bashrc file:
alias conda='/opt/anaconda2/bin/conda'  # For Anaconda2
alias conda='/opt/anaconda3/bin/conda'  # For Anaconda3

See below for more information about .bashrc.

Confusion about Python versions
The following versions of Python are available in Ivy's Linux VMs:
/usr/bin/python2.6
/usr/bin/python 
/usr/lib/python2.6 
/usr/lib64/python2.6 
/opt/anaconda2/bin/python2.7 
/opt/anaconda2/bin/python 
/opt/anaconda3/bin/python3.5 
/opt/anaconda3/bin/python 
/opt/anaconda3/bin/python3.5m

This means at least three versions of Python are available to you: 2.6, 2.7, and 3.5. If your code
has specific requirements for a Python version (usually there are differences in how you write against
2.x versus 3.x) find the version that suits your needs.
In order to run a Python script against a specific version, simply declare the path at the head of your
script with a ""shebang"" like this:
#!/opt/anaconda2/bin/python2.7

Then be sure to chmod 755 myscript.py to make it executable. Run your script with ./myscript.py and your it will 
execute against the specified version of Python.
In order to make a specific choice of Python more convenient when you are writing and executing code in the console, 
you may want to edit your ~/.bashrc file and add an alias like this. While you're there, set an alias for conda too:
alias python='/opt/anaconda2/bin/python2.7'   # set a new default ver of Python
alias conda='/opt/anaconda2/bin/conda'        # conda for Anaconda2

Finally, issue a source ~/.bashrc command to re-read your file to the session, and test with python -V. 
Subsequent logins will use the new value as well."
rc-website-fork/content/userinfo/reference/bioinformatics-pipelines.md,"+++
date = ""2017-02-22T15:28:51-05:00""
description = """"
title = ""Data Processing Trends in Genomics and Bioinformatics""
draft = true
tags = [""bioinformatics"",""genomics""]
categories = [""reference""]
images = [""""]
author = ""RC Staff""
type = ""reference""
+++
The fast pace of innovation, data generation, and collaboration in genomics and bioinformatics has necessitated in new data processing frameworks. This guide is aimed at introducing bioinformatics researchers to some of the latest innovations in pipeline development on desktops, AWS, and HPC systems. The number of tools available to researchers and the pros and cons of each can be somewhat daunting. This post is a minimalistic introduction that serves as a quick reference to the state-of-the-art. 
There are three major categories of data processing innovations happening in the bioinformatics community. One is to string together known tools and best practices in packages that can be run with little programming from the researcher. The other is to come up with scripting languages that can be used to develop a fully custom pipeline. Scripting languages provide advantages that range from determining failure points in the pipeline to automating resource management. The third is in a way a hybrid of the two, where the pipelines can be customized to an extent, but also don't need significant programming from the researcher.
As an important aside, a number of pipeline packages are being built around the StarCluster architecture. StarCluster is an open-source library that makes setting up computing clusters on AWS quite painless and efficient. The documentation is quite excellent with quick start guides and tutorials to get familiar with the architecture.  
Prebuilt Pipelines
Most of the Next-Gen analyses revolve around conducting analyses using the best practices codified over time. For RNA-Seq, ChIP-Seq, cancer variant calling, etc. bcbio-nextgen and Omics Pipe are two solutions that provide easy setup, automated analysis, and easy maintenance of a pipeline. The pipeline can be set to take advantage of cluster resources by running on the StarCluster. 
Scripting Languages
Packaged pipelines such as bcbio-nextgen and Omic Pipe are not fully customizable without significant coding. The standard solution has been to write scripts (such as using Shell) stringing programs together to develop a custom pipeline. Producing production quality pipelines through Shell scripting, again, requires significant coding, and hence, the need for new scripting frameworks. One such framework is bpipe which solves a few problems involved in traditional scripting, including pinpointing failures, cleaning up failed runs, and the difficulties associated with modifying a pipeline. One other development framework that attempts to simplify the process of writing pipelines for different computing architectures is BigDataScript. Scripts written in BigDataScripts can be used on a desktop, HPC cluster, or AWS without material modifications. This simplifies the task of running pipelines on multiple computing resources.
""Hybrid"" Pipelines
The poster child for hybrid pipelines is the Galaxy toolkit. Galaxy is a web-based interface that lets researchers combine genomic tools in a flexible graphical way without programming the dependencies and failure conditions themselves. The learning curve for Galaxy is short, and the toolkit covers a wide range of cases where no more programming would be required, but is not geared for researchers looking for a fully-customizable solution.
Further Resources
For an overview of pipeline development and scripting innovations, see here.
An exhaustive list of data processing pipelines being developed is available here."
rc-website-fork/content/userinfo/reference/ebs-storage.md,"+++
draft = true
object = """"
type = ""reference""
tags = [""aws"",""ebs"",""storage""]
author = ""RC Staff""
description = ""How to add an EBS storage volume to an AWS EC2 instance.""
categories = [""reference""]
images = [""""]
date = ""2017-02-17T11:15:51-05:00""
title = ""EBS Storage""
+++
How to add additional EBS storage to an existing EC2 instance. The following instructions can be performed in either the AWS web interface or the AWS command-line tools,
Create the EBS volume
Create the EBS volume of the appropriate size and type. Make sure you create it in the same availability zone as the instance you want to attach it to. For instance, if you have a Linux instance running in US-East-1c, then make sure to create your EBS volume in that zone.
Attach the EBS volume
Attach your new EBS volume to your EC2 instance. You will be asked for a mount point.
Format the EBS volume"
rc-website-fork/content/userinfo/reference/domino-data-lab.md,"+++
author = ""RC Staff""
description = """"
title = ""Domino Data Lab (Python/R)""
draft = true
date = ""2017-03-01T15:28:51-05:00""
tags = [""r"",""python"",""ddl"",""domino""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""DDL Home""
projecturl = ""https://www.dominodatalab.com/resources""
+++
{{< define ""Domino Data Lab"" ""DDL is blah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blahblah blah blah blah blah"" >}}
Overview

The DDL Environment

Packages

Jupyter / NNN Notebooks"
rc-website-fork/content/userinfo/reference/docker.md,"+++
author = ""RC Staff""
description = """"
title = ""Docker Containers""
draft = true
date = ""2017-03-01T15:28:51-05:00""
tags = [""docker"",""containers""]
categories = [""reference""]
images = [""""]
type = ""reference""
project = ""Docker""
projecturl = ""https://www.docker.com/""
+++
{{< define ""Docker"" ""Docker is a popular version of LXC (Linux Containers). Docker containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries ‚Äì anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in."" >}}
What is Docker?
{{< youtube PfTKwblbkpE >}}

Install Docker
Docker is available for Windows, Mac, and Linux. Download the appropriate Docker Edition for your platform directly from Docker. We suggest the CE ""Community Edition.""
Download Docker

Finding Containers
There are thousands of pre-built containers already available for common use cases. If you need a web server, a database instance, or portions of a genomics 
pipeline, there is probably a container ready for you to use. 
Here are some good places to search for containers:

Docker Hub
BioContainer
GitHub


Running Containers
If you have found a container you would like to try, download it (using the nginx web server as an example):
docker pull nginx

View a list of all container images you have pulled:
docker images

REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
whalesay            latest              188e03692c84        25 hours ago        277 MB
rocker/rstudio      latest              919e13c956b8        2 weeks ago         990 MB
nginx               latest              6b914bbcb89e        3 weeks ago         182 MB
hello-world         latest              48b5124b2768        2 months ago        1.84 kB
docker/whalesay     latest              6b362a9f73eb        22 months ago       247 MB

Run a container image:
docker run -d nginx

This runs the container as a daemon (service). But you may want to expose the container to a specific port locally, so that you can interact with it.
For example, if you wanted to expose nginx locally over port 80, enter this:
docker run -d -p 8080:80 nginx

The -p 8080:80 flag publishes your local computer's port 8080 with the container's port 80.
Another useful flag for runtime is a volume mapping, so that your running container can read or write to portions of your local computer's filesystem.
So, extending the earlier command:
docker run -d -p 8080:80 -v /User/local/dir:/var/www/html nginx

View all running containers:
docker ps -a

CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS                    NAMES
1d17f542be53        rocker/rstudio      ""/init""                  18 hours ago        Up 18 hours               0.0.0.0:8787->8787/tcp   elegant_banach

You can also run containers interactively (i.e. logging in) instead of running as a service. This allows you to explore the structure, features, or
configuration of a container, or modify how it works:
docker run -it nginx /bin/bash

This runs the container interactively (-i) in a pseudo-TTY (-t), and instantiates a shell for your session to use. Once you are done, simply exit the shell and you will leave the
container and return to your local computer's shell. If you have made any changes to the container, be sure to save it.


Building Containers
If you cannot find just the right container, you can always build your own. There are two ways to do this:

Download a container image, run it and log into it, and customize as if it were your own custom virtual machine. Then, save the container for later deployment. Instructions for interactively logging into a container can be found above.
Alternatively, you can write a custom Dockerfile and build the container from scratch, using docker build. More on Docker files and builds can be found at https://docs.docker.com/engine/getstarted/step_four/


Tutorials




Docker Training
Docker documents this process in great detail, and provides a step-by-step overview of their container system.
Launch


Katacoda Interactive Labs
Katacoda offers a free series of interactive trainings that build sequentially. The tutorials require you to
          engage with the Docker command-line as you progress.
Launch




{{< space >}}
{{< space >}}"
rc-website-fork/content/userinfo/reference/awscli-powershell.md,"+++
date = ""2017-02-21T13:43:32-05:00""
description = """"
title = ""AWS Command Line (PowerShell)""
draft = true
tags = [""cli"",""powershell"",""windows""]
categories = [""reference""]
images = [""""]
author = ""RC Staff""
type = ""reference""
project = ""AWS CLI for PowerShell""
projecturl = ""https://aws.amazon.com/powershell/""
+++

AWS Command Line for PowerShell
All the functionality of the Linux-based AWSCLI tools are also available for Windows users. 
Install the AWS Tools for Windows PowerShell if you need to script against AWS or automate repetitive tasks.

Installation
Download and run the AWS Tools for Windows PowerShell.
Requirements:

Microsoft Windows XP or later
Windows PowerShell 2.0 or later

Refer to http://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-set-up.html for more information.
Configure with Credentials
Your should either receive (from an admin) or generate an access key and secret key using the IAM service. These will serve to authenticate and 
authorize your command-line requests. To configure the AWS CLI for PowerShell to use these credentials, use the Set-AWSCredentials cmdlet:
PS C:\> Set-AWSCredentials -AccessKey {AKIAIOSFODNN7EXAMPLE} -SecretKey {wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY} -StoreAs {MyProfileName}


-AccessKey - Your access key.
-SecretKey - Your secret key.
-StoreAs - The profile name, which must be unique. Leave to default if you want to give credentials to a default profile.

You can then check your credentials (and profiles) issue this command:
PS C:\> Get-AWSCredentials -ListStoredCredentials

Basic Usage
The easiest way to begin using the AWS CLI for PowerShell is to begin to automate simple tasks, and then build up in complexity. This one-line command starts an EC2 instance:
PS C:\> Start-EC2Instance -InstanceIds i-1a2b3c4d5e6f

The following example loops through a log directory, finds files older than one week, and then archives any non-empty ones to Amazon S3 before deleting the log file:
foreach ($i in Get-ChildItem C:\Logs)
{
    if ($i.CreationTime -lt ($(Get-Date).AddDays(-7)))
    {
        if ($i.Length -gt 0)
        {
            Write-S3Object -BucketName mylogbucket -Key Logs/$i -File $i.FullName
        }
        Remove-Item $i.FullName
    }
}

Real-world Examples"
rc-website-fork/content/userinfo/reference/aws-cli.md,"+++
type = ""reference""
draft = true
object = ""awscli""
images = [""""]
author = ""RC Staff""
description = """"
title = ""AWS Command Line (Shell)""
date = ""2017-01-31T12:19:03-05:00""
tags = [""aws"",""cli"",""cloud"",""shell""]
categories = [""reference""]
project = ""AWS Command Line""
projecturl = ""https://aws.amazon.com/cli/""
+++

AWS Command-Line Interface
The AWS CLI is a command-line interface to the AWS service APIs. It allows users to interact programmatically with services such as EC2, S3, and others via bash and PowerShell scripts. 
This can be useful for local scripts or automated cron tasks. The AWS CLI also serves as a good introduction for programmatic API interaction using language-specific SKDs (Python, C#, PHP, Go, etc.)
Learn more at the AWS Command Line Interface page.


Install the AWSCLI
Mac/Linux
Using pip you can install the current release:
pip install awscli
Windows
Download and run the 64-bit or 32-bit Windows installer.

Configure With Credentials
Using the access key and secret access key generated for your account, enter those into the AWS CLI configuration. Use this command:
aws configure
When prompted, enter the appropriate region you are working in, such as us-east-1 and your preferred output format text | table | json.

Profiles
If you access AWS through numerous accounts, you can create multiple profiles. To do this, use the --profile myprofile parameter when configuring the AWS CLI, with a name you like (replacing myprofile.
Then to use a profile:
aws --profile mycoolprofile ec2 describe-instances


Basic Usage
The aws command is used, followed by the service name, and then the specific operation you want to call:
{{< gist nmagee 2f8426406a99c6cfd11e11d8e2aee11b >}}

Contextual Help
General help with listing services:
aws help
Find available commands specific to one service:
aws ec2 help
Specific parameters for a call within a service:
aws ec2 describe-instances help

Real-world example
Use a bash script to turn off your EC2 instance at night, and send you a notification. Use a similar script for a morning startup:
{{< gist nmagee 64bbe2b80fd90514b463032d01ba8d9f >}}

Combine with other CLI tools
See the jq page for more examples of how to make AWS CLI calls and parse the response JSON in meaningful ways."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/image-processing.md,"+++
categories = [
  ""userinfo"",
]
tags = [
  ""image processing"",
  ""ivy"",
  ""linux"",
  ""hipaa"",
  ""software""
]
draft = false
date = ""2018-01-31T14:18:18-05:00""
title = ""Image Processing Software on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
+++
Pre-approved packages
The following software packages are pre-approved for image processing on an Ivy Linux VM
KNIME
KNIME is open source analytics platform for data mining and pipelining. 
KNIME's Image Processing Plugin allows users to perform common image processing
techniques such as registration, segmentation, and feature extraction. KNIME is compatible with over 120 image file types and can be
used alongside ImageJ.
ImageJ
ImageJ is a Java-based image processing program developed at the NIH.
ImageJ can be used interactively through a graphical user interface or automatically with Java.
OpenCV
OpenCV is an open source library for computer vision applications.
OpenCV includes modules for image processing, video analysis, machine learning, and much more."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/complete-list.md,"+++
type = ""ivy""
date = ""2025-05-29T00:00:00-05:00""
tags = [
  ""ivy"",
  ""lmod"",
  ""software"",
  ""hpc""
]
draft = false
title = ""UVA High Security Zone Software List""
description = ""List of all linux software modules on the UVA High Security Zone""
author = ""RC Staff""
categories = [""userinfo""]
+++
{{< hsz-software moduleclasses=""all"" >}}"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/x2go.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/_index.md,"+++
title = ""Software on Ivy Linux Virtual Machines""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Linux"",
    ""Software"",
]
date = ""2020-01-19T17:45:12-05:00""
draft = false
layout = ""single""
+++
{{% callout %}}
Each Linux Virtual Machine (VM) comes with a set of preinstalled software applications.  Each VM can further be customized via installation of optional software packages.
{{% /callout %}}
An overview of available software packages for Windows VMs is provided here.
Preinstalled Software
{{< ivy-approved-sw-detailed os=""Linux"" installation=""preinstalled"" category=""all"" >}}


Optional Software
In addition to the preinstalled software, researchers may request installation of the following approved software packages for their Virtual Machine.
Request Ivy Software

Bioinformatics
{{< ivy-approved-sw-detailed os=""Linux"" installation=""optional"" category=""Bioinformatics"" >}}


Data Analysis
{{< ivy-approved-sw-detailed os=""Linux"" installation=""optional"" category=""Data Analysis"" >}}


Database Software
{{< ivy-approved-sw-detailed os=""Linux"" installation=""optional"" category=""Database Software"" >}}


Engineering
{{< ivy-approved-sw-detailed os=""Linux"" installation=""optional"" category=""Engineering"" >}}


Image Processing
{{< ivy-approved-sw-detailed os=""Linux"" installation=""optional"" category=""Image Processing"" >}}"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/image-processing.md,"+++
categories = [
  ""userinfo"",
]
tags = [
  ""image processing"",
  ""ivy"",
  ""hipaa"",
  ""windows"",
  ""software""
]
draft = false
date = ""2018-01-31T14:18:18-05:00""
title = ""Image Processing Software on Ivy Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
+++
Pre-approved packages
The following software packages are pre-approved for image processing on an Ivy Windows VM
Axiovision
Axiovision is software for microscopy image processing and analysis.
Axiovision is highly configurableto meet the needs of your individual workflows.
KNIME
KNIME is open source analytics platform for data mining and pipelining. 
KNIME's Image Processing Plugin allows users to perform common image
processing techniques such as registration, segmentation, and feature extraction. KNIME is compatible with over 120 image file types and can be used alongside ImageJ.
ImageJ
ImageJ is a Java-based image processing program developed at the NIH. 
ImageJ can be used interactively through a graphical user interface or automatically with Java.
OpenCV
OpenCV is an open source library for computer vision applications.
OpenCV includes modules for image processing, video analysis, machine learning, and much more."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/x2go.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/_index.md,"+++
title = ""Software on Ivy Windows Virtual Machines""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Windows"",
    ""Software"",
]
date = ""2020-01-19T17:45:12-05:00""
draft = false
layout = ""single""
+++
{{% callout %}}
Each Windows Virtual Machine (VM) comes with a set of preinstalled software applications.  Each VM can further be customized via installation of optional software packages.
{{% /callout %}}
An overview of available software packages for Linux VMs is provided here.
Preinstalled Software
{{< ivy-approved-sw-detailed os=""Windows"" installation=""preinstalled"" category=""all"" >}}


Optional Software
In addition to the preinstalled software, researchers may request installation of the following approved software packages for their Virtual Machine.
Request Ivy Software

Bioinformatics
{{< ivy-approved-sw-detailed os=""Windows"" installation=""optional"" category=""Bioinformatics"" >}}


Data Analysis
{{< ivy-approved-sw-detailed os=""Windows"" installation=""optional"" category=""Data Analysis"" >}}


Database Software
{{< ivy-approved-sw-detailed os=""Windows"" installation=""optional"" category=""Database Software"" >}}


Engineering
{{< ivy-approved-sw-detailed os=""Windows"" installation=""optional"" category=""Engineering"" >}}


Image Processing
{{< ivy-approved-sw-detailed os=""Windows"" installation=""optional"" category=""Image Processing"" >}}"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/database/mariadb.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/database/mysql.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/database/sw-list.md,"+++
title = ""Database Software on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""linux"", 
    ""Ivy"",
    ""Databases""
]
draft = false
date = ""2018-02-01T17:45:12-05:00""
+++
The following database software are available on the Ivy Linux Virtual Machines
MySQL
Is the most popular open-source relational database, used in academia and industry worldwide. It has been
in use for over 20 years and is backed by a large developer community. It is available in both free and 
proprietary versions. 
MariaDB
MariaDB is a community developed version of MySQL, and is highly compatible with MySQL and other relational databases. Existing databases can be easily migrated between MySQL and MariaDB, and vice versa.
PostgreSQL
Unlike MariaDB and MySQL, PostgreSQL is an object relational database, and can be used in a manner similar to other relational databases. It is available for free, and is open-source under its own special license type, called PostgreSQL License. 
MongoDB
MongoDB is a NoSQL, Document store database. It treats all data as key-value pair documents, stored within collections, instead of tables. MongoDB is great for large scale data processing needs."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/database/postgresql.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/database/mongodb.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/programming/openjdk-8.md,"+++
title = ""Preinstalled Java SDK on Ivy Linux VM""
description = """"
date = ""2018-01-29T15:45:12-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""java"", 
    ""linux"",
    ""software""
]
draft = false
+++
Java SDK Overview
Ivy Linux VMs are installed with Java SDK 1.8. Java is a popular Object-Oriented programming
language and is used in a multitude of scenarios. It is available under the GNU General Public
License for all users. The SDK consists of a large number of tools such as javac that 
help in application development. 
Running Java commands from the Command Line
Open a Command Line Terminal and enter java followed by the desired command. E.g. to find
the version of the SDK
java -version

Running your code
To compile java code, first cd to the location of your .java file and then do
javac <your_class_name>.java

After the java compiler runs and gives no errors, a .class file would be created. Run the following command to see the output:
java <your_class_name>

Important Note
While Ivy VMs have full support for the Java SDK, certain aspects of programming full-scale Java on Ivy may not work without running into issues.
In order to execute Java programs correctly, please load the entire package into the VM's storage instead of compiling it on the VM. 
More Information
Please visit the official Oracle documentation to learn more about Java at this web address."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/programming/perl.md,"+++
title = ""Preinstalled Perl on Ivy Linux VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-01-29T15:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""perl"", 
    ""linux""
]
draft = false
+++
Perl
Our VMs have Perl 5.16.3 available as part of the base linux 
VM. Licensed as open source under the GPL, it is most often used 
to develop mission critical software, and has excellent integration
with markup languages such as HTML, XML, amongst others. Since it is both Object-Oriented and procedural, it could be used within a multitude
of programming projects. It includes built in database integration via
its DBI module. Other than DBI, it has thousands of modules, making it
one of the most extensible languages. Due to its interpreted nature, 
Perl is similar to Python and would be easy to understand for those 
familiar with Python.
Running Perl code
Perl has an interactive interpreter, which could be run by simply typing
perl -e <perl_code_goes here>. E.g. to print a number:
    perl -e 'print 10'
the -e flag is simply to denote that the code is not a file, but code
itself. To run a Perl script, do the following:
    perl 
Installing modules
Since Ivy VM's do not allow outward connections to CPAN's website, you would have to
install perl modules using the procedure below:

Check if CPAN is installed and configured on your VM by typing cpan into a terminal
window:
    cpan
If it asks you if CPAN needs to be configured, type yes
Once it is configured, type cpan to enter the CPAN shell
In a browser from outside the Ivy VM, search for the proper name of the Perl module you wish to download
    search.cpan.org
E.g. if you want to install the MySQL driver for Perl, type 
    install DBD::MySQL
This would start the installation of the module. Ivy is able to download modules from CPAN using this method. 

* You could manually install a module from its compressed file, once you have transferred the file into Ivy. However, using the process above downloads the modules' dependencies as well. 
Verifying if a module is installed
Run the following command after installing your module :
perldoc -l DBD::mysql

(e.g. if you installed DBD::mysql). It should output the path to the installed module. 
More Information
Please visit the official Perl website for more details."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/rodeo.md,"+++
title = ""Preinstalled Rodeo on Ivy Linux VM""
date = ""2018-01-29T15:45:12-05:00""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""rodeo"", 
    ""Ivy"",
    ""linux""
]
draft = false
+++
Rodeo Overview
Our Linux VMs are installed with Rodeo version 2.5, as of the last update. Rodeo is a lightweight, Python based, IDE for data science.
It has a very streamlined code-to-plot workflow, with easily extensible packages that make it simple to 
analyze difficult patterns in data. It includes many data analysis features under one roof, and adopts features from 
iPython Notebook (it actually runs atop the iPython kernel). Like most Python projects, 
it is open source and available for free. 
Launching Rodeo
You can launch Rodeo from the Applications menu. It is a self-contained IDE that would not require any knowledge of the command line.
Rodeo can be used in the same manner as any other Python IDE such as iPython Notebook or Jupyter Notebook. 
Basic Rodeo Usage
It is important to understand that all Python code, such as lists, Dataframes, etc. are saved to the 
Environment. We then use the Environment tab to view our data. 
E.g. if you create a Dataframe
df = pd.DataFrame(np.random.rand(50,3),columns=['col1','col2','col3'])

You can then open the Environment tab to view this in tabular form. 
Command History in Rodeo
All commands can be viewed under the History tab
More Information
For more information, please visit the official [Rodeo website] (https://github.com/yhat/rodeo)"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/stata.md,"+++
title = ""User Licensed Stata on Ivy Linux VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T00:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Linux"",
    ""Software"",
    ""Data Analysis""
]
draft = false
+++
Stata Overview
Stata is a graphical data analysis tool developed by StataCorp, and is short for Statistics and Data. It 
is used in various disciplines, including biomedicine, economics, epidemiology, among others. It is capable
of performing statistical analysis, simulations, regression, and data management. Besides the standard version
Stata also ships with the MP version (multi=processing), and SE for large databases. 
{{% callout %}}
Users requesting an installation of Stata are required to provide their own license. Please consult with us before
requesting an installation. 
{{% /callout %}}
You may also request a Stata license from the UVa Software Gateway
Installing programs from SSC
Please first run the following commands to use the proxy:
set httpproxy on
set httpproxyhost ""figgis-s.hpc.virginia.edu""
set httpproxyport 8080
You can now install new packages with the ssc install command."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/sw-list.md,"+++
title = ""Data Analysis Packages on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
  ""linux"", 
  ""Ivy"",
  ""Data Analysis"",
  ""matlab""
]
draft = false
date = ""2018-02-01T17:45:12-05:00""
+++
Available Packages
The following Data Analysis packages are available on the Ivy Linux Virtual Machines
MATLAB
MATrix LABoratory (MATLAB for short) is a software designed for quick scientific calculations, such as matrix manipulation, plotting, and others.
It has hundreds of built-in functions for a wide variety of computations and several tools designed for specific 
research disciplines, including statistics and partial differential equations.
* Limited licenses available, for more information on MATLAB and licensing, please click [here] (/userinfo/ivy/ivy-linux-sw/data-analysis/matlab)
SAS
SAS is large platform independent software with multiple components, and is used for statistical analysis, data ETL operations, as well as several other
reporting problems. 
* Limited licenses available, for more information on SAS and licensing, please click here
Stata
Stata is a large GUI based data analysis software package. It is great for statistical analysis in a broad spectrum of research disciplines. 
* Requires user to bring their own license(s). For more information on Stata, please click here
IDL
IDL is a programming language used widely in the areas of medicine, physics, and astronomy. Image processing can be performed easily on it. 
Users can use it on the VM using its command line based
interface. 
* Limited licenses available, for more information IDL and licensing, please click here
Apache cTAKES
The clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open source system to extract information from clinical health records. 
It uses natural language processing to extract information about the patient, drugs, symptoms, procedures, etc. It is an efficient clinical analysis
tool, and was specifically written for the clinical domain. 
* cTAKES requires pre-installation approval. Please get in touch with us regarding your requirements before making a VM request.
For information about cTAKES, please click here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/idl.md,"+++
title = ""UVa Licensed IDL on Ivy Linux VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T00:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Linux"",
    ""Software"",
    ""Data Analysis""
]
draft = false
+++
IDL Overview
IDL, short for Interactive Data Language, is an interactive shell based data analysis programming language. Used vastly in medical imaging, it can quickly create visualizations and graphs 
of large data sets in a few easy steps due to its vector nature. FORTRAN users would be familiar with the IDL syntax. IDL is not to be confused with
Java IDL or Microsoft IDL. 
Basic IDL Usage
To start IDL, open a terminal window and type idl. This will start the interactive shell. 
Variables in IDL
To initialize variables in IDL, type:
<variable_name> = <variable_value>

e.g. 
x = 3

and hit Enter/Return
To print the variable, type 
print, x

Arrays in IDL
To make arrays in IDL, follow the format below:
<array_name> = [val1, val2, val3,...,valx]

e.g. 
myarray = [1,2,3,4]

Licensing
We have a limited number of IDL Licenses available, which are provided on a first-come-first-serve basis. 
As a consequence, availability of IDL on your VM is not always guaranteed once a VM request is submitted.
Please consult with us before requesting IDL. 
More Information
For more Information on IDL, please consult the IDL documentation on its [official website] (https://www.nv5geospatialsoftware.com/Products/IDL#language)"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/r.md,"+++
title = ""Preinstalled R on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Software"",
    ""R""
]
date = ""2018-01-19T17:45:12-05:00""
draft = false
+++
R Overview
R is an open source programming language, used by Data Miners, Scientists, Data Analysts, 
and Statisticians. It is available under the GNU GPL V2 license from the Comprehensive R 
Archive Network
R can be used for many statistical, modeling, and graphical solutions. It is very Object-Oriented in nature and is 
easily extensible. 
Running the command line R console
Type R at the terminal to launch the R console. 
Installing packages
Our Linux VMs come equipped with R preinstalled. Most major R packages are also installed
and further could be installed from CRAN using (from within the R console)
install.packages(""your_package_name"")

Once the package is loaded, you could start it using
library(your_package_name)

Running R Scripts from the command line
Simply type in the following 
Rscript /path/to/script/your_script_file.R

More Information
For more information, please visit the official R website"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/anaconda.md,"+++
title = ""Preinstalled Python 2 and Python 3 with Anaconda on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""linux"",
    ""Software"",
]
date = ""2018-01-19T17:45:12-05:00""
draft = false
+++
Anaconda
Our VMs have Python 2 and 3 available as part of the Anaconda distribution. Please refer to this page for more information."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/sas.md,"+++
title = ""UVa Licensed SAS on the Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2018-02-01T00:45:12-05:00""
tags = [
    ""ivy"", 
    ""software"",
    ""data analysis"",
    ""linux""
]
draft = false
+++
SAS Overview
SAS is a command-driven software package used for statistical analysis
and data visualization. It is available in .
It is one of the most widely used statistical software packages in both industry and academia.
You may use it if you have a large number of statistical algorithms. It is not limited to an industry,
and could be used in both scientific and non-scientific contexts. We only offer the Teaching & Research version
at the moment. 
Common Usage
For this example we will use a common scenario from SAS Clinical Standards Toolkit, which is used for supporting clinical
research activities. The SAS Clinical Standards Toolkit initially focuses on standards defined by the Clinical Data 
Interchange Standards Consortium (CDISC). CDISC is a global, open, multidisciplinary, nonprofit organization that 
has established standards to support the acquisition, exchange, submission, and archival of clinical research data and metadata. 
Starting SAS
Open a terminal window and type sas.
Getting a list of all installed standards
%cst_getregisteredstandards(
_cstOutputDS=work.regStds
);

Creating Data Sets Used by the Framework
%cst_createdsfromtemplate(
_cstStandard=CST-FRAMEWORK,
_cstType=control,
_cstSubType=reference,
_cstOutputDS=work.sasrefs
);

Licensing
We have a limited number of SAS Licenses available, which are provided on a first-come-first-serve basis. 
As a consequence, availability of SAS on your VM is not always guaranteed once a VM request is submitted.
Please consult with us before requesting SAS. 
More information
For more information on SAS, please consult its official documentation here
You may obtain the Administration version of SAS from the UVA Software Gateway portal here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/ctakes.md,"+++
title = ""cTAKES on Ivy Linux VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T00:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Linux"",
    ""Software"",
    ""Data Analysis""
]
draft = false
+++
cTAKES Overview
cTAKES or The clinical Text Analysis and Knowledge Extraction System, is a Mayo Clinic developed Natural Language Processing (NLP) tool used to 
extract information out of clinical records. It is open-source, and built on the Apache Unstructured Information Management Architecture. cTAKES
is modular, expandable, for a number of generic use cases, and contains excellent best practice notes. 
cTAKES Usage
cTAKES components
Some of cTAKES components are listed below:

Sentence boundary detection (OpenNLP technology)
*Tokenization (rule-based)
Morphologic normalization (NLM‚Äôs LVG)
POS tagging (OpenNLP technology)
Shallow parsing (OpenNLP technology)
Named Entity Recognition
Negation and context identification (both based on NegEx)

cTAKES Named Entities
cTAKES contains the following Named Entities:

Drug mentions
Disease/disorder mentions
Sign/symptom mentions
Anatomical site mentions
Smoking status classifier
Detailed drug mention annotator
Peripheral Artery Disease (PAD) annotator
Dependency parser

cTAKES Installation
Please note that cTAKES requires pre-installation approval by us. Please consult us prior to requesting a new VM regarding your cTAKES requirements. 
More Information
For more information on cTAKES, and how to use it, please visit the following links:

Official Website: http://ctakes.apache.org/index.html
Documentation: https://cwiki.apache.org/confluence/display/CTAKES/cTAKES
cTAKES scientific paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2995668/
"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/data-analysis/matlab.md,"+++
title = ""UVa Licensed MATLAB on Ivy Linux Virtual Machines""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2018-02-01T00:45:12-05:00""
tags = [
    ""Ivy"",
    ""Software"",
    ""Data Analysis"",
    ""Linux""
]
draft = false
+++
MATLAB Overview
MATLAB is a high-performance language for technical computing. It integrates
computation, visualization, and programming environment. MATLAB stands for MATrix LABoratory. MATLAB was made
to provide easy access to matrix software developed by the LINPACK (linear system package)
and EISPACK (Eigen system package) projects. MATLAB includes a programming language
environment with built-in editing and debugging tools, and supports object-oriented programming.
Programming in MATLAB
MATLAB has many advantages compared to conventional computer languages (e.g.,
C, FORTRAN) for solving technical problems. MATLAB is an interactive system whose
basic data element is an array, and almost all problems can be solved in MATLAB using that
one data element.
Starting MATLAB
To start MATLAB, open a terminal window and type matlab
Since MATLAB is a large software, it may take time to load up. When it starts, the first screen to appear
would be the MATLAB Desktop, with several windows within it. These might include the ribbons (Home, Plot, and App), the Command Window,
the Current Folder window, and the Workspace window.
Basic Arithmetic
The following are basic arithmetic operators in MARLAB
    * (Multiplication), + (Addition), - (Subtraction), / (Division)
In order to perform a simple calculation click anywhere on the Command Window and type
1+2+3

and hit Enter/Return. the result shows up on the Command Window as Ans = 6
Variables
To create a variable, type the name of the variable, followed by the equals sign (assignment operator) and the value. Hit Enter/Return to store the value.
x = 12

If you type x again on the Command Window, you would see the value of the variable displayed. Variables could be used in any number of arithmetic
calculations.
Arrays
A very basic plot can be drawn using MATLAB arrays. To create an array in MATLAB, use the square brackets:
x = [1 2 3 4 5 6]
y = [4 -4 7 3 7 1]

Notice how the array elements do not have commas between them. These arrays could be used like any other variable.
Licensing
University of Virginia has recently upgraded the Matlab license so that Matlab is available to everyone at UVa. There is one version of Matlab for students, faculty, and staff. MATLAB is available on the Windows, Mac OSX, and Linux platforms. To get started go to the UVa Software Gateway Matlab can be found under the Data Analysis & Research grouping.
The Campus Wide License configuration now includes all 100+ MathWorks products. To better reflect the new configuration they have re-named the license option to Campus Wide Suite. For further information see the URL https://data.library.virginia.edu/research-software/matlab/
More Information
For help, type help in the Command Window or click on the Help button on the HOME ribbon. For more Information on MATLAB itself, please
visit the official website here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/sw-list.md,"+++
title = ""Bioinformatics Packages on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""linux"", 
    ""ivy"",
    ""bioinformatics""
]
draft = false
date = ""2018-02-01T17:45:12-05:00""
+++
Available Packages
The following bioinformatics packages are available on the Ivy Linux Virtual Machines
Bowtie2
Bowtie2 is a memory-efficient tool for aligning short sequences to long reference genomes.
For bowtie2 usage information, please click [here] (/userinfo/ivy/ivy-linux-sw/bioinformatics/bowtie2)
HISAT2
HISAT2 is a fast and sensitive tool for aligning short reads against the general human population 
(as well as single reference genome)
* Requires approval before installation 
For HISAT2 usage information, please click here"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/cufflinks.md,"+++
title = ""Cufflinks on Ivy Linux VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""bioinformatics"", 
    ""linux"",
    ""software""
]
date = ""2018-02-01T00:45:12-05:00""
draft = true
+++
Cufflinks Overview
The Cufflinks suite of tools can be used to perform a number of different types of analyses
for RNA-Seq experiments. The Cufflinks suite includes a number of different programs that work
together to perform these analyses. The complete workflow, performing all the types of analyses
Cufflinks can execute, are summarized below."
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/bowtie2.md,"+++
title = ""Bowtie2 on Ivy Linux VM""
description = """"
date = ""2018-02-01T17:45:12-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""linux"", 
    ""bioinformatics"",
    ""software""
]
draft = false
+++
Bowtie2 is a memory-efficient tool for aligning short sequences to long reference genomes.
It indexes the genome using FM Index, which is based on Burrows-Wheeler Transform algorithm,
to keep its memory footprint small. Bowtie2 supports gapped, local and paired-end alignment modes.
Alignment to a known reference using Bowtie2 is often an essential first step in a myriad of NGS analyses workflows. 
Bowtie2 Usage
Alignment using bowtie2 is a 2-step process - indexing the reference genome, followed by aligning the sequence data.


Create indexes of your reference genome of interest stored in reference.fasta file:
bowtie2-build [option(s)] <reference.fasta> <bt2-index-basename>

This will create new files with the provided basename and extensions .1.bt2, .2.bt2, .3.bt2 and 
.4.bt2, .rev.1.bt2 and .rev.2.bt2. 
These files constitute the index.


Align paired-end reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores:
bowtie2 -x <bt2-index-basename> -1 <sampleR1.fq> -2 \

<sampleR2.fq> -p <N> -S <output.sam>



The alignment results in SAM format are written to the file output.sam
More Information
For more information, please refer to the Bowtie2 manual.
Citation:
If you use bowtie2 for your work, please cite:
Langmead B, Salzberg S. Fast gapped-read alignment with Bowtie 2. Nature Methods. 2012, 9:357-359"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/samtools.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/hisat2.md,"+++
title = ""HISAT2 on Ivy Linux VM""
description = """"
date = ""2018-02-01T17:45:12-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""linux"", 
    ""bioinformatics"",
    ""software""
]
draft = false
+++
* Please note that HISAT2 requires approval prior to installation on the VM
HISAT2 is a fast and sensitive tool for aligning short reads against the general human population 
(as well as single reference genome). It indexes the genome using a Hierarchical Graph FM Index 
(HGFM) strategy, i.e. a large set of small indexes that collectively cover the whole genome
 (each index representing a genomic region of 56 Kbp).
HISAT2 Usage:
Alignment using HISAT2 is a 2-step process - indexing the reference genome, followed by aligning the sequence data.


Create indexes of your reference genome of interest stored in reference.fasta file: 
        hisat2-build [option(s)]  
    This will create new files with the provided basename and extensions *.ht2. These files constitute the index.


Align paired-end reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores:
        hisat2 -x  -1  -2  -p  -S 
    The alignment results in SAM format are written to the file output.sam


Note on using the --sra-acc option
Since Ivy VM‚Äôs do not allow outside connections, --sra-acc option will not work with HISAT2. 
If users plan to use SRA data, they will have to download it and move into Ivy prior to alignment.
Please refer to our Globus user guide for help on how to do that.  
More information
For detailed information, please refer to HISAT2 manual.
Citation:
If you use HISAT2 for your work, please cite:
Kim D, Langmead B and Salzberg SL. HISAT: a fast spliced aligner with low memory requirements. Nature Methods 2015"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/bioinformatics/sicer.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/image-processing/imagej.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-linux-sw/productivity/libreoffice.md,"+++
title = ""LibreOffice On Ivy Linux VM""
date = ""2018-01-29T15:45:12-05:00""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""libre"", 
    ""linux"",
    ""software""
]
draft = false
+++
LibreOffice Overview
Our Linux VMs come prepackaged with the open source alternative to Microsoft Office(R), called LibreOffice. 
As of last writing, version 5 is installed, including the specific software suites mentioned below.
LibreOffice is compatible with all Microsoft Office formats, and can be connected to services like 
Google Drive or DropBox. It is available under the Mozilla Public License. LibreOffice is full GUI 
software and would require you to RDP into your VM or use a graphical tool such as FastX in order to 
render it. 
LibreOffice Writer
LibreOffice Writer is the word processor component of LibreOffice. It can save documents in .Doc, .Docx, .PDF, etc.
LibreOffice Calc
LibreOffice Calc is the spreadsheet component of LibreOffice. It can save spreadsheets in CSV, XLS, and PDF formats. 
LibreOffice Impress
LibreOffice Impress is the presentation maker component of LibreOffice. It can save presentations in PPT, PPTX, and SWF formats. 
LibreOffice Draw
LibreOffice Draw is the vector graphics component of LibreOffice. It can save files in various graphical formats such as SVG and others. 
LibreOffice Base
LibreOffice Base is the database component of LibreOffice. It uses the HSQLDB as its storage engine, and can be used as a front end for 
larger databases systems such as MySQL and MariaDB. 
More Details
For more details, please visit LibreOffice's [official website] (https://www.libreoffice.org/)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/database/mariadb.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/database/mysql.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/database/postgresql.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/database/sql-server.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/database/mongodb.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/strawberry-perl.md,"+++
title = ""Preinstalled Strawberry Perl on Ivy Windows VM""
description = """"
author = ""RC Staff""
date = ""2023-02-23T15:45:12-05:00""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Perl"", 
    ""windows"",
    ""ivy""
]
draft = false
+++
Perl
Our VMs have Strawberry Perl 5.24 available as part of the Windows 
VM, as of the last writing. Licensed as open source under the GPL, it is most often used 
to develop mission critical software, and has excellent integration
with markup languages such as HTML, XML, amongst others. Since it is both Object-Oriented and procedural, it could be used within a multitude
of programming projects. It includes built in database integration via
its DBI module. Other than DBI, it has thousands of modules, making it
one of the most extensible languages. Due to its interpreted nature, 
Perl is similar to Python and would be easy to understand for those 
familiar with Python.
Running Perl code
Strawberry Perl has an interactive interpreter, available under Start>
All Programs> Strawberry Perl. Perl commands could be executed by simply typing
perl -e <perl_code_goes here>. E.g. to print a number:
    perl -e 'print 10'
the -e flag is simply to denote that the code is not a file, but code
itself. To run a Perl script, do the following:
    perl 
Installing modules
Since Ivy VM's do not allow outward connections to CPAN's website, you would have to
install perl modules using the procedure below:

Check if CPAN is installed and configured on your VM by typing cpan into a terminal
window:
    cpan
If it asks you if CPAN needs to be configured, type yes
Once it is configured, type cpan to enter the CPAN shell
In a browser from outside the Ivy VM, search for the proper name of the Perl module you wish to download
    search.cpan.org
E.g. if you want to install the MySQL driver for Perl, type 
    install DBD::MySQL
This would start the installation of the module. Ivy is able to download modules from CPAN using this method. 

* You could manually install a module from its compressed file, once you have transferred the file 
into Ivy. However, using the process above downloads the module's dependencies as well. 
Verifying if a module is installed
Run the following command after installing your module :
perldoc -l DBD::mysql

(e.g. if you installed DBD::mysql)
It should output a path to the installed module. 
More Information
For more information on Strawberry Perl, please visit the official website."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/openjdk-8.md,"+++
title = ""Preinstalled Java SDK on Ivy Windows VM""
description = """"
author = ""RC Staff""
date = ""2018-01-29T15:45:12-05:00""
images = [""""]
categories = [""userinfo""]
tags = [
    ""ivy"", 
    ""windows""
]
draft = false
+++
Java SDK Overview
Ivy Windows VMs are installed with Java SDK 1.8. Java is a popular Object-Oriented programming
language and is used in a multitude of scenarios. It is available under the GNU General Public
License for all users. The SDK consists of a large number of tools such as javac that 
help in application development. 
Running Java commands from the Command Prompt
Open a Windows Command Prompt and enter java followed by the desired command. E.g. to find
the version of the SDK
java -version

Running your code
To compile java code, first cd to the location of your .java file and then do
javac <your_class_name>.java

After the java compiler runs and gives no errors, a .class file would be created. Run the following command to see the output:
java <your_class_name>

Important Note
While Ivy VMs have full support for the Java SDK, certain aspects of programming full-scale Java on Ivy may not work without running into issues.
In order to execute Java programs correctly, please load the entire package into the VM's storage instead of compiling it on the VM. 
More Information
Please visit the official Oracle documentation to learn more about Java at this web address."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/programming/perl.md,"+++
title = ""Perl""
date = ""2022-02-14T11:45:12-05:00""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
  ""code"", 
  ""perl"",
  ""vm"",
  ""ivy""
]
draft = false
+++
Perl
Our VMs have Perl 5.16.3 available as part of the base linux 
VM. Licensed as open source under the GPL, it is most often used 
to develop mission critical software, and has excellent integration
with markup languages such as HTML, XML, amongst others. Since it is both Object-Oriented and procedural, it could be used within a multitude
of programming projects. It includes built in database integration via
its DBI module. Other than DBI, it has thousands of modules, making it
one of the most extensible languages. Due to its interpreted nature, 
Perl is similar to Python and would be easy to understand for those 
familiar with Python.
Running Perl code
Perl has an interactive interpreter, which could be run by simply typing
perl -e <perl_code_goes here>. E.g. to print a number:
    perl -e 'print 10'
the -e flag is simply to denote that the code is not a file, but code
itself. To run a Perl script, do the following:
    perl 
Installing modules
Since Ivy VM's do not allow outward connections to CPAN's website, you would have to
install perl modules using the procedure below:

Check if CPAN is installed and configured on your VM by typing cpan into a terminal
window:
    cpan
If it asks you if CPAN needs to be configured, type yes
Once it is configured, type cpan to enter the CPAN shell
In a browser from outside the Ivy VM, search for the proper name of the Perl module you wish to download
    search.cpan.org
E.g. if you want to install the MySQL driver for Perl, type 
    install DBD::MySQL
This would start the installation of the module. Ivy is able to download modules from CPAN using this method. 

NB: You could manually install a module from its compressed file, once you have transferred the file 
into Ivy. However, using the process above downloads the modules' dependencies as well."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/rodeo.md,"+++
title = ""Preinstalled Rodeo On Ivy Windows VM""
date = ""2018-01-29T15:45:12-05:00""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""rodeo"", 
    ""windows"",
    ""ivy""
]
draft = false
+++
Rodeo Overview
Our Windows VMs are installed with Rodeo version 1.3, as of the last update. Rodeo is a lightweight, Python based, IDE for data science.
It has a very streamlined code-to-plot workflow, with easily extensible packages that make it simple to 
analyze difficult patterns in data. It includes many data analysis features under one roof, and adopts features from 
iPython Notebook (it actually runs atop the iPython kernel). Like most Python projects, 
it is open source and available for free. 
Launching Rodeo
You can launch Rodeo from the Start menu. It is a self-contained IDE that would not require any knowledge of the command line.
Rodeo can be used in the same manner as any other Python IDE such as iPython Notebook or Jupyter Notebook. 
Basic Rodeo Usage
It is important to understand that all Python code, such as lists, Dataframes, etc. are saved to the 
Environment. We then use the Environment tab to view our data. 
E.g. if you create a Dataframe 
df = pd.DataFrame(np.random.rand(50,3),columns=['col1','col2','col3'])

You can then open the Environment tab to view this in tabular form. 
Command History in Rodeo
All commands can be viewed under the History tab
More Information
For more information, please visit the official [Rodeo website] (https://github.com/yhat/rodeo)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/stata.md,"+++
title = ""User Licensed Stata on Ivy Windows VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T00:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Windows"",
    ""Software"",
    ""Data Analysis""
]
draft = false
+++
Stata Overview
Stata is a graphical data analysis tool developed by StataCorp, and is short for Statistics and Data. It 
is used in various disciplines, including biomedicine, economics, epidemiology, among others. It is capable
of performing statistical analysis, simulations, regression, and data management. Besides the standard version
Stata also ships with the MP version (multi=processing), and SE for large databases. 
{{% callout %}}
Users requesting an installation of Stata are required to provide their own license. Please consult with us before
requesting an installation. 
{{% /callout %}}
You may also request a Stata license from the UVa Software Gateway
Installing programs from SSC
Please first run the following commands to use the proxy:
set httpproxy on
set httpproxyhost ""figgis-s.hpc.virginia.edu""
set httpproxyport 8080
You can now install new packages with the ssc install command."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/spss.md,"+++
title = ""UVa Licensed SPSS on Ivy Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T17:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Data Analysis"",
    ""Windows"",
    ""Software""
]
draft = false
+++
SPSS Overview
SPSS (or Statistical Package for Social Sciences), was initially developed as a social survey project but later on has grown to encompass statistical
applications in almost all disciplines. Different industries use SPSS for their data analysis work. Its features include database management, reporting,
graphing, among many others. 
SPSS Usage
SPSS is available only on the Windows VM at the moment. To run SPSS go to:
Start Menu > All Programs > IBM SPSS Statistics

Licensing
We have a limited number of SPSS licenses available, which are provided on a first-come-first-serve basis. 
As a consequence, availability of SPSS on your VM is not always guaranteed once a VM request is submitted.
Please consult with us before requesting SPSS.  
More Information
For detailed documentation on SPSS, please visit the official website here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/sw-list.md,"+++
title = ""Data Analysis Packages on Ivy Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Windows"", 
    ""Ivy"",
    ""Data Analysis""
]
draft = false
date = ""2018-02-01T17:45:12-05:00""
+++
Available Packages
The following Data Analysis packages are available on the Ivy Windows Virtual Machines
MATLAB
MATrix LABoratory (MATLAB for short) is a software designed for quick scientific calculations, such as matrix manipulation, plotting, and others.
It has hundreds of built-in functions for a wide variety of computations and several tools designed for specific 
research disciplines, including statistics and partial differential equations.
* Limited licenses available, for more information on MATLAB and licensing, please click [here] (/userinfo/ivy/ivy-windows-sw/data-analysis/matlab)
SAS
SAS is large platform independent software with multiple components, and is used for statistical analysis, data ETL operations, as well as several other
reporting problems. 
* Limited licenses available, for more information on SAS and licensing, please click here 
Stata
Stata is a large GUI based data analysis software package. It is great for statistical analysis in a broad spectrum of research disciplines. 
* Requires user to bring their own license(s). For more information on Stata, please click here
IDL
IDL is a programming language used widely in the areas of medicine, physics, and astronomy. Users can use it on the VM using its command line based
interface. x
* Limited licenses available, for more information on IDL and licensing, please click here
Apache cTAKES
The clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open source system to extract information from clinical health records. 
It uses natural language processing to extract information about the patient, drugs, symptoms, procedures, etc. It is an efficient clinical analysis
tool, and was specifically written for the clinical domain. 
* cTAKES requires pre-installation approval. Please get in touch with us regarding your requirements before making a VM request. 
For information about cTAKES, please click here 
SPSS
SPSS is a large scale, yet easy to use data analysis packages. It has been widely used since the last few decades and is considered one of the world's
de facto data analysis and reporting tools. 
* Limited licenses available, for more information on SPSS and licensing, please click here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/idl.md,"+++
title = ""UVa Licensed IDL on Ivy Windows VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T00:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Windows"",
    ""Software"",
    ""Data Analysis""
]
draft = false
+++
IDL Overview
IDL, short for Interactive Data Language, is an interactive shell based data analysis programming language. Used vastly in medical imaging, it can quickly create visualizations and graphs 
of large data sets in a few easy steps due to its vector nature. FORTRAN users would be familiar with the IDL syntax. IDL is not to be confused with
Java IDL or Microsoft IDL. 
Licensing
We have a limited number of IDL Licenses available, which are provided on a first-come-first-serve basis. 
As a consequence, availability of IDL on your VM is not always guaranteed once a VM request is submitted.
Please consult with us before requesting IDL. 
More Information
For more Information on IDL, please consult the IDL documentation on its [official website] (https://www.nv5geospatialsoftware.com/Products/IDL#language)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/r.md,"+++
title = ""Preinstalled R on Ivy Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-01-19T17:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Windows"",
    ""R""
]
draft = false
+++
R Overview
R is an open source programming language, used by Data Miners, Scientists, Data Analysts, 
and Statisticians. It is available under the GNU GPL V2 license from the Comprehensive R 
Archive Network
R can be used for many statistical, modeling, and graphical solutions. It is very Object-Oriented in nature and is 
easily extensible. 
Running Rstudio from the desktop
You can start R in a Graphical interface using the RStudio application from the desktop
Running the command line R console
Type R at the command prompt to launch the R console. 
Installing packages
Our Windows VMs come equipped with R preinstalled. Most major R packages are also installed
and further could be installed from CRAN using (from within the R console) 
install.packages(""your_package_name"")

Once the package is loaded, you could start it using
library(your_package_name)

Running R Scripts from the command line
Simply type the following on Command Prompt 
Rscript path\\to\\script\\your_script_file.R

More Information
For more information, please visit the official R Studio website."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/anaconda.md,"+++
title = ""Preinstalled Python 2 and Python 3 with Anaconda on Ivy Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-01-18T17:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Windows"",
    ""software""
]
draft = false
+++
Anaconda
Our VMs have python 2 and 3 available as part of the Anaconda 
distribution. Anaconda comes installed with many packages best suited 
for scientific computing, data processing, and data analysis, while making deployment
very simple. Its package manager conda installs and updates python packages and 
dependencies, keeping different package versions isolated on a project-by-project basis.
Anaconda is available as open source under the New BSD license. It also ships 
with pip, the common python package manager. 
Installing packages
Packages could be installed via pip or conda package managers
Installing packages on a Windows VM
A) Using conda 
From the Start menu, open a new Command Prompt (or Anaconda prompt) window, and type:

conda search package_name (search for a package by name)
conda install package_name (install a package)
conda update package_name --upgrade (upgrade the package to the latest stable version)
conda list (list all installed packages)

B) Using pip
From the Start menu, open a new Command Prompt (or Anaconda Prompt) window and type:

pip search package_name (search for a package by name)
pip install package_name (install a package)
pip update package_name --upgrade (upgrade the package to the latest stable version)
pip list (list all installed packages)

Running Python2 and Python3 using Virtual Environments {#running-python2-and-python3-using-virtual-environments}
You can specify which version of Python you want to run using conda. This can be done 
on a project-by-project basis, and is part of what is called a ""Virtual Environment"". 
A Virtual Environment is simply your isolated copy of Python in which you maintain your
own version of files and directories. It enables you to keep other projects unaffected.
With projects that have similar dependencies, you can freely install different versions
of the same package without worry on two different Virtual Environments. In order to jump
between two VE's, you simply activate or deactivate your environment. Follow the steps below:


Update Conda:
conda update conda 


Set up your Virtual Environment:
conda create -n your_env_name_goes_here (default Python version: use conda info to find out)
OR 
conda create -n your_env_name_goes_here python=version_goes_here (to find specific Python versions, use conda search ""^python$"")


If it asks you for y/n, hit y to proceed. It will start the installation

Activate your newly created environment activate your_env_name_goes_here

Install a package in your activated environment
conda install -n your_env_name_goes_here your_package_name_goes_here
OR 
conda install -n your_env_name_goes_here \ your_package_name_goes_here=version_goes_here
OR (even better)
In your home directory or Conda installation folder, create a file called .condarc (if not already there)
Inside the file write the following:
create_default_packages
    - your_package_name_goes_here
    - your_package_name_goes_here
    - your_package_name_goes_here
    ...
Now everytime you create a new environment, all those packages listed in .condarc will be installed.
6. To end the current environment session:
deactivate
7. Remove an environment:
conda remove -n your_env_name_goes_here -all


More Information
For more information, please visit the official [Anaconda website] (https://anaconda.org/anaconda/python)"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/sas.md,"+++
title = ""UVa Licensed SAS on the Ivy Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2018-02-01T00:45:12-05:00""
tags = [
    ""ivy"", 
    ""software"",
    ""data analysis"",
    ""Windows""
]
draft = false
+++
SAS Overview
SAS is a command-driven software package used for statistical analysis
and data visualization. It is available in .
It is one of the most widely used statistical software packages in both industry and academia.
You may use it if you have a large number of statistical algorithms. It is not limited to an industry,
and could be used in both scientific and non-scientific contexts. We only offer the Teaching & Research version
at the moment. 
Common Usage
For this example we will use a common scenario from SAS Clinical Standards Toolkit, which is used for supporting clinical
research activities. The SAS Clinical Standards Toolkit initially focuses on standards defined by the Clinical Data 
Interchange Standards Consortium (CDISC). CDISC is a global, open, multidisciplinary, nonprofit organization that 
has established standards to support the acquisition, exchange, submission, and archival of clinical research data and metadata. 
Starting SAS
Open the Start Menu and from All Programs locate the SAS folder. Within that click on SAS 9.4. You may also run SAS via the 
SAS Studio, which uses a browser. 
Getting a list of all installed standards
%cst_getregisteredstandards(
_cstOutputDS=work.regStds
);

Creating Data Sets Used by the Framework
%cst_createdsfromtemplate(
_cstStandard=CST-FRAMEWORK,
_cstType=control,
_cstSubType=reference,
_cstOutputDS=work.sasrefs
);

Licensing
We have a limited number of SAS Licenses available, which are provided on a first-come-first-serve basis. 
As a consequence, availability of SAS on your VM is not always guaranteed once a VM request is submitted.
Please consult with us before requesting SAS. 
More information
For more information on SAS, please consult its official documentation here
You may obtain the Administration version of SAS from the UVA Software Gateway portal here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/ctakes.md,"+++
title = ""cTAKES on Ivy Windows VMs""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2018-02-01T00:45:12-05:00""
categories = [""userinfo""]
tags = [
    ""Ivy"", 
    ""Windows"",
    ""Software"",
    ""Data Analysis""
]
draft = false
+++
cTAKES Overview
cTAKES or The clinical Text Analysis and Knowledge Extraction System, is a Mayo Clinic developed Natural Language Processing (NLP) tool used to 
extract information out of clinical records. It is open-source, and built on the Apache Unstructured Information Management Architecture. cTAKES
is modular, expandable, for a number of generic use cases, and contains excellent best practice notes. 
cTAKES Usage
cTAKES components
Some of cTAKES components are listed below:

Sentence boundary detection (OpenNLP technology)
*Tokenization (rule-based)
Morphologic normalization (NLM‚Äôs LVG)
POS tagging (OpenNLP technology)
Shallow parsing (OpenNLP technology)
Named Entity Recognition
Negation and context identification (both based on NegEx)

cTAKES Named Entities
cTAKES contains the following Named Entities:

Drug mentions
Disease/disorder mentions
Sign/symptom mentions
Anatomical site mentions
Smoking status classifier
Detailed drug mention annotator
Peripheral Artery Disease (PAD) annotator
Dependency parser

cTAKES Installation
Please note that cTAKES requires pre-installation approval by us. Please consult us prior to requesting a new VM regarding your cTAKES requirements. 
More Information
For more information on cTAKES, and how to use it, please visit the following links:

Official Website: http://ctakes.apache.org/index.html
Documentation: https://cwiki.apache.org/confluence/display/CTAKES/cTAKES
cTAKES scientific paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2995668/
"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/data-analysis/matlab.md,"+++
title = ""UVa Licensed MATLAB on Ivy Windows Virtual Machines""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
date = ""2018-02-01T00:45:12-05:00""
tags = [
  ""Ivy"",
  ""Software"",
  ""Data Analysis"",
  ""Windows"",
  ""matlab""
]
draft = false
+++
MATLAB Overview
MATLAB is a high-performance language for technical computing. It integrates
computation, visualization, and programming environment. MATLAB stands for MATrix LABoratory. MATLAB was made
to provide easy access to matrix software developed by the LINPACK (linear system package)
and EISPACK (Eigen system package) projects. MATLAB includes a programming language
environment with built-in editing and debugging tools, and supports object-oriented programming.
Programming in MATLAB
MATLAB has many advantages compared to conventional computer languages (e.g.,
C, FORTRAN) for solving technical problems. MATLAB is an interactive system whose
basic data element is an array, and almost all problems can be solved in MATLAB using that
one data element.
Starting MATLAB
To start MATLAB, open the Start menu window and find MATLAB.
Since MATLAB is a large software, it may take time to load up. When it starts, the first screen to appear
would be the MATLAB Desktop, with several windows within it. These might include the ribbons (Home, Plot, and App), the Command Window,
the Current Folder window, and the Workspace window.
Basic Arithmetic
The following are basic arithmetic operators in MARLAB
    * (Multiplication), + (Addition), - (Subtraction), / (Division)
In order to perform a simple calculation click anywhere on the Command Window and type
1+2+3

and hit Enter/Return. the result shows up on the Command Window as Ans = 6
Variables
To create a variable, type the name of the variable, followed by the equals sign (assignment operator) and the value. Hit Enter/Return to store the value.
x = 12

If you type x again on the Command Window, you would see the value of the variable displayed. Variables could be used in any number of arithmetic
calculations.
Arrays
A very basic plot can be drawn using MATLAB arrays. To create an array in MATLAB, use the square brackets:
x = [1 2 3 4 5 6]
y = [4 -4 7 3 7 1]

Notice how the array elements do not have commas between them. These arrays could be used like any other variable.
Licensing
University of Virginia has recently upgraded the Matlab license so that Matlab is available to everyone at UVa. There is one version of Matlab for students, faculty, and staff. MATLAB is available on the Windows, Mac OSX, and Linux platforms. To get started go to the UVa Software Gateway Matlab can be found under the Data Analysis & Research grouping.
The Campus Wide License configuration now includes all 100+ MathWorks products. To better reflect the new configuration they have re-named the license option to Campus Wide Suite. For further information see the URL https://data.library.virginia.edu/research-software/matlab/
More Information
For help, type help in the Command Window or click on the Help button on the HOME ribbon. For more Information on MATLAB itself, please
visit the official website here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/sw-list.md,"+++
title = ""Bioinformatics Packages on Windows VM""
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""Windows"", 
    ""Ivy"",
    ""Bioinformatics""
]
draft = false
date = ""2018-02-01T17:45:12-05:00""
+++
Available Packages
The following bioinformatics packages are available on the Windows Virtual Machines
Bowtie2
For more information on bowtie2, please click [here] (/userinfo/ivy/ivy-windows-sw/bioinformatics/bowtie2) -->
HISAT2
Requires approval before installation. For more information on HISAT2, please click here"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/idl.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/cufflinks.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/bowtie2.md,"+++
title = ""Bowtie2 on Ivy Windows VM""
description = """"
date = ""2018-02-01T17:45:12-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""windows"", 
    ""bioinformatics"",
    ""software""
]
draft = false
+++
Bowtie2 is a memory-efficient tool for aligning short sequences to long reference genomes.
It indexes the genome using FM Index, which is based on Burrows-Wheeler Transform algorithm,
to keep its memory footprint small. Bowtie2 supports gapped, local and paired-end alignment modes.
Alignment to a known reference using Bowtie2 is often an essential first step in a myriad of NGS analyses workflows. 
Bowtie2 Usage
Alignment using bowtie2 is a 2-step process - indexing the reference genome, followed by aligning the sequence data.


Create indexes of your reference genome of interest stored in reference.fasta file:
bowtie2-build [option(s)] <reference.fasta> <bt2-index-basename>

This will create new files with the provided basename and extensions .1.bt2, .2.bt2, .3.bt2 and 
.4.bt2, .rev.1.bt2 and .rev.2.bt2. 
These files constitute the index.


Align paired-end reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores:
bowtie2 -x <bt2-index-basename> -1 <sampleR1.fq> -2 \

<sampleR2.fq> -p <N> -S <output.sam>



The alignment results in SAM format are written to the file output.sam
More Information
For more information, please refer to the Bowtie2 manual.
Citation
If you use bowtie2 for your work, please cite:
Langmead B, Salzberg S. Fast gapped-read alignment with Bowtie 2. Nature Methods. 2012, 9:357-359"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/samtools.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/hisat2.md,"+++
title = ""HISAT2 on Ivy Windows VM""
description = """"
date = ""2018-02-01T17:45:12-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""windows"", 
    ""bioinformatics"",
    ""software""
]
draft = false
+++
* Please note that HISAT2 requires approval prior to installation on the VM
HISAT2 is a fast and sensitive tool for aligning short reads against the general human population 
(as well as single reference genome). It indexes the genome using a Hierarchical Graph FM Index 
(HGFM) strategy, i.e. a large set of small indexes that collectively cover the whole genome
(each index representing a genomic region of 56 Kbp).
HISAT2 Usage:
Alignment using HISAT2 is a 2-step process - indexing the reference genome, followed by aligning the sequence data.


Create indexes of your reference genome of interest stored in reference.fasta file: 
        hisat2-build [option(s)]  
    This will create new files with the provided basename and extensions *.ht2. These files constitute the index.


Align paired-end reads sampleR1.fq and sampleR2.fq to the reference genome indexed in the previous step, using N cores:
        hisat2 -x  -1  -2  -p  -S 
    The alignment results in SAM format are written to the file output.sam


Note on using the --sra-acc option
Since Ivy VM‚Äôs do not allow outside connections, --sra-acc option will not work with HISAT2. 
If users plan to use SRA data, they will have to download it and move into Ivy prior to alignment.
Please refer to our Globus user guide for help on how to do that.  
More information
For detailed information, please refer to HISAT2 manual.
Citation
If you use HISAT2 for your work, please cite:
Kim D, Langmead B and Salzberg SL. HISAT: a fast spliced aligner with low memory requirements. Nature Methods 2015"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/bioinformatics/sicer.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/image-processing/imagej.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/image-processing/axiovision.md,"+++
title = """"
description = """"
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""tag1"", 
    ""tag2""
]
draft = true
+++"
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/productivity/sumatrapdf.md,"+++
title = ""Sumatra PDF on Ivy Windows VM""
description = """"
date = ""2018-01-24T10:08:29-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""pdf"", 
    ""windows"",
    ""ivy"",
    ""software""
]
draft = false
+++
Sumatra PDF Overview
Sumatra PDF is an open source software to view PDF files in Windows. It could be used to view PDF documents stored within the Ivy VM. 
As of the latest version, Sumatra supports multiple formats including PDF, EPUB, MOBI, and XPS. 
Running Sumatra PDF
From the Start menu, go to All Programs and search for Sumatra PDF. Click on the icon to run it. 
More Information
For more information, visit the Sumatra PDF official website."
rc-website-fork/content/userinfo/ivy/ivy-windows-sw/productivity/microsoft-office-professional-plus.md,"+++
title = ""Preinstalled Office 2016 on Ivy Windows VM""
description = """"
date = ""2017-02-24T10:08:29-05:00""
author = ""RC Staff""
images = [""""]
categories = [""userinfo""]
tags = [
    ""windows"", 
    ""ivy"",
    ""software""
]
draft = false
+++
Microsoft Office Overview
The Ivy Windows VMs are installed with Microsoft Office 2016. Features such as OneDrive are not available
since Ivy is not connected to the public internet. Therefore in order to load documents in and out of the 
VM, you would have to use the Globus DTN. 
Available Software
The following software packages are available for use on the Ivy Windows VM:

Word 2016
Excel 2016
PowerPoint 2016
Access 2016
OneNote 2016
Outlook 2016
Publisher 2016

Running Office
All Office software could be accessed from the Start menu using Start > All Programs
More Information
For more Information about Microsoft Office, please visit its official website."
rc-website-fork/content/userinfo/hpc/software/spark.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2021-09-30T00:00:00-05:00""
tags = [
  ""chem"",
  ""multi-core"",
  ""mpi""
]
draft = false
modulename = ""spark""
softwarename = ""Spark""
title = ""Spark and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Using Spark interactively
There are three ways to use Spark interactively:
1. Shell prompt
1. Open OnDemand PySpark
1. Open OnDemand Desktop
If you need the Web UI you must use the third method (OOD Desktop).
Shell prompt
First submit an ijob.
Scala/PySpark
To start up a Scala or PySpark shell prompt, run spark-shell or pyspark. For example:
``
$ spark-shell
...
Spark context Web UI available at http://udc-xxxx-xx:4040
Spark context available as 'sc' (master = local[*], app id = local-1633023285536).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _/ /  '/
   // .__/_,// //_\   version 3.1.2
      //
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.2)
Type in expressions to have them evaluated.
Type :help for more information.
scala>
```
R
To start an R prompt, you must load R first. Then run sparkR. If the R version is different from 4.1.0, you will see a warning message:
package ‚ÄòSparkR‚Äô was built under R version 4.1.0
We recommend loading the closest available version.
Open OnDemand PySpark
Python users can run Spark in a JupyterLab interface via the PySpark Interactive App on Open OnDemand.
Open OnDemand Desktop
Spark provides a user interface (UI) for you to monitor your Spark job. If you intend to use the Web UI, you must request a Desktop session through Open OnDemand.
The URL is displayed upon launching Spark and is of the form http://udc-xxxx-xx:4040 where udc-xxxx-xx is the hostname of the compute node. You can either right-click on the link and select ""Open Link,"" or enter localhost:4040 in the browser.
Jupyter notebook/lab
You can redirect pyspark to open a Jupyter notebook/lab as follows. The jupyter command is provided by the jupyterlab module.
Set two environment variables:
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=lab
If you'd prefer a notebook session, replace lab with notebook.
Navigate to your working directory and run:
pyspark
This will start up Jupyter inside a browser automatically. Use the ""Python 3"" kernel.
The example below estimates the value of pi in a PySpark session running on 16 cores, with the JupyterLab window on the left and the Spark Web UI event timeline on the right. Note that the SparkContext object sc is initialized automatically.

Slurm Script Templates for Batch Jobs
Local mode on a single node
{{< pull-code file=""/static/scripts/spark_single_node.slurm"" lang=""no-highlight"" >}}
You must initialize SparkContext explicitly in your script, e.g.:
python
from pyspark import SparkContext
sc = SparkContext(""local[*]"")
The Spark log is written to slurm-<JOB_ID>.out. After the job is finished, use the seff <JOB_ID> command to verify that the cores are used effectively:
$ seff 1232109
...
Cores per node: 10
CPU Utilized: 01:17:16
CPU Efficiency: 82.20% of 01:34:00 core-walltime
...
If the CPU efficiency is much lower, please consider using fewer cores for your future jobs.
Standalone cluster mode using multiple nodes
As of 5/29/2024 this is no longer working. We will update as soon as we have a solution. For the time being please use the local mode up to 96 cores in the afton partition.
We gratefully acknowledge the Pittsburg Supercomputing Center for giving us permission to use their Spark configuration and launch scripts.
Before using multiple nodes, please make sure that your job can use a full standard node effectively. When you request N nodes in the standalone cluster mode, one node is set aside as the master node and the remaining N-1 nodes are worker nodes. Thus, running on 2 nodes will have the same effect as running on 1 node.
{{< pull-code file=""/static/scripts/spark_multinode.slurm"" lang=""no-highlight"" >}}
In the above Slurm script template, note that:

Request parallel nodes with exclusive access.
You may reduce the number of cores if the job needs more memory per core.

Your code should begin with:
python
from pyspark import SparkConf
from pyspark import SparkContext
conf = SparkConf()
sc = SparkContext(conf=conf)
spark = SparkSession(sc)


You may need to set the number of partitions explicitly, e.g. in the second argument of sc.parallelize():
python
sc.parallelize(..., os.environ['PARTITIONS'])
where the PARTITIONS environment variable is defined as the total number of cores on worker nodes in the Slurm script for your convenience. Without doing so only one partition will be created on each node.


Benchmark
We used a code that estimates the value of pi as a benchmark. The following table illustrates good scaling performance across multiple nodes (40 cores per node) on the HPC system.
| Nodes | Worker nodes | Time (s) |
|--:|--:|--:|
|1|1|134.4|
|3|2|71.3|
|5|4|39.6|
|9|8|23.6|
Cleanup
Temporary files are created inside your scratch directory during a multinode Spark job. They have the form:

spark-mst3k-org.apache.spark.deploy.master.Master-1-udc-aw33-2c1.out
spark-8147c5b8-eb70-4b98-809e-19fdbcf3eafb
app-20211012113817-0000
blockmgr-b41a7c79-cbf4-49f0-b373-f6c6467e9d01

You may safely remove these files when your job is done by running:
{{< code-snippet >}}
find /scratch/$USER -maxdepth 1 -regextype sed ( -name ""spark-$USER-"" -o -regex './spark-[0-9a-z]{8}-.' -o -regex './app-[0-9]{14}-.' -o -regex './blockmgr-[0-9a-z]{8}-.*' ) -exec rm -rf {} \;
{{< /code-snippet >}}
Make sure that you do not use this pattern to name other files!"
rc-website-fork/content/userinfo/hpc/software/wdltool.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""wdltool""
softwarename = ""WDLTool""
title = ""WDLTool and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/engineering.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [""cae""
]
draft = false
shorttitle = ""Engineering""
title = ""Engineering and UVA HPC""
description = ""Engineering Software in the HPC environment""
author = ""RC Staff""
+++
Overview
Several software packages for computer-aided engineering are available on the UVA HPC system.
General considerations
Some engineering software packages utilize single node, multicore or multi-node MPI for parallel execution.  Accordingly, the Slurm job scripts should contain either of the following two SBATCH directives:
Single Node Multi-Core
```
SBATCH -N 1                    # request single node
SBATCH --cpus-per-task=     # request X multiple cpu cores
``
Replace` with the actual number of cpu cores to be requested.
Multi Node MPI
```
SBATCH -N                   # request M nodes (replace with a number)
SBATCH --ntasks-per-node=   # request L MPI processes per node
You should launch your program with `srun` as the MPI executor, for example for Quantum Espresso
srun pw.x -in mymol.in
```
Please see the page of the particular package you wish to use for more details.
VASP Users
The Vienna Ab-Initio Simulation Package, is licensed by individual groups and we do not have a common installation.  We have basic instructions for building VASP on the HPC system at its page.
Available Engineering Software
To get an up-to-date list of the installed engineering applications, log on to UVA HPC and run the following command in a terminal window:
module keyword cae
To get more information about a specific module version, run the module spider command, for example:
module spider ansys

List of Engineering Software Modules
{{< rivanna-software moduleclasses=""cae"" >}}
Using a Specific Software Module
To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module.
After loading a module, you are ready to run the application(s) provided by the module. For example:
module load ansys"
rc-website-fork/content/userinfo/hpc/software/chemistry.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [""chem"",""computational-chemistry"",""dft""
]
draft = false
shorttitle = ""Chemistry""
title = ""Chemistry and UVA HPC""
description = ""Chemistry Software in the HPC environment""
author = ""RC Staff""
+++
Overview
Many popular software packages for computational chemistry are available on Rivanna and Afton.
General considerations
Most computational chemistry packages utilize MPI for parallel execution.  Accordingly, the Slurm job scripts should contain the following two SBATCH directives:
```
SBATCH -N                   # request M nodes (replace with a number)
SBATCH --ntasks-per-node=   # request L MPI processes per node
You should launch your program with `srun` as the MPI executor, for example for Quantum Espresso
srun pw.x -in mymol.in
```
Please see the page of the particular package you wish to use for more details.
VASP Users
The Vienna Ab-Initio Simulation Package, is licensed by individual groups and we do not have a common installation.  We have basic instructions for building VASP on the HPC system at its page.
Available Chemistry Software
To get an up-to-date list of the installed chemistry applications, log on to UVA HPC and run the following command in a terminal window:
module keyword chem
To get more information about a specific module version, run the module spider command, for example:
module spider quantumespresso

List of Chemistry Software Modules
{{< rivanna-software moduleclasses=""chem"" >}}
Using a Specific Software Module
To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module.
After loading a module, you are ready to run the application(s) provided by the module. For example:
module load intel quantumespresso"
rc-website-fork/content/userinfo/hpc/software/quantumespresso.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
  ""chem"",
  ""gpu""
]
draft = false
modulename = ""quantumespresso""
softwarename = ""QuantumEspresso""
title = ""QuantumEspresso and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Local support is not available. For detailed documentation and tutorials, visit the {{% software-name %}} website. QuantumEspresso (QE) has a large and active community of users; to search or join the mailing list see the instructions here.
Software Category: {{% module-category %}}
Available Versions
We built versions of QE incorporating the most popular optional packages. To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Users may build their own versions of QE if they wish to use a different compiler+MPI combination, or to choose individual optional packages.  Instructions are available at the installation FAQ.
Example Slurm script
To run the system version of QE, a script similar to the following can be used.  QE has many options so only the most basic is shown.
Please run the CPU version on non-gpu partitions and the GPU version only on the gpu partition. In both cases, we highly recommend running a benchmark to decide how many CPU cores and/or GPU devices you should use.
CPU
{{< pull-code file=""/static/scripts/qe_cpu.slurm"" lang=""no-highlight"" >}}
GPU
{{< pull-code file=""/static/scripts/qe_gpu.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/compilers.md,"+++
type = ""rivanna""
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""compiler"", ""c/c++"", ""fortran""
]
draft = false
title = ""Compilers and UVA HPC""
description = ""Compilers and UVA HPC""
author = ""RC Staff""
+++
UVA HPC offers multiple compiler bundles for C, C++, and Fortran.  Different compilers have different strengths and weaknesses and different error messaging and debugging features, so users should be willing to try another one when appropriate.  The modules system manages the compiler environment and ensures that only compatible libraries are available for loading.
Many users of compiled languages are working with codes that can employ MPI for multinode parallel runs.  MPI users should first understand how their chosen compiler works, then see the MPI instructions at our parallel programming page.
Compiled languages can be more difficult to debug, and the assistance of a good debugger can be essential.  Descriptions of debuggers available on the HPC system can be found at our debuggers and utilities page.
Available Compilers on The HPC System
{{< rivanna-software moduleclasses=""compiler"" exclude=""mpi"" >}}
GNU Compiler
The GNU Compiler Collection compilers are free, open-source tools. Additional tools included are the gdb debugger and the gprof performance profiler. For detailed documentation, visit the GNU website.
The compilers are:

Fixed-format Fortran: gfortran [options] filename.f
Free-format Fortran: gfortran [options] filename.f90
C: gcc [options] filename.c
C++: g++ [options] filename.cpp or g++ [options] filename.cxx

A list of compiler options can be obtained by invoking the compiler with the -help option. For example: gfortran -help
More information is available from the manpage, e.g.:
man g++
The default GNU compilers on the HPC system are typically fairly old. Newer versions can be invoked through an appropriate module. For available versions, please run
module spider gcc
It is important to load the correct module if you want to use more advanced features available in newer standards, particularly for C++ and Fortran.
module load gcc
It may be necessary to use an older compiler for programs using GPGPUs.
Available GNU Compilers
{{< module-versions module=""gcc"" >}}
Intel Compiler
The Intel Linux Fortran and C/C++ compilers are for x86-64 processor-based systems running Linux. These compilers have specific optimizations for Intel architectures. The University has floating network licenses for the Intel compiler suite as well as for the Math Kernel Libraries.
For detailed information, review the documentation on the Intel C/C++ and Fortran compiler website.
The Intel compilers are accessed on the cluster by using the modules software to dynamically set the appropriate environmental variables (e.g. PATH and LD_LIBRARY_PATH). To initialize your environment to use the Intel compilers, use the command:
module load intel

Fortran fixed format: ifort [options] filename.f
Fortran free format: ifort [options] filename.f90
C: icc [options] filename.c
C++: icpc [options] filename.cpp or: icpc [options] filename.cxx

A list of compiler options can be obtained by invoking the compiler with the -help option, e.g.:
ifort -help
or by accessing the manpage, e.g.
man ifort
To see the available versions, run
module spider intel
Then load the appropriate module, in this case the default version
module load intel
Available Intel Compilers
{{< module-versions module=""intel"" >}}
Important note for Fortran programmers: Nearly all Fortran code must be compiled with the flag -heap-arrays added or it will encounter a segmentation violation.
If you still experience segmentation violations, recompile with -g -CB (for debugging and bounds checking respectively) and run the program under the control of a debugger.  Once the program is debugged, be sure to remove the -g and certainly the -CB flags and recompile with -O or -O -ipo. If that works, try -O3 or -O3 -ipo for a higher level of optimization.  
If mathematical libraries are needed, we strongly recommend the Intel Math Kernel Libraries (MKL). They provide LAPACK, BLAS, and a number of other libraries. They are highly optimized, especially for Intel architecture, and they automatically work with the compiler. The Intel module for any version loads the MKL that is compatible with that compiler. The module script sets an environment variable MKL_DIR (with MKLROOT as a synonym). This variable can be used in scripts and makefiles. For example, in a [mM]akefile: LIBS=-L$(MKLROOT).
The MKL consists of a number of libraries, and which ones to link is not always obvious.  In newer Intel compilers a flag -mkl can be added for both compiler and linker.  However, that does not always suffice, so Intel provides a link line advisor at their site. Remember that default integers are 32 bits even on 64-bit systems. The MKL bundled with our compiler suite includes ScaLAPACK.
NVIDIA CUDA Compiler
The NVIDIA HPC SDK C, C++, and Fortran compilers support GPU acceleration of HPC modeling and simulation applications with standard C++ and Fortran, OpenACC directives, and CUDA.
NVIDIA CUDA compilers are accessed on the HPC cluster by using modules to dynamically set the appropriate environmental variables (e.g. PATH and LD_LIBRARY_PATH). To initialize your environment to use the CUDA compilers, use the command
module load cuda
or
module load nvhpc
Available NVIDIA CUDA Compilers
{{< module-versions module=""cuda"" >}}
{{< module-versions module=""nvhpc"" >}}
Please see here for details.
PGI Compiler
Please use the nvhpc module instead (see previous section).
Building on the HPC System
For more information about building your code on UVA HPC, please see our howto."
rc-website-fork/content/userinfo/hpc/software/python.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-10-23T00:00:00-05:00""
tags = [
  ""programming""
]
draft = false
modulename = ""python""
softwarename = ""Python""
shorttitle = ""Python""
title = ""Python and UVA HPC""
description = ""Python in the HPC environment""
author = ""RC Staff""
+++
Overview
Python is an integrated technical computing environment that combines sophisticated computation, advanced graphics and visualization, and a high-level programming language.
Learning Python
Research Computing offers an online ""Introduction to Programming in Python"" course. Click here to start learning Python.
Python on the HPC System
The default Python is required for system purposes and is generally too old for applications. We offer Python through the Miniforge distribution.
View our Miniforge on the HPC System documentation for details."
rc-website-fork/content/userinfo/hpc/software/openfoam.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2020-02-27T11:59:46-05:00""
tags = []
draft = false
shorttitle = ""OpenFoam""
modulename = ""openfoam""
softwarename = ""OpenFOAM""
title = ""OpenFoam and UVA HPC""
author = ""RC Staff""
+++
Description
{{< module-description >}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Overview
OpenFOAM is an open-source, free software package for computational fluid dynamics (CFD). It is powerful, and is an alternative to ANSYS Fluent, but it may have a steeper learning curve than Fluent's.  OpenFOAM does not provide a graphical user interface for setting up problems.  
Tutorials
Tutorials are available online.  OpenFOAM itself is distributed with a number of tutorials. To find them, run
ls $FOAM_TUTORIALS
Generally there are several tutorials for each topic.  Once you have chosen one, you will need to copy all its files to one of your directories, since you cannot write into the general OpenFOAM directories; for example
mkdir foam_test
cd foam_test
cp -r $FOAM_TUTORIALS/compressible/rhoSimpleFoam/aerofoilNACA0012
Documentation for the tutorials is available at the OpenFOAM site.
Parallel Processing
OpenFOAM has been compiled on the HPC system to run with MPI.  Please see our Slurm documentation for information about running MPI programs.  
PostProcessing
OpenFOAM uses ParaView for visualization of results.  You must use the version of Paraview that has been compiled to be compatible with OpenFOAM.  It will be loaded automatically when you load the openfoam module. It is invoked through the paraFoam command.  You must use our FastX Web access on UVA HPC in order to run Paraview.  To invoke paraFoam, start a terminal on the MATE desktop and run
vglrun -c proxy paraFoam"
rc-website-fork/content/userinfo/hpc/software/physics.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2019-06-23T08:37:46-05:00""
tags = [
  ""physics"",
]
draft = false
shorttitle = ""Physics""
title = ""Physics and UVA HPC""
description = ""Physics Software in the HPC environment""
author = ""RC Staff""
+++"
rc-website-fork/content/userinfo/hpc/software/apptainer.md,"+++
categories = [""userinfo""]
type = ""rivanna""
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""containers"", ""apptainer""
]
draft = false
title = ""Apptainer and UVA HPC""
description = ""Apptainer and UVA HPC""
author = ""RC Staff""
+++
Introduction
Apptainer is a continuation of the Singularity project (see here). On December 18, 2023 we migrated from Singularity to Apptainer.
Containers created by Singularity and Apptainer are mutually compatible as of this writing, although divergence is to be expected.
One advantage of Apptainer is that users can now build container images natively on the UVA HPC system.
Apptainer and UVA HPC (after 12/18/2023)
Apptainer is available as a module. The RC staff has also curated a library of pre-prepared Apptainer container images for popular applications as part of the shared software stack.  Descriptions for these shared containers can be found via the module avail and module spider commands.
module load apptainer
module avail
Loading any of these container modules produces an on-screen message with instructions on how to copy the container image file to your directory and how to run the container.
Building Apptainer Containers
To build your own image from scratch, create an Apptainer definition file (say myimage.def) and run:
module load apptainer
apptainer build myimage.sif myimage.def
For containers larger than several GBs we recommend that you perform the build on a compute node in the largemem partition, either through a batch job or an interactive job. Building such containers on the frontend will likely fail silently due to insufficient memory.
Details on how to write a definition file will be provided in this forthcoming workshop.
What is Inside the Container?
Use the shell command to start up a shell prompt and navigate (more later).
For containers built with Apptainer, you can use the run-help command to learn more about the applications and libraries:
apptainer run-help /path/to/sif
You can also use the inspect --runscript command to find the default execution command. Using the TensorFlow module as an example:
```bash
$ module load apptainer tensorflow/2.10.0
$ apptainer inspect --runscript $CONTAINERDIR/tensorflow-2.10.0.sif
!/bin/sh
OCI_ENTRYPOINT='""python""'
...
```
This shows that python will be executed when you run (more later) the container.
Running non-GPU Images
If your container does not require a GPU, all that is necessary is to load the apptainer module and provide it with a path to the image.
module load apptainer
apptainer <CMD> <OPTIONS> <IMAGEFILE> <ARGS>
CMD defines how the container is used. There are three main commands:

run: Executes a default command inside the container. The default command is defined at container build time.
exec: Executes a specific application/command inside the container as specified with ARGS. This provides more flexibility than the run command.
shell: Starts a new interactive command line shell inside the container.

OPTIONS define how the apptainer command is executed.  These can be omitted in most cases.
IMAGEFILE refers to the single Apptainer container image file, typically with a .sif or .simg extension.
ARGS define additional arguments passed inside the container.  In combination with the exec command they define what command to execute inside the container.
apptainer run
apptainer run myimage.sif
This executes a default application or set of commands inside the container.  The default application or set of commands to execute is defined in the image build script and cannot be changed after the container is built.  After execution of the default command, the container is closed.
apptainer exec
apptainer exec myimage.sif python myscript.py
This is similar to apptainer run but more versatile by allowing the specification of the particular application or command to execute inside the container.  In this example it launches the python interpreter and executes the myscript.py script, assuming that Python was installed into the container image.  After execution of the command, the container is closed.
apptainer shell
apptainer shell myimage.sif
This opens a new shell inside the container, notice the change of the prompt:
Apptainer>
Now you can execute any command or application defined in the container, for example ls to list all files in the current directory:
Apptainer> ls
You can navigate the container file system, including /scratch and /nv, and run any application that is installed inside the container. To leave the interactive container shell, type exit:
Apptainer> exit
Running GPU Images
Apptainer can make use of the local NVIDIA drivers installed on the host.  To use a GPU image, load the apptainer module and add the --nv flag when executing the apptainer shell, apptainer exec, or apptainer run commands.
module load apptainer
apptainer <CMD> --nv <IMAGE_FILE> <ARGS>
Example:
module load tensorflow/2.10.0.sif
apptainer run --nv $CONTAINERDIR/tensorflow-2.10.0.sif myscript.py
In the container build script, python was defined as the default command to be executed and apptainer passes the argument(s) after the image name, i.e. myscript.py, to the Python interpreter. So the above apptainer command is equivalent to
apptainer exec --nv $CONTAINERDIR/tensorflow-2.10.0.sif python myscript.py
This image was built to include CUDA and cuDNN libraries that are required by TensorFlow.  Since these libraries are provided by the container, we do not need to load the CUDA/cuDNN libraries available on the host.
Running Images Interactively
Start an ijob:
ijob  -A mygroup -p gpu --gres=gpu -c 1
salloc: Pending job allocation 12345
salloc: job 12345 queued and waiting for resources
salloc: job 12345 has been allocated resources
salloc: Granted job allocation 12345
If your image starts a graphical user interface or otherwise needs a display, you should use the Open OnDemand Desktop rather than a command-line ijob.  Once the Desktop is launched, start a terminal window and type the commands as in any other shell.
module purge
module load apptainer
apptainer shell --nv /path/to/sif
Running Image Non-Interactively as Slurm jobs
Example script:
```
!/usr/bin/env bash
SBATCH -J tftest
SBATCH -o tftest-%A.out
SBATCH -e tftest-%A.err
SBATCH -p gpu
SBATCH --gres=gpu:1
SBATCH -c 1
SBATCH -t 00:01:00
SBATCH -A mygroup
module purge
module load apptainer tensorflow/2.10.0
apptainer run --nv $CONTAINERDIR/tensorflow-2.10.0.sif tensorflowtest.py
```
Interaction with the Host File System
Each container provides its own file system.  In addition, directories of the host file system can be overlayed onto the container file system so that host files can be accessed from within the container.  These overlayed directories are referred to as bind paths or bind points.  The following system directories of the host are exposed inside a container:

/tmp
/proc
/sys
/dev

In addition, the following user directories are overlayed onto each container by default on the HPC system:

/home
/scratch
/nv
/project

Due to the overlay these directories are by default the same inside and outside the container with the same read, write, and execute permissions.  This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits.  The /nv and /project directories refer to leased storage locations that may not be available to all users.
Disabling the Default Bind Paths
Under some circumstances this default overlay of the host file systems is undesirable.  Users can disable the overlay of /home, /scratch, /nv, /project by adding the -c flag when executing the apptainer shell, apptainer exec, or apptainer run commands.
For example,
apptainer run -c myimage.sif
Adding Custom Bind Paths
Users can define custom bind paths for host directories via the -B/--bind option, which can be used in combination with the -c flag.
For example, the following command adds the /scratch/$USER directory as an overlay without overlaying any other user directories provided by the host:
apptainer run -c -B /scratch/$USER myimage.sif
To add the /home directory on the host as /rivanna/home inside the container:
apptainer run -c -B /home:/rivanna/home myimage.sif"
rc-website-fork/content/userinfo/hpc/software/snakemake.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2021-07-08T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""snakemake""
softwarename = ""Snakemake""
title = ""Snakemake and UVA HPC""
author = ""RC Staff""
+++
Description

{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.

Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}

Snakemake workflow:

Snakemake is a workflow management system used to create reproducible and scalable data analyses
Workflows are written in Python and can be deployed in parallel on the HPC system
Workflows can be executed in containerized environments: Conda or Apptainer

Snakemake rules:

- Snakemake follows the GNU Make paradigm
- Workflows are defined in rules, starting with the target rule
- Dependencies between the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be parallelized
Config.yaml file:

Config files are for users to input filenames and paths for the workflow
In the case below, the user inputs 3 samples for a simple RNA-seq pipeline
Threads can be passed as an argument for multithreading

```
Inset sample names as a list:
SAMPLES: [""exp1"", ""exp3"", ""Sham1""]
Insert path to GTF file:
GENOME_GTF: ""refGene.gtf""
Insert path to Hisat2 index files:
HISAT2_INDEX_PREFIX: ""grcm38_snp/genome_snp""
Insert the path to the directory that has your fastq files:
DATA_PATH: ""/project/some_directory/fastq_files""
Each sample should have R1 and R2 extensions:
R1_EXT: ""_R1_001_paired.fq.gz""
R2_EXT: ""_R2_001_paired.fq.gz""
Insert number of threads you would like to use:
THREADS: 8
```
Snakefile:

The Snakefile contains the rules of your workflow (the steps)
The target rule is your final output, Snakemake will determine the order of the rules in order to create that output
Each rule consists of 3 required parts: the input files, the output files, and the shell (command)
Below is an example of a rule to align sequences using hisat. The log and threads options are optional, but included for reference
The target output is a gene count matrix in a csv format

```
Target Rule (last output):
rule all:
     input: ""differential_expression/gene_count_matrix.csv""
rule align_hisat:
    input:
        fq1= config['DATA_PATH'] + ""{sample}"" + config['R1_EXT'],
        fq2= config['DATA_PATH'] + ""{sample}"" + config['R2_EXT'],
        hisat2_index=expand(f""{HISAT2_INDEX_PREFIX}.{{ix}}.ht2"", ix=range(1, 9))
    output: ""align_hisat2/{sample}.bam""
    log: ""align_hisat2/{sample}.alignment.summary""
    threads: config['THREADS']
    shell:
        ""hisat2 2>{log} -p {threads} --dta -x {HISAT2_INDEX_PREFIX} ""
        ""-1 {input.fq1} -2 {input.fq2} | ""
        ""samtools sort -@ {threads} -o {output}""
```

After the rule align_hisat is completed, the workflow can move to the next rule stringtie_assemble
Notice that the output of align_hisat is a .bam file, this is now the input to the rule stringtie_assemble

rule stringtie_assemble:
    input:
        genome_gtf=config['GENOME_GTF'],
        bam=""align_hisat2/{sample}.bam""
    output: ""stringtie/assembled/{sample}.gtf""
    threads: config['THREADS']
    shell:
        ""stringtie -p {threads} -G {input.genome_gtf} ""
        ""-o {output} -l {wildcards.sample} {input.bam}""

You can add as many rules as you like as long as they are sequential with inputs and outputs

Slurm for Snakemake:

The Snakemake pipeline can be executed using a SLURM script on the HPC system
Below is an example script to submit to the standard partition with 8 threads
This script is using a conda environment called rnaseq

{{< pull-code file=""/static/scripts/snakemake.slurm"" lang=""no-highlight"" >}}
Dry Runs:

Dry-runs are a great way to check your commands before running them
The code is printed, but not actually run
For a dry run, use snakemake -n
"
rc-website-fork/content/userinfo/hpc/software/mpi.md,"+++
type = ""rivanna""
date = ""2019-04-23T08:37:46-05:00""
tags = [
  ""rivanna"", ""software"", ""mpi""
]
draft = false
title = ""Message Passing Interface (MPI) and UVA HPC""
description = ""Distributed Memory and Message Passing Interface and UVA HPC""
author = ""RC Staff""
+++
Overview
MPI stands for Message Passing Interface. The MPI standard is defined by the Message Passing Interface Forum. The standard defines the interface for a set of functions that can be used to pass messages between processes on the same computer or on different computers. MPI can be used to program shared memory or distributed memory computers. There is a large number of implementations of MPI from various computer vendors and academic groups. MPI is supported on the HPC clusters.
MPI On the HPC System
MPI is a standard that describes the behavior of a library.  It is intended to be used with compiled languages (C/C++/Fortran).  Several implementations of this standard exist.  UVA HPC supports OpenMPI for all our compilers and IntelMPI for the Intel compiler.   MPI can also be used with the interpreted languages R and Python through packages that link to an implementation; on the HPC system these languages use OpenMPI.  
Selecting Compiler and Implementation
An MPI implementation must be built with a specific compiler. Consequently, only compilers for which MPI has been prepared can be used with it. All versions of the Intel compiler will have a corresponding IntelMPI. For OpenMPI run
module spider openmpi
This will respond with the versions of OpenMPI available. To see which version goes with which compiler, run
module spider openmpi/<version>
For example:
module spider {{< module-firstversion modulename=""openmpi"" >}}
Example output:
You will need to load all module(s) on any one of the lines below before the
""{{< module-firstversion modulename=""openmpi"" >}}"" module is available to load.
   gcc/11.4.0
This shows that OpenMPI version {{< module-firstversion modulename=""openmpi"" >}} is available for gcc 11.4.0.
Once a choice of compiler and MPI implementation have been made, the modules must be loaded.  First load the compiler, then the MPI.  For instance, to use OpenMPI with gcc 11.4.0, run
module load gcc/11.4.0
module load openmpi
To load the Intel compiler version and its IntelMPI version, run
module load intel
However, for Intel 18.0, run:
module load intel/18.0
module load intelmpi/18.0
It is also possible to combine these into one line, as long as the compiler is specified first (this can result in errors if you are not using the default compiler, however)
module load gcc openmpi
For a detailed description of building and running MPI codes on the HPC system, please see our HowTo.

Available MPI library modules
{{< rivanna-software moduleclasses=""mpi"" >}}
Example Slurm Scripts
This example is a Slurm job command file to run a parallel (MPI) job using the OpenMPI implementation:
{{< pull-code file=""/static/scripts/mpi_job.slurm"" lang=""no-highlight"" >}}
In this example, the Slurm job file is requesting two nodes with sixteen tasks per node for a total of 32 processes.  Both OpenMPI and IntelMPI are able to obtain the number of processes and the host list from Slurm, so these are not specified.  In general, MPI jobs should use all of a node, but some codes cannot be distributed in that manner so we are showing a more general example here.
Slurm can also place the job freely if the directives specify only the number of tasks. In this case do not specify a node count.  This is not generally recommended, however, as it can have a significant negative impact on performance.
{{< pull-code file=""/static/scripts/mpi_job_free_placement.slurm"" lang=""no-highlight"" >}}
Example: MPI over an odd number of tasks
{{< pull-code file=""/static/scripts/mpi_job_odd_number.slurm"" lang=""no-highlight"" >}}
MPI with OpenMP
The following example runs a total of 32 MPI processes, 8 on each node, with each task using 5 cores for threading.  The total number of cores utilized is thus 160.
{{< pull-code file=""/static/scripts/hybrid_job.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/gurobi.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2025-03-28T00:00:00-05:00""
tags = [
  ""gurobi"",
]
draft = false
shorttitle = ""Gurobi""
softwarename = ""Gurobi""
modulename = ""gurobi""
title = ""Gurobi and UVA HPC""
description = ""Gurobi in the HPC environment""
author = ""RC Staff""
toc = true
+++
Gurobi
The Gurobi Optimizer is a state-of-the-art solver for mathematical programming. The solvers in the Gurobi Optimizer were designed from the ground up to exploit modern architectures and multicore processors, using the most advanced implementations of the latest algorithms.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions. To load the most recent version of {{% software-name %}}, at the terminal window prompt run:
module load gurobi
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
License and Permission
We have an academic site license that allows UVA faculty, staff, and students to use Gurobi on the HPC system. The license is restricted to academic use and all use for commercial purposes is forbidden.
Please submit a ticket if you are UVA faculty/staff/student and need access to the software.
Using Gurobi
There are several ways to use Gurobi. First load the module.
Gurobi command prompt
Run:
gurobi.sh
Python
To import gurobipy as a Python module, you can use either Gurobi's own python3.7 executable or a different python. gurobi/10.0.1 uses python3.7 and gurobi/11.0.0 uses python3.11.
Gurobi Python
Please replace python with python3.7 or python3.11 in your Slurm scripts. However, note that Gurobi does not provide pip. If you need additional Python packages please use a non-Gurobi Python (e.g. via module load miniforge). See next section.
Non-Gurobi Python
Gurobi/10.0.1 supports Python versions 2.7, 3.6 - 3.9. Please follow the instructions in the module load message. To check the version of your python, run python -V.
If you are using the base python from the miniforge module and have trouble installing additional packages, run:
bash
export PYTHONPATH=$EBROOTMINIFORGE/lib/python3.11:$PYTHONPATH
You will still be able to import gurobipy from the gurobi module. Do not install your own gurobipy.
If you followed these instructions and still have trouble importing gurobipy in your Python script, please use the Gurobi Python.
Julia
The GUROBI_HOME environment variable is already defined. Load the julia module and run:
julia
import Pkg
Pkg.add(""Gurobi"")
Pkg.build(""Gurobi"")"
rc-website-fork/content/userinfo/hpc/software/pytorch.md,"+++
type = ""rivanna""
date = ""2020-02-28T08:37:46-05:00""
tags = [
  ""rivanna"", ""software"", ""machine-learning"",""deep-learning""
]
draft = false
modulename = ""pytorch""
softwarename = ""PyTorch""
title = ""PyTorch and UVA HPC""
description = ""PyTorch and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
PyTorch Jupyter Notebooks
Jupyter Notebooks can be used for interactive code development and execution of Python scripts and several other codes. PyTorch Jupyter kernels are backed by containers in the corresponding modules.
Accessing the JupyterLab Portal

Open a web browser and go to:  https://ood.hpc.virginia.edu.
Use your ‚ÄúNetbadge‚Äù credentials to log in.
On the top right of the menu bar of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on JupyterLab.

Requesting access to a GPU node
To start a JupyterLab session, fill out the resource request webform.  To request access to a GPU, verify the correct selection for the following parameters:

Under Rivanna Partition, choose ""GPU"".
Under Optional GPU Type, choose a GPU type or leave it as ""default"" (first available).
Click Launch to start the session.

Editing and Running the Notebook
Once the JupyterLab instance has started, you can edit and run your notebook as described here.
PyTorch Slurm jobs
The following is a Slurm script template. The commented numbers correspond to the items in the ensuing notes.
```
!/bin/bash
SBATCH -A mygroup
SBATCH -p gpu          # 1
SBATCH --gres=gpu:1    # 1
SBATCH -c 1
SBATCH -t 00:01:00
SBATCH -J pytorchtest
SBATCH -o pytorchtest-%A.out
SBATCH -e pytorchtest-%A.err
module purge
module load apptainer pytorch/2.0.1  # 2
apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif pytorch_example.py # 3
```
Notes:


The Slurm script needs to include the #SBATCH -p gpuand #SBATCH --gres=gpu directives in order to request access to a GPU node and its GPU device.  Please visit the Jobs Using a GPU section for details.


To use the pytorch container, load the apptainer and pytorch modules. You may choose a different version (see module spider above).
Do not load the cuda or cudnn modules since these libraries are included with pytorch.


The --nv flag sets up the container's environment to use a GPU when running a GPU-enabled application. The run command executes the default command defined in the container, which in this case is python. What follows after the *.sif is passed as arguments. In summary, the apptainer command can be translated as: ""Use the python interpreter inside the pytorch container to execute pytorch_example.py with GPU enabled.""


PyTorch Interactive Jobs (ijob)
Start an ijob.  Note the addition of -p gpu and --gres=gpu to request access to a GPU node and its GPU device.
ijob -A mygroup -p gpu --gres=gpu -c 1
module purge
module load apptainer pytorch/2.0.1
apptainer run --nv $CONTAINERDIR/pytorch-2.0.1.sif pytorch_example.py
Interaction with the Host File System
The following user directories are overlayed onto each container by default on the HPC system:

/home
/scratch
/nv
/standard
/project

Due to the overlay, these directories are by default the same inside and outside the container with the same read, write, and execute permissions. This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits. The /nv and /project directories refer to leased storage locations that may not be available to all users."
rc-website-fork/content/userinfo/hpc/software/ide.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = []
draft = false
shorttitle = ""IDEs and Editors""
title = ""IDEs and Editors""
description = ""Integrated Development Environments (IDEs) and Editors in the HPC environment""
author = ""RC Staff""
+++
Editors
Several text editors are available on the HPC system.  Most provide features such as syntax coloring.  
Vim (Vi iMproved)
Vim is an updated version of the early Unix text editor vi (for ""visual"").  It provides many extensions over plain vi.  On the HPC system, the vi command is equivalent to the vim command.  Vim is primarily utilized through keyboard commands.  Once learned, it is extremely efficient to use.  Many tutorials can be found online such as https://vim.fandom.com/wiki/Tutorial.
Emacs
Emacs is another well-known Unix text editor. Like vim, it is largely operated through the keyboard.  It can run a compiler and debugger so has some of the capabilities of an IDE.  An introduction can be found here.
Nano
Nano is a simplified version of Emacs. It is easy to use and mostly self-explanatory.  An introduction is available at its homepage.
Pluma
Pluma is a simple WYSIWYG text editor provided by the MATE desktop.  It is a variant of gedit and we provide an alias to it, so either name should work. It is very similar to Notepad++ on Windows and can do syntax coloring.
IDEs
An Integrated Development Environment (IDE) provides more features than a text editor.  They are nearly all graphical in nature and so must be used through a graphics-capable frontend.  On the HPC system we recommend using them via FastX.
Geany
Geany is a lightweight IDE.  In some respects it is intermediate between a text editor such as pluma and a full-featured IDE. It is capable of managing building C/C++/Fortran programs, including through make.  It provides syntax coloring for many languages other than the three compiled languages.  It is accessed through a module:
module load geany
{{% module-versions module=""geany"" %}}
Code Server
See here
{{% module-versions module=""code-server"" %}}"
rc-website-fork/content/userinfo/hpc/software/gaussian.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""chem"",
  ""multi-core"",
  ""mpi""
]
draft = false
modulename = ""gaussian""
softwarename = ""Gaussian""
title = ""Gaussian and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
License and Permission
We have a site license. Please contact us if you need access to the software. (Non-PIs should ask their PI to send in the request.)
For research, please load gaussian. For course work, please load gaussian/grads16.
GaussView
The GaussView graphical interface is available on the UVA HPC system.  Users must log in through a client capable of displaying X11 graphics.  We recommend the Desktop session through Open OnDemand.  GaussView can be used to create input files for Gaussian jobs which should then be submitted to the compute nodes.  To start GaussView, in an X11-enabled terminal first load the gaussian module as above, then run
gview &
The ampersand (&) returns the terminal to input mode while GaussView is running. 
Note: If the above command launches a text editor instead of GaussView, either you have not been granted access or your PATH is incorrectly configured. In either case, please run the following commands and send us the output:
```bash
groups
if ""gaussian"" is not present in the output:
- if you are the PI, please contact us for access
- if you are not the PI, please ask your PI to request access for you
- no need to execute the remaining commands
module load gaussian # or gaussian/grads16, see explanation in previous section
which g16 gview
hostname
```
Single-Core Slurm Script
This is a Slurm job command file to run a Gaussian 16 batch job. In this example, the Gaussian 16 input is in the file h2o.com.  If no output file is specified, it will go to h2o.log.  The script assumes it will be submitted from the user's /scratch directory and the input file is in the same directory.  Gaussian also tends to use a lot of memory, so we make sure to request the amount per core that is available.  We pass that to g16 with the -m flag.  Be sure the value is less than or equal to what you request from Slurm.
{{< pull-code file=""/static/scripts/gaussian_serial.slurm"" lang=""no-highlight"" >}}
Multicore Gaussian Job
By default, Gaussian runs on a single core.  However, many jobs can efficiently utilize more than one core on a node.  See the Gaussian documentation for their recommendations.  Not all jobs will scale at all, and some will scale only to a limited number of cores, so it's important to run tests and track the speedup for multicore jobs, so as not to waste resources or service units.
The Gaussian documentation on multicore jobs contains instructions to specify core numbers and they are moving to this system, away from users specifying the number of cores.  However, on a resource-managed system the user must not specify core numbers, since these are assigned by Slurm.  Gaussian 16 still provides an option to request a particular number of cores.  The safest way in a resource-managed environment is to use the command-line option with a Slurm environment variable.
{{< pull-code file=""/static/scripts/gaussian_multicore.slurm"" lang=""no-highlight"" >}}
Multinode Computations with Linda
A few types of computation can make effective use of more than one node through Gaussian's Linda parallel execution system.  The Gaussian documentation states that ""HF, CIS=Direct, and DFT calculations are Linda parallel, including energies, optimizations, and frequencies. TDDFT energies and gradients and MP2 energies and gradients are also Linda parallel. Portions of MP2 frequency and CCSD calculations are Linda parallel.""
Only a few very large scale computations should need to use Linda.  If your code does not scale well past a small number of threads, you may be able to use multiple nodes to increase the effective number of processes.  Some jobs may also scale acceptably beyond 20 cores and so will benefit from Linda.  Linda requires that your processes be able to ssh between nodes and you must specify ssh in the Link 0 section of your description file with
```
UseSSH
```
To request permission for internode ssh, please contact us.
Linda does not utilize the high-speed Infiniband network, so it is best to use one Linda worker per node.  You specify the node list using information from Slurm, then use a cpus-per-task directive as for the multicore case above.
{{< pull-code file=""/static/scripts/gaussian_multinode.slurm"" lang=""no-highlight"" >}}
galloc: could not allocate memory
According to here:
Explanation of error
This is a memory allocation error due to lack of memory. Gaussian handles memory in such a way that it actually uses about 1GB more than %mem.
Fixing the error
The value for %mem should be at least 1GB less than the value specified in the job submission script. Conversely, the value specified for --mem in your job script should be at least 1GB greater than the amount specified in the %mem directive in your Gaussian input file. The exact increment needed seems to depend on the job type and input details; 1GB is a conservative value determined empirically.
If you encounter this error, please keep the -m value passed to g16 constant and increase the --mem value passed to #SBATCH. For example:
```
SBATCH --mem=130000
...
g16 -m=128gb ...
```
See here for a list of common Gaussian error messages."
rc-website-fork/content/userinfo/hpc/software/cromwell.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""cromwell""
softwarename = ""Cromwell""
title = ""Cromwell and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.

Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}

The Backend Configuration
In order to allow Cromwell to interact with the HPC system (via SLURM), we need to define a backend to dispatch jobs. A Cromwell configuration file, written in HOCON syntax, can be used to define the execution behavior of the pipeline and its integration with a job scheduler, in this case SLURM.
The following configuration can be used as a base, you can save it as cromwell-rivanna.conf in your home directory on the system.
```
include statement
this ensures defaults from application.conf
include required(classpath(""application""))
backend {
    default = ""SLURM""
    providers {
        SLURM {
            actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""
            config {
                run-in-background = false
                root = ""workdir""
                filesystems {
                    local {
                        localization : [""copy"", ""hard-link"", ""soft-link""]
                        caching {
                            duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]
                            hashing-strategy: ""file""
                        }
                    }                 
                }
                runtime-attributes = """"""
                    Int runtime_minutes = 600
                    Int cpu = 1
                    Int requested_memory_mb = 8000
                    String queue = ""standard""
                    String allocation = ""uvarc""
                """"""
                submit = """"""
                    sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} \
                        -t ${runtime_minutes} \
                        -p ${queue} \
                        -A ${allocation} \
                        -c ${cpu} \
                        --mem=${requested_memory_mb} \
                        --wrap ""/bin/bash ${script}""
                """"""
                job-id-regex = ""Submitted batch job (\d+).*""
                check-alive = ""squeue -j ${job_id}""
                kill = ""scancel ${job_id}""
            }
        }
    }
}
```
The include statement: The default Cromwell configuration values are set via Cromwell‚Äôs application.conf file that is part of the Cromwell installation. To ensure that you always have the defaults from the application.conf, you must include it at the top of your new configuration file.
include required(classpath(""application""))
...
Slurm backend
In our customized cromwell-rivanna.conffile, the Slurm backend is specified via the actor-factory field and should be set to ConfigBackendLifecycleActorFactory. 
backend {
    default = ""SLURM""
    providers {
        SLURM {
            actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""
            config {
                ...
            }
        }
    }
}
The config field contains custom configurations defined in the following subfields:

root: defines the working directory
filesystems: defines file copying and duplication strategies
runtime-attributes: defines job scheduler parameters
submit, job-id-regex, check-alive, kill: define key job scheduler commands 

The root field
This backend assumes that the Cromwell process and the jobs both have read/write access to the current working directory of the job.
...
            config {
                root = ""workdir""
                ...
            }
...
When Cromwell runs a workflow, it will create a directory ./workdir/<workflow_uuid>. This is called the workflow_root and it is the root directory for all activity in this workflow.
The filesystems field
This block defines the filesystem to store the directory structure and results of an executed workflow.
...
            config {
                ...
                filesystems {
                    local {
                        localization : [""copy"", ""hard-link"", ""soft-link""]
                        caching {
                            duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]
                            hashing-strategy: ""file""
                        }
                    }
                }
            }
...
Each call has its own subdirectory located at <workflow_root>/call-<call_name>. This is the <call_dir>. Any input files to a call need to be localized into the <call_dir>/inputs directory. The above stanza defines the localization strategy - copy / hard-link / soft-link, in that order, until one works.
The caching block defines Cromwell‚Äôs behavior if call caching is enabled.
The runtime-attributes field
The next code-block defines default runtime attributes for each call.
...
            config {
                ...
                runtime-attributes = """"""
                    Int runtime_minutes = 600
                    Int cpu = 1
                    Int requested_memory_mb = 8000
                    String queue = ""standard""
                    String allocation = ""MY_ALLOCATION""
                """"""
            }
...
Here, we are initializing various runtime variables: runtime_minutes, cpu, requested_memory_mb, queue, allocation with default values, i.e. we are defining our workflow environment.  You can keep the defaults but must update the allocation field with the name of your specific allocation.
{{% callout %}}
Note: The runtime attributes defined in your WDL task will override these defaults. This is useful to customize the environment for each call!
{{% /callout %}}
The submit field
The <workflow_root>/call-<call_name>/execution/ directory for each call will contain a script file, which will have the Slurm job submission command formed by the submit code-block, using the runtime attributes defined earlier.
```
...
            config {
                ...
                submit = """"""
                    sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} \
                    -t ${runtime_minutes} \
                    -p ${queue} \
                    -A ${allocation} \
                    ${""-c "" + cpu} \
                    --mem=${requested_memory_mb} \
                    --wrap ""/bin/bash ${script}""
                """"""
            job-id-regex = ""Submitted batch job (\\d+).*""
            check-alive = ""squeue -j ${job_id}""
            kill = ""scancel ${job_id}""
        }

...
```
The job-id-regex, check-alive, and  kill configuration values define how to capture the job identifier from the stdout of the submit command, how to check if the job is still running, and how to kill the job.
Running Cromwell
The path to your custom Cromwell backend configuration file is passed as -Dconfig.file command line option. To submit and run the pipeline as defined in the myWorkflow.inputs.json and myWorkflow.wdl files, execute these command:

module load {{% module-firstversion %}}

java -Dconfig.file=~/cromwell-rivanna.conf \
    -jar /path/to/cromwell-[VERSION].jar \
    run myWorkflow.wdl \
    -i myWorkflow.inputs.json 
An example of how to run a bioinformatics pipeline is documented here.
Additional Documentation
Please refer the Cromwell Backends documentation for additional details."
rc-website-fork/content/userinfo/hpc/software/rstudio.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2023-12-20T00:00:00-05:00""
tags = [
  ""lang"",
]
draft = false
shorttitle = ""RStudio Server""
title = ""RStudio Server and UVA HPC""
description = ""RStudio Server in the HPC environment""
author = ""RC Staff""
+++
Overview
RStudio Server is a web-based interface to RStudio -- a development environment for R programming.
Research Computing provides a web portal where RStudio Server can be accessed on the HPC system. However, to use RStudio Server, you must have an account on UVA HPC. Instructions for setting up an account can be found here.
Accessing RStudio Server
To access RStudio Server, you will begin by connecting to our Open OnDemand portal:

Open a web browser and go to URL:  https://ood.hpc.virginia.edu.
Use your ‚ÄúNetbadge‚Äù credentials to log in.
On the top right of the menu bar of the Open OnDemand dashboard, click on ‚ÄúInteractive Apps‚Äù.
In the drop-down box, click on ‚ÄúRStudio Server‚Äù.

Note that if you want to use your local R packages installed through the R module, you need to run
echo ""R_LIBS_USER=~/R/goolf/x.y"" > ~/.Renviron
before launching the instance. Here x.y is the major-minor version of R, e.g. 4.3. Alternatively, you may run inside RStudio:
.libPaths('~/R/goolf/x.y')
Requesting an Instance
Your instance (or copy) of RStudio will run on a compute node. So, it will need a list of resources, such as partition, time, and allocation. If you are new to UVA HPC, you may want to read the Getting Started Guide to learn more about the partitions.

After connecting to RStudio through Open OnDemand, a form will appear where you can fill in the resources for RStudio.
When done filling in the resources, click on the blue ‚ÄúLaunch‚Äù button at the bottom of the form.

It will take a few minutes for the system to gather the resources for your instance of RStudio. When the resources are ready a ‚ÄúConnect to RStudio Server‚Äù button will appear. Click on the button.
Using RStudio
When RStudio opens in your web browser, it will appear just like the RStudio that you have on your laptop or desktop.  You can use it just as you always have, including installing packages.  (If you have not used RStudio in the past, you may wish to review this tutorial.)
Closing the Interactive Session
When you are done, quit the RStudio Server application and terminate the session. The interactive session will be closed and the allocated resources will be released. If you leave the session open, your allocation will continue to be charged until the specified time limit is reached."
rc-website-fork/content/userinfo/hpc/software/workflow_managers.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2021-07-08T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
title = ""Workflow Managers and UVA HPC""
author = ""RC Staff""
+++
Workflow managers are used to create reproducible and scalable analysis pipelines. These managers are useful when you have a series of scripts that you want to tie together in the form of a pipeline. 
The most popular workflow managers on the HPC system are listed below:




Snakemake


                                Snakemake is a workflow management system written in Python. It integrates with both conda environments and singularities.
            


Cromwell


                                Cromwell is a Workflow Management System geared towards scientific workflows
            


Nextflow


                                Nextflow enables scalable and reproducible scientific workflows using software containers
            


"
rc-website-fork/content/userinfo/hpc/software/namd.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2021-05-21T00:00:00-05:00""
tags = [
  ""chem"",
  ""multi-core"",
  ""mpi"",
  ""namd""
]
draft = false
modulename = ""namd""
softwarename = ""NAMD""
title = ""NAMD and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
NAMD with MPI
The NAMD module was built on the HPC system with MPI support. Below is a Slurm script template.
{{< pull-code file=""/static/scripts/namd.slurm"" lang=""no-highlight"" >}}
You may want to benchmark it to see how well it scales for the type of job that you are running. Please refer to our tutorial on this topic."
rc-website-fork/content/userinfo/hpc/software/vasp.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2024-07-16T00:00:00-05:00""
tags = [
  ""multi-core"",
  ""chem""
]
draft = false
modulename = ""vasp""
softwarename = ""VASP""
title = ""VASP and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Local support is not available. The package is supported by its developers through their documentation site. VASP is licensed software and licenses are issued to individual research groups. Each group must build and maintain its own copy of the code.
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Building VASP
VASP is typically built with the Intel compiler and relies on Intel's Math Kernel Libraries (MKL).  VASP users should read our documentation for this compiler before beginning.  VASP version 5.4.1 and up provides a sample makefile.include.linux_intel that can be modified for local requirements and for different distributions of MPI.
We recommend that users copy makefile.include.linux_intel from the arch subdirectory to makefile.include in the top-level VASP directory, i.e.
cp arch/makefile.include.linux_intel ./makefile.include
This makefile.include is preconfigured to use the Intel compiler, IntelMPI, and the Intel MKL libraries. We recommend a few local modifications:

VASP is written primarily in Fortran and on the HPC system the compiler option -heap-arrays should be added to the makefile.include. This can be added to the FFLAGS variable, e.g. FFLAGS = -heap-arrays -assume byterecl -w
It is advisable to change the SCALAPACK library name to -lmkl_scalapack_lp64.so.

To use OpenMPI, the user must also change the Fortran compiler to FC=mpif90 and the BLACS library to -lmkl_blacs_openmpi_lp64 while leaving SCALAPACK = -lmkl_scalapack_lp64.a.
Installation details can be found on the VASP wiki: 5.x, 6.x.
Example Slurm script
To run VASP, the user prepares a group of input files with predetermined names.  The path to the vasp binary must be provided to the Slurm process manager srun; in the example below we assume it is in a directory bin under the user's home directory.  All input and potential files must be located in the same directory as the Slurm job script in this example.
{{< pull-code file=""/static/scripts/vasp.slurm"" lang=""no-highlight"" >}}
Known issues
Slow CHGCAR file write
We have received a few reports that a VASP job may occasionally appear to hang at the end during the ""writing wavefunction"" step. The slowness actually happens to CHGCAR instead of WAVECAR (the cause of which is unclear). You can disable the file write in INCAR:
LCHARG = .FALSE.
Alternatively, if you set up VASP jobs using ASE's Python package, you can disbale CHGCAR writing using:
lcharg = False
vasp_gam on AMD node
When running vasp_gam on AMD nodes (i.e. all nodes in parallel, Afton nodes in standard), ScaLAPACK must be disabled or else your job may hang at the first electronic step. In INCAR:
LSCALAPACK = .FALSE.
The ASE Python pacakge disables ScaLAPACK through the line:
lscalapack = False
Alternatively, if your job fits on 40 cores or fewer, you can choose not to disable ScaLAPACK and run it in standard with the rivanna constraint so that it will not land on an AMD node:
```
SBATCH -p standard
SBATCH -C rivanna
```
All ASE tags for the INCAR can be found in the GitHub repository for ASE's VASP calculator."
rc-website-fork/content/userinfo/hpc/software/gatk.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""gatk""
softwarename = ""GATK""
title = ""GATK and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
For a GitHub reference, visit: https://github.com/broadinstitute/gatk
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Note: Make sure to invoke GATK using the gatk wrapper script rather than calling the jar directly, because the wrapper will select the appropriate jar file (there are two!) and will set some parameters for you.
For help on using gatk itself, run 
gatk --help
To print a list of available tools, run 
gatk --list
To print help for a particular tool, run
gatk ToolName --help
General Syntax
To run a GATK tool locally, the syntax is:
gatk ToolName toolArguments 
Basic Usage Examples
Below are few trivial examples of using GATK4 tools in single-core mode.  
1. PrintReads
PrintReads is a generic utility tool for manipulating sequencing data in SAM/BAM format. 
In order to print all reads that have a mapping quality above zero in 2 input BAMs (say - input1.bam and input2.bam) and write the output to output.bam.
gatk PrintReads \
        -I input1.bam \
        -I input2.bam \
        -O output.bam \
        --read_filter MappingQualityZero 
2. HaplotypeCaller
The HaplotypeCaller is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region.
Basic syntax for variant-only calling on DNAseq.
gatk --java-options ""-Xmx4g"" HaplotypeCaller \
        -R reference.fasta \
        -I sample1.bam [-I sample2.bam ...] \
        [--dbsnp dbSNP.vcf] \ 
        [-strand_call_conf 30] \
        [-L targets.interval_list] \ 
        -o output.raw.snps.indels.vcf
Note: Here, we are setting the maximum Java heap size to 4GB. This argument varies based on the volume of data at-hand. 
Note: If you are working with human reference genome, please refer the local genome repository on Rivanna at /project/genomes/Homo_sapiens/ for the reference.fasta, as well as the corresponding GATK data bundle at /project/genomes/Homo_sapiens/GATK_bundle/, for resource files like the dbSNP, hapmap, 1000G. No need to download them to your working directory. 
For example: if you were to run HaplotypeCaller on reference-aligned BAMs for 3 samples (say - sample1-hg38.bam, sample2-hg38.bam and sample3-hg38.bam), accessing files from the Rivanna genomes repository.
gatk --java-option ""-Xmx4g"" HaplotypeCaller \
        -R /project/genomes/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.fa \
        -I sample1-hg38.bam \
        -I sample2-hg38.bam \
        -I sample3-hg38.bam \
        --dbsnp /project/genomes/Homo_sapiens/GATK_bundle/hg38/dbsnp_146.hg38.vcf.gz \ 
        -strand_call_conf 30 \
        -o output.raw.snps.indels.vcf
The output will be written to the file - output.raw.snps.indels.vcf, in the Variant Call Format.
Parallelism in GATK4
The concepts involved and their application within GATK are well explained in this article. 

In GATK3, there were two options for tools that supported multi-threading, controlled by  the arguments -nt/--num_threads and -nct/--num_cpu_threads_per_data_thread.
In GATK4, tools take advantage of an open-source industry-standard Apache Spark software library.  

Spark-enabled GATK tools
Not all GATK tools use Spark. Check the respective Tool Doc to make sure of Spark-capabilities.
Briefly; Spark is a piece of software that GATK4 uses to do multithreading, which is a form of parallelization that allows a computer (or cluster of computers) to finish executing a task sooner. You can read more about multithreading and parallelism in GATK here.
The ""sparkified"" versions have the suffix ""Spark"" at the end of their names. Many of these are still experimental; please carefully check for expected output, and validate against non-spark tools.
You DO NOT need a Spark cluster to run Spark-enabled GATK tools! 
While working on a compute node (with multiple CPU cores), the GATK engine can use Spark to create a virtual standalone cluster in place, for its multithreaded processing. 
""local""-Spark Usage Example:
The PrintReads tool we explored above has a Spark version called: PrintReadsSpark. In order to set up a local Spark environment to run the same job using 8 threads, we can use the --spark-master argument. 
gatk PrintReadsSpark \
    --spark-master local[8] \
    -I input1.bam \
    -I input2.bam \
    -O output.bam \
    --read_filter MappingQualityZero
Note: Make sure to request for 8 CPU cores before executing the above command, either by starting an interactive session using ijob or by submitting the job via a Slurm batch submission script. 
Below is an example gatk-printReadsSpark.slurm.sh batch submission script for the above job. 
{{< pull-code file=""/static/scripts/gatk.slurm"" lang=""no-highlight"" >}}
Note: replace <YOUR_ALLOCATION> with your allocation group.
To submit the job.
sbatch gatk-printReadsSpark.slurm.sh
To monitor the progress of the job.
jobq 
OR
squeue -u <mst3k> # replace <mst3k> with your computing ID."
rc-website-fork/content/userinfo/hpc/software/deeplabcut.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2021-08-11T00:00:00-05:00""
draft = false
modulename = ""deeplabcut""
softwarename = ""DeepLabCut""
title = ""DeepLabCut and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Dockerfile
We cannot use the official Docker image on the UVA HPC system because:
- the CUDA version is incompatible with our NVIDIA driver version (as of August 2021);
- at runtime it tries to download pretrained models inside the container, which is not possible via Apptainer.
For further details please visit here.
Usage
Python script
Please submit jobs to the GPU partition. A Slurm script template is provided below.
{{< pull-code file=""/static/scripts/deeplabcut.slurm"" lang=""no-highlight"" >}}
GUI
Please request a Desktop session on the GPU partition via our Open OnDemand portal. Open a terminal and load the module. Then execute:
{{< code-snippet >}}
module load apptainer deeplabcut
apptainer run --nv $CONTAINERDIR/deeplabcut-2.2.1.1-anipose.sif -m deeplabcut
{{< /code-snippet >}}"
rc-website-fork/content/userinfo/hpc/software/imageprocessing.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2019-08-26T08:37:46-05:00""
tags = [""vis""
]
draft = false
shorttitle = ""Image Processing & Visualization""
title = ""Image Processing & Scientific Visualization and UVA HPC""
description = ""Image Processing & Scientific Visualization Software in the HPC environment""
author = ""RC Staff""
+++
Available Software
To get an up-to-date list of the installed image processing and visualization tools, log on to UVA HPC and run the following command in a terminal window:
module keyword vis
To get more information about a specific module version, run the module spider command, for example:

module spider blender/2.78c


List of Image Processing and Visualization Software Modules
{{< rivanna-software moduleclasses=""vis"" >}}
Running Interactive Visualizations
Many of the provided image processing and visualization applications provide a graphical user interface (GUI). In order to use a GUI on the HPC system, users must log in through a client capable of displaying X11 graphics.  We recommend FastX Web which provides a GPU to accelerate rendering.
To start an applications GUI in an X11-enabled terminal, first load the software module and then run the GUI application executable, e.g.
module load blender
When connected to UVA HPC via FastX Web, rendering of the graphical user interface can be accelerated by executing this command:
module load blender
vglrun -c proxy blender &
The ampersand & returns the terminal to input mode while the application is running."
rc-website-fork/content/userinfo/hpc/software/cellprofiler.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""cellprofiler""
softwarename = ""CellProfiler""
title = ""CellProfiler and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
The latest version of CellProfiler is available as an Apptainer container.  Containers encapsulate applications, in this case CellProfiler, and all their required libraries isolated from the application and libraries provided by the system. The basic concepts of software containers, and Apptainer container in particular, are explained here.  We recommend using the latest CellProfiler container version whenever possible.  Please contact us for help with this package.
CellProfiler can be run interactively with a graphical user interface (GUI) or non-interactively without any user interface.  The interactive GUI mode is used to define image analysis pipelines; the non-interactive mode is used for image batch processing based on previously configured image analysis pipelines.  
Preparation
The CellProfiler container image file is provided in a shared user space.  For best performance it is recommended that users copy this container to their individual /scratch storage location.  This has to be done only once and the following steps describe this process.
In a terminal window on the HPC system execute these commands:
module load apptainer cellprofiler
cp $CONTAINERDIR/cellprofiler-4.2.5.sif /scratch/$USER
Image Pipeline Configuration
Option A: ssh terminal


In a terminal window on your local workstation execute the following command:
ssh -Y YOUR_ID@login1.hpc.virginia.edu


Continue with instructions under Starting an interactive CellProfiler job.


Please note that this option may be very slow.
Option B: Starting an interactive CellProfiler job
To start an interactive job and launch the CellProfiler graphical user interface from within the container, obtain desktop through the Open OnDemand Desktop app, start a terminal window, then run the following commands
module load apptainer cellprofiler
apptainer run $CONTAINERDIR/cellprofiler-4.2.5.sif
Non-interactive Slurm jobs for batch image processing
If you have a large number of images that all need to be processed in the same manner, you can use compute nodes for efficient non-interactive batch image processing. The details of CellProfiler's batch processing strategy are explained here.
Setup

Move image files to be processed to a directory accessible on the HPC system (ideally /scratch).
Use an interactive CellProfiler session to define a CellProfiler image analysis pipeline file (.cppipe) that defines how those particular images are to be processed,  see Interactive Jobs with Graphical User Interface for Image Pipeline Configuration.
In the interactive CellProfiler session, add the CreateBatchFiles module to the end of your pipeline and click Analyze Images. This will create the file Batch_data.h5 which defines the entire image processing step including paths to the images.

Note:  The pipeline batch file created in step 3 contains hardcoded paths to the to-be-processed image files. So steps 2 and 3 need to be repeated when you want to process images in a different directory.
Create and submit the Slurm job script
A general premise in the batch processing workflow is that processing of images can occur independently from each other in a parallel fashion.  The easiest way to implement parallel image processing with CellProfiler is to create a job array where each job in the array (referred to as job array task) processes a unique subset of the total image set.  
Let us assume that we have a directory with 100 image files to process in /scratch/$USER/images and that we completed steps 1-3 as described above.  The following two steps create the Slurm job script and submit it to the cluster:

Create/edit Slurm job script /scratch/mstk/cp_jobs/cellprofiler.slurm (see below).  This script
defines a job array with 100 tasks, each task processing a single image,
loads the cellprofiler container module and runs the CellProfiler.py script inside the container, and
passes the /scratch/$USER/pipelines/Batch_data.h5 file with the image processing definition to the CellProfiler instance
Run these commands to submit the job and execute the preconfigured image analysis pipeline.:
cd /scratch/$USER/cp_jobs
sbatch cellprofiler.slurm

The Slurm job script cellprofiler.slurm:
{{< pull-code file=""/static/scripts/cellprofiler.slurm"" lang=""no-highlight"" >}}


The directive #SBATCH --array=100 defines the size of the job array, i.e. the creation of 100 job tasks, each running a single CellProfiler instance.


The directive #SBATCH --cpus-per-task=1 specifies that each job task, i.e. each CellProfiler instance, runs on a single cpu core since CellProfiler does not support multi-threading.


SLURM_ARRAY_TASK_ID is a variable set by Slurm when the job is running. For each job array task this variable is set to a unique value between 1 and 100 (job array size). We use it to define for each job array task which image needs to be processed.

"
rc-website-fork/content/userinfo/hpc/software/lammps.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""multi-core"",
  ""chem""
]
draft = false
modulename = ""lammps""
softwarename = ""LAMMPS""
title = ""LAMMPS and UVA HPC""
author = ""RC Staff""
+++
Description
LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) is a molecular-dynamics code. The code is designed to be easy to modify or extend with new functionality. LAMMPS can run on a single core but is designed to be highly efficient running on a large number of cores in parallel using message-passing techniques and a spatial decomposition of the simulation domain.  It solves systems ranging from single atoms through polymers and proteins to rigid collections of particles.  A variety of force fields is supported.
Local support is not available.  LAMMPS has documentation and tutorials at its Website.   It has a large and active community of users; to search or join the mailing list see the instructions here.
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Users may build their own versions of LAMMPS if they wish to use a different compiler and MPI combination, or to choose individual optional packages.  Instructions are available at the LAMMPS Getting Started"" page.
Example Slurm script
To run the system version of LAMMPS, a script similar to the following can be used.  LAMMPS has many options so only the most basic is shown.
CPU
{{< pull-code file=""/static/scripts/lammps-cpu.slurm"" lang=""no-highlight"" >}}
GPU
{{< pull-code file=""/static/scripts/lammps-gpu.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/paraview.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2019-06-23T08:37:46-05:00""
tags = [
  ""lang"",
]
draft = false
shorttitle = ""ParaView""
modulename = ""paraview""
softwarename = ""ParaView""
title = ""ParaView and UVA HPC""
author = ""RC Staff""
+++
Description
{{< module-description >}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Interactive Sessions through Open Ondemand
Interactive sessions of {{% software-name %}} can be launched through the HPC web portal, Open OnDemand. To launch an instance of {{% software-name %}}, you will begin by connecting to our Open OnDemand portal. Your {{% software-name %}} session will run on a GPU node. In addition, you need to specify required resources, e.g. time, your UVA HPC allocation, etc.. If you are new to UVA HPC, you may want to read the Getting Started Guide to learn more about the partitions.
Starting an Interactive Session

Open a web browser and go to URL:  https://ood.hpc.virginia.edu.
Use your Netbadge credentials to log in. This will open the Open OnDemand web portal.
On the top banner of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on {{% software-name %}}.
After connecting to {{% software-name %}} through Open OnDemand, a form will appear where you can fill in the resources for {{% software-name %}}. {{% software-name %}} supports GPUs and should be run in the GPU partition.
When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times.
It may take a few minutes for the system to gather the resources for your instance of {{% software-name %}}. When the resources are ready a Launch {{% software-name %}} button will appear. Click on the button to start {{% software-name %}}.

Using {{% software-name %}}
When {{% software-name %}} opens in your web browser, it will appear just like the {{% software-name %}} that you have on your laptop or desktop.
Closing the Interactive Session
When you are done, quit the {{% software-name %}} application. The interactive session will be closed and the allocated resources will be released."
rc-website-fork/content/userinfo/hpc/software/cellranger.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2024-01-02T00:00:00-05:00""
draft = false
modulename = ""cellranger""
softwarename = ""Cell Ranger""
title = ""Cell Ranger and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/gpu.md,"+++
type = ""rivanna""
date = ""2023-04-14T08:37:46-05:00""
tags = [
  ""rivanna"",
  ""lmod"",
  ""software"",
  ""hpc"",
  ""amber"",
  ""reinforcement-learning"",
  ""machine-learning"",
  ""deep-learning""
]
draft = false
title = ""GPU-enabled Software and UVA HPC""
description = ""List of GPU-enabled Software Modules on the UVA HPC System""
author = ""RC Staff""
categories = [""userinfo""]
+++
{{% callout %}}
Please note that certain modules can only run on specific GPU types. This will be displayed in a message upon loading the module.
Certain software applications may also able to take advantage of the advanced capabilities provided by the NVIDIA DGX BasePOD‚Ñ¢.
Learn More ¬†¬†
{{% /callout %}}
{{< rivanna-software moduleclasses=""all"" arch=""gpu"" >}}"
rc-website-fork/content/userinfo/hpc/software/rapidsai.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2021-06-15T00:00:00-05:00""
tags = []
draft = false
shorttitle = ""RAPIDS""
modulename = ""rapidsai""
softwarename = ""RAPIDS""
title = ""RAPIDS and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/modules.md,"+++
type = ""rivanna""
date = ""2019-04-23T08:37:46-05:00""
tags = [
  ""rivanna"",
  ""lmod"",
  ""software"",
  ""hpc""
]
draft = false
title = ""Software Modules""
description = ""Software Modules""
author = ""RC Staff""
categories = [""userinfo""]
+++
The lmod modules system on the HPC system enables users to easily set their environments for selected software and to choose versions if appropriate.
The lmod system is hierarchical; not every module is available in every environment.  We provide a core environment which contains most of the software installed by Research Computing staff, but software that requires a compiler or MPI is not in that environment and a compiler must first be loaded.
{{< button button-class=""primary"" button-text=""View All Modules"" button-url=""/userinfo/hpc/software/complete-list/"" >}}

Basic Commands
List all available software
{{< code-snippet >}}
module avail
{{< /code-snippet >}}
Use key to list all modules in a particular category.  The current choices are
{{< code-snippet nobutton >}}
base, bio, cae, chem, compiler, data, debugger, devel, geo, ide, lang, lib, math, mpi, numlib, perf, phys, system, toolchain, tools, vis, licensed
{{< /code-snippet >}}
Example:
{{< code-snippet >}}
module key bio
{{< /code-snippet >}}
Load the environment for a particular package
{{< code-snippet >}}
module load thepackage
{{< /code-snippet >}}
If you do not specify a version, the system default is loaded.  For example, to load the default version of our Python distribution, run
{{< code-snippet >}}
module load miniforge
{{< /code-snippet >}}
If you do not wish to use the default version chosen by the module's environment, you must specify the version explicitly. For example, to select a version of the gcc compiler suite that is different from the default:
{{< code-snippet >}}
module load gcc/13.3.0
{{< /code-snippet >}}
Note that the 'default' version of a module may change, so if you are developing applications yourself we highly recommend that you load explicit versions of modules; that is, do not invoke the default package, but specify a version.  If the version is eventually dropped for newer versions, loading the module will fail, which will make you aware that you will need to update your application appropriately.
Remove a module
{{< code-snippet >}}
module unload thepackage
{{< /code-snippet >}}
List all modules you have loaded in the current shell
{{< code-snippet >}}
module list
{{< /code-snippet >}}
Change from one version to another
{{< code-snippet >}}
module swap
{{< /code-snippet >}}
For example, if you have loaded gcc/11.4.0 and you wish to switch to intel/2023.1
{{< code-snippet >}}
module swap gcc/11.4.0 intel/2023.1
{{< /code-snippet >}}
This will unload the gcc/11.4.0 environment entirely, and load the intel/2023.1 environment. 
Clear all modules you have loaded
module purge
Finding prerequisites
Use ""module spider"" to find all possible modules.  Once you determine the name of the software you want to use, spider will also show you all available versions.
```
module spider
module spider hdf5
```
Note that spider will sometimes show modules whose version numbers begin with a period, e.g. icc/.17.0.  Any such module is ‚Äúhidden‚Äù and users should not be required to load them explicitly.
For many applications, you will need to load one or more prerequisites. For example, to find out how to load a specific R version, run
module spider R/version
where you must replace ""version"" with the value.  At one particular time, module spider shows us that the following R versions are available:
```
module spider R

R:
Description:
  R is a free software environment for statistical computing and
  graphics.

 Versions:
    R/3.2.1
    R/3.4.4
    R/3.5.3
    R/3.6.3
    R/4.0.0

If we wish to determine how to run R 3.6.3 we run module spider for it
module spider R/3.6.3

R: R/3.6.3
Description:
  R is a free software environment for statistical computing and
  graphics.


You will need to load all module(s) on any one of the lines below before the

""R/3.6.3"" module is available to load.
  gcc/7.1.0  openmpi/3.1.4
  intel/18.0  intelmpi/18.0

```
This shows that two versions of R 3.6.3 are available, one built with the gcc compiler and one built with the Intel compiler suite.  We must choose one of those.  We'll select the gcc version.
module load gcc/7.1.0
module load openmpi/3.1.4
module load R/3.6.3
More about these commands can be found in the documentation.

Modules Best Practices
Whenever several modules are loaded at the same time, there is the potential for modules to conflict with one another. Conflicting modules can cause code dependent on these modules to not work. Here are some ways to commit to best practices when using modules:
Start with a clean slate
module purge before beginning your workflow. If you need to switch what you are doing within the current session, like changing from working in Python to R, purge and load your new modules so there is no chance for conflicts.
Know what you are loading
When loading modules, it is best to specify what version you are using instead of using the default. If you commit to using the default option each time, you may miss when our default changes and load modules that are no longer compatible with your workflow. Use module spider to see what versions of each module we offer.
Advanced Usage
If you are consistently loading the same modules on startup, you might find it convenient to load your modules using your .bashrc file. This is NOT within best practices. Interactive apps like Jupyter Labs and RStudio automatically load some modules that they are dependent on. Your .bashrc file still executes on startup within those settings, thus leading to potential conflicts.
A better way to load modules efficiently is to use a bash script. Writing a bash script that will load all your necessary modules with an aliased command falls more within best practices than filling your .bashrc file. Whenever you need to switch to a new workflow, module purge then execute your other script. Remember to change your scripts whenever we offer different versions of the modules that you use so your scripts are not out of date.

Modules in Job Scripts
After the definition of job variables, and before the command line to run the program, add module load lines for every application that you want included in your run environment.  Although it is not required, we also recommend that you clear your module environment before your job starts executing.  For example, to run R version 3.6.3 in the module environment described above, your job script should resemble the following:
```
!/bin/bash
SBATCH -p standard
SBATCH -A MyAcct
SBATCH -n 1
SBATCH --time=00:10:00
SBATCH --mem-per-cpu=4000
module purge
module load gcc/7.1.0
module load openmpi/3.1.4
module load R/3.6.3
Rscript myScript.R
```
Please contact us if you encounter any problems using these applications in your job scripts.

Creating Your Own Modules
If you install your own software, you can create your own modules stored in your home directories.  This is not necessary; you can also add the path to your .bashrc file or write a script or just provide the path.  But it may be convenient to have a custom module.
The first step is to create a directory to store the modulefiles used by lmod.
{{< code-snippet >}}
mkdir $HOME/modulefiles
{{< /code-snippet >}}
Next, download the software package for which you would like to create a module. Carefully follow the instructions to install your software.  Remember that most packages assume you can install as an administrator, and that is not permitted on the HPC system, so you must change your installation directory.  If you are compiling software you can find a tutorial at our training site.
In the following example we are installing a version of git according to its instructions:
$ wget https://www.kernel.org/pub/software/scm/git/git-2.6.2.tar.gz
$ tar xf git-2.6.2.tar.gz
$ cd git-2.6.2
$ ./configure --prefix=$HOME/git/2.6.2
$ make
$ make install
Once the software has been installed, a modulefile (written in the Lua programming language) must be created. The modulefile should be placed in a subdirectory named after the software and the file should be named using the version number of the software. Any text editor of your choice may be used.  The modulefile in this example adds ~/git/2.6.2/bin to the user's path so that the personal version of git can be found.
$ cd ~/modulefiles
$ mkdir git
$ cd git
Use an editor to create the following file and name it 2.6.2.lua
local home    = os.getenv(""HOME"")
local version = myModuleVersion()
local pkgName = myModuleName()
local pkg     = pathJoin(home,pkgName,version,""bin"")
prepend_path(""PATH"", pkg)
The first line reads the user‚Äôs HOME directory from the environment. The second line uses the ‚ÄúpathJoin‚Äù function provided from Lmod. It joins strings together with the appropriate number of ‚Äú/‚Äù. The last line calls the ‚Äúprepend_path‚Äù function to add the path to this version of git at the beginning of the user‚Äôs path.
Finally, to use the module:
$ module use $HOME/modulefiles
$ module load git
$ which git
~/git/2.6.2/bin/git

Finding Modules with the Same Name
If the user has created a module and at a later date the system provides a newer version, the default behavior if no version is specified to the module load command is to use the highest version available in all module paths searched.
```
$ module avail git
----------------------------- /home/mst3k/modules ------------------------------
git/2.6.2
--------------- /apps/modulefiles/standard/core ------------------------
git/1.7.4   git/2.0.1   git/3.5.4 (D)
$ module load git
```
The module load command will load git/3.5.4 because it is the highest version.
More about user created modules can be found in the documentation."
rc-website-fork/content/userinfo/hpc/software/bwa.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""bwa""
softwarename = ""BWA""
title = ""BWA and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

BWA provides three alignment algorithms:

BWA-backtrack
BWA-SW
BWA-MEM

The BWA-backtrack algorithm is exclusively used for short sequence reads up to 100bp, the latter two can be used for sequence reads of up to 1MB.  The BWA-MEM algorithm can also be used for high-quality short Illumina sequence reads (< 100bp) in many cases with better performance compared to the original BWA-backtrack algorithm.  Therefore, the more universal BWA-MEM algorithm is recommended as a starting point for most alignment scenarios.
Before any of the alignment algorithms can be used, a FM-index needs to be constructed for the reference genome (see below).

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
For a GitHub reference, visit: https://github.com/lh3/bwa
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Slurm Script Examples
Creating a BWA Index for a Reference Genome
Index files are created with the bwa index command. A reference genome sequence in FASTA format needs to be provided, e.g. /scratch/$USER/bwaanalysis/refgenome.fa
{{< pull-code file=""/static/scripts/bwa.slurm"" lang=""no-highlight"" >}}
Alignment of Sequence Reads to a Reference Genome
BWA provides three basic alignment algorithms to align sequence reads to a reference genome, BWA-backtrack, BWA-SW, and BWA-MEM.  Below we show an example for using the BWA-MEM algorithm (command bwa mem), which can process short Illumina reads (70bp) as well as longer reads up to 1 MB.  The alignment output is saved in SAM file format.  The use of SAMtools on the HPC system is documented here.  
Specification of files

Reference genome file: /scratch/$USER/bwaanalysis/refgenome.fa
Sequence read file 1: /scratch/$USER/bwaanalysis/read1.fq
Sequence read file 2: /scratch/$USER/bwaanalysis/read2.fq
Output Alignment (SAM file): /scratch/$USER/bwaanalysis/aln-pe.sam

```
!/bin/bash
SBATCH -A YOUR_ALLOCATION
SBATCH --nodes=1
SBATCH --ntasks=1
SBATCH --cpus-per-task=16
SBATCH --mem-per-cpu=9000
SBATCH -p standard
Run program
module purge
module load bwa
module list
cd /scratch/$USER/bwaanalysis
using paired-ends reads from two .fq sequence files
bwa mem refgenome.fa read1.fq read2.fq -t $SLURM_CPUS_PER_TASK > aln-pe.sam
```
Notes:

The use of -t $SLURM_CPUS_PER_TASK to define the numbe of processing threads based on the numbe of requested cpu core (1 thread / cpu core). Follow the online {{% software-name %}} documentation to adjust parameters for aligning single-end reads.
The use of --mem and --mem-per-cpu options are mutually exclusive. Job scripts should specify one or the other but not both.

Using an Interactive Session to run BWA
You should NOT do your computational processing on the head node. In order to obtain a login shell on a compute node, use the ijob command. 
ijob -N 1 -n 1 -A <YOUR_ALLOCATION> -p standard -c 20 --mem=20000
Replace <YOUR_ALLOCATION> with your account name to charge SUs. The arguments for -c and --mem options depend on the resources you will use for the alignment step. For more details about submitting interactive jobs please see here.
Load module
First, let us load the bwa module:
module load bwa
In order to check all available bwa commands run: 
bwa
If you wish to check various options for each command run: 
bwa index
bwa mem"
rc-website-fork/content/userinfo/hpc/software/blast.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
  ""bioinformatics"",
]
draft = false
modulename = ""blast""
softwarename = ""NCBI Blast""
title = ""NCBI Blast and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/julia.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""math""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
  ""programming"",
]
draft = false
modulename = ""julia""
softwarename = ""julia""
title = ""Julia and UVA HPC""
author = ""RC Staff""
+++
Description
Julia is a high-level programming language designed for high-performance numerical analysis and computational science. Distinctive aspects of Julia's design include a type system with parametric polymorphism and types in a fully dynamic programming language and multiple dispatch as its core programming paradigm. It allows concurrent, parallel and distributed computing, and direct calling of C and Fortran libraries without glue code. A just-in-time compiler that is referred to as ""just-ahead-of-time"" in the Julia community is used. [Ref: Wikipedia](https://en.wikipedia.org/wiki/
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Installing Julia Packages
Julia wants to update any existing packages whenever a user tries to add a package. Of course, the basic packages were installed in a system directory that is not writable by the users.
One work-around is to force Julia to use your local directory the first time that you add a package. For example, from the Linux command line set the shell variable,
export JULIA_DEPOT_PATH=""~/.julia""
Then when you add a package from within Julia, it will be added to the .julia folder in your home directory, e.g.
After that first time, it should always default to /home/$USER/.julia .
The following link is a useful reference for loading Julia packages.
Loading Packages
The following code snippet shows the steps used on my UVA HPC account to install and verify the SharedArrays package.
``
udc-ba34-36-gahlmann$module load julia
udc-ba34-36-gahlmann$julia
               _
   _       _ _(_)_     |  Documentation: https://docs.julialang.org
  (_)     | (_) (_)    |
   _ _   _| |_  __ _   |  Type ""?"" for help, ""]?"" for Pkg help.
  | | | | | | |/ _ |  |
  | | || | | | (| |  |  Version 1.6.0 (2021-03-24)
 / |_'|||_'_|  |  Official https://julialang.org/ release
|__/                   |
julia> using Pkg
(v1.6) pkg> status
      Status /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Project.toml
  [91a5bcdd] Plots v1.19.4
  [8ba89e20] Distributed
  [de0858da] Printf
julia>
help?> sdata
search: isdirpath isdispatchtuple StridedMatrix StridedVecOrMat searchsortedlast
Couldn't find sdata
Perhaps you meant stat, sort, sort!, sqrt, ispath, lstat, edit, Meta or atan
  No documentation found.
Binding sdata does not exist.
(v1.6) pkg> add SharedArrays
    Updating registry at ~/.julia/registries/General
   Resolving package versions...
    Updating /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Project.toml
  [1a1011a3] + SharedArrays
  No Changes to /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Manifest.toml
(v1.6) pkg> status
      Status /sfs/qumulo/qhome/teh1m/.julia/environments/v1.6/Project.toml
  [91a5bcdd] Plots v1.19.4
  [8ba89e20] Distributed
  [de0858da] Printf
  [1a1011a3] SharedArrays
julia> using SharedArrays
help?> sdata
search: sdata isdirpath isdispatchtuple SharedMatrix StridedMatrix
sdata(S::SharedArray)
Returns the actual Array object backing S.
julia>
```
You can work with Julia on the UVA HPC frontend nodes; we recommend FastX for this application. 
Interactive Sessions through the Open OnDemand Web Portal
Starting an Interactive Session
To launch an instance of {{% software-name %}} through a notebook interface, you will begin by connecting to our Open OnDemand portal. You need to specify required resources, e.g. node partition, time, your UVA HPC allocation, etc. If you are new to HPC, you may want to read the Getting Started Guide to learn more about the partitions.

Open a web browser and go to URL:  https://ood.hpc.virginia.edu.
Use your Netbadge credentials to log in. This will open the Open OnDemand web portal.
On the top banner of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on JupyterLab.
After connecting to JupyterLab`` through Open OnDemand, a form will appear where you can fill in the resources forJupyterLab`.
When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times.
It may take a few minutes for the system to gather the resources for your instance of JupyterLab. When the resources are ready a Connect to JupyterLab button will appear. Click on the button to start JupyterLab.

Using JupyterLab
When JupyterLab opens in your web browser, it will appear with a selection of notebook kernels to choose from, as shown below.

If you double-click on one of the Julia kernels, an IPython notebook will open connected to Julia, ready for interactive commands.
Closing the Interactive Session
When you are done, quit the JupyterLab application.  The interactive session will be closed and the allocated resources will be released. If you leave the session open, your allocation will continue to be charged until the specified time limit is reached.
Running a Julia Batch Jobs on the HPC System
The UVA HPC system uses the Slurm resource manager to schedule and run jobs on the cluster compute nodes. The following are example Slurm scripts for submitting different types of Julia batch jobs to the cluster.
Submitting a batch job using a single core of a compute node.
Once your program is debugged, we recommend running in batch mode when possible. This runs the job in the background on a compute node. Write a Slurm script similar to the following:
{{< pull-code file=""/static/scripts/julia_serial.slurm"" lang=""no-highlight"" >}}
The simple example code hello.jl is shown below.
msg=""hello world""
println(msg)
Submitting a batch job using multiple cores on a compute node
The Distributed package can be used to run Julia code across multiple cores of a compute node. The Slurm script in this case would look like the following:
{{< pull-code file=""/static/scripts/julia_serial.slurm"" lang=""no-highlight"" >}}
The Julia code in this case is,
```
using Distributed
launch worker processes
num_cores = parse(Int, ENV[""SLURM_CPUS_PER_TASK""])
addprocs(num_cores)
println(""Number of cores: "", nprocs())
println(""Number of workers: "", nworkers())
each worker gets its id, process id and hostname
for i in workers()
    id, pid, host = fetch(@spawnat i (myid(), getpid(), gethostname()))
    println(id, "" "" , pid, "" "", host)
end
remove the workers
for i in workers()
    rmprocs(i)
end
and the output is,
Number of cores: 9
Number of workers: 8
2 11643 udc-ba26-19
3 11644 udc-ba26-19
4 11645 udc-ba26-19
5 11646 udc-ba26-19
6 11649 udc-ba26-19
7 11650 udc-ba26-19
8 11651 udc-ba26-19
9 11652 udc-ba26-19
```
Documentation on distributed computing with Julia can be accessed at the URL
Julia Jobs using Slurm Job Arrays
Slurm has a mechanism for launching multiple independent jobs with one
job script using the --array directive.
Array of Multiple Single-Core Julia Jobs
The following slurm script shows how to run 5 single core Julia jobs using
Slurm job arrays.
{{< pull-code file=""/static/scripts/julia_serial.slurm"" lang=""no-highlight"" >}}
The jobArray.jl code can use the SLURM_ARRAY_TASK_ID shell variable assigned by
Slurm for indexing input file.
using Distributed
num_replication = Base.parse(Int, ENV[""SLURM_ARRAY_TASK_ID""])
@everywhere println(""Job array task id: "", num_replication, "" on host $(gethostname())"")
The Slurm script will produce 5 separate output files, each of the form
Job array task id: 4 on host udc-ba25-33c0
Parallel Julia on Multiple Compute Nodes
To run Julia parallel jobs that require more cores than are available on one compute node (e.g. > 40), please use the system MPI libraries. You cannot use the aforementioned Distributed package since it requires SSH permission into compute nodes.
{{< pull-code file=""/static/scripts/julia_mpi.slurm"" lang=""no-highlight"" >}}
This involves importing the Julia MPI module:
```
import MPI
MPI.Init()
comm = MPI.COMM_WORLD
my_rank = MPI.Comm_rank(comm)
comm_size = MPI.Comm_size(comm)
println(""Hello! I am "", my_rank, "" of "", comm_size, "" on "",gethostname())
MPI.Finalize()
```
To take advantage of the fast network interfaces between compute node, use the
system-provided MPI implementations. See the documentation on using a system-provided MPI.
The output of this program should look like,
Hello! I am 6 of 8 on udc-ba34-16c4
Hello! I am 1 of 8 on udc-ba34-10c9
Hello! I am 3 of 8 on udc-ba34-16c1
Hello! I am 5 of 8 on udc-ba34-16c3
Hello! I am 7 of 8 on udc-ba34-16c5
Hello! I am 2 of 8 on udc-ba34-16c0
Hello! I am 4 of 8 on udc-ba34-16c2
Hello! I am 0 of 8 on udc-ba34-10c8
Utilizing GPUs with Julia
General guidelines on requesting GPUs on UVA HPC
The following slurm script is for submitting a Julia job that uses 1 GPU. For each GPU requested, the script requests one cpu (ntasks-per-node). The article An Introduction to GPU Programming in Julia provides more details to get started.
{{< pull-code file=""/static/scripts/julia_gpu.slurm"" lang=""no-highlight"" >}}
The Julia code is
```
using Flux, CuArrays
z = CuArrays.cu([1, 2, 3])
println(2 * z)
m = Dense(10,5) |> gpu
x = rand(10) |> gpu
println(m(x))
and the output is
slurm allocates gpus  4
[2, 4, 6]
Float32[0.6239201, -0.36249122, -1.1242702, -0.9278027, 0.004131808]
```"
rc-website-fork/content/userinfo/hpc/software/jupyterlab.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""lang"",
]
draft = false
shorttitle = ""Jupyter Lab""
title = ""Jupyter Lab and UVA HPC""
description = ""Jupyter Lab in the HPC environment""
author = ""RC Staff""
+++
Overview
Jupyter Notebooks are documents which can combine executable code, formatted text, and interactive graphics into a single file.  Because Notebooks can be shared, they provide developers with a tool for capturing and explaining their computational results.  To use a Jupyter Notebook, a web application, such as JupyterLab, is needed.
We now provide a web portal where JupyterLab can be accessed on Rivanna and Afton.
However, to use JupyterLab, you must have an account on UVA HPC.
Accessing JupyterLab
To access JupyterLab, you will begin by connecting to our Open OnDemand portal:

Open a web browser and go to https://ood.hpc.virginia.edu.
Use your Netbadge credentials to log in.
On the top right of the menu bar of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on JupyterLab.


Requesting an Instance
Your instance (or copy) of JupyterLab will run on a compute node. So it will need a list of resources, such as partition, time, and allocation. If you are new to UVA HPC, you may want to read the HPC User Guide to learn more about the partitions.

After connecting to JupyterLab through Open OnDemand, a form will appear where you can fill in the resources for JupyterLab.
When done filling in the resources, click on the blue ‚ÄúLaunch‚Äù button at the bottom of the form.
It will take a few minutes for the system to gather the resources for your instance of JupyterLab.  When the resources are ready a Connect to Jupyter button will appear. Click on the button and the Notebook session will open in a new tab.

The following screenshots illustrate the sequeunce of steps decribed above.




Note that under the Work Directory field, the drop-down menu allows you to choose
either your HOME or SCRATCH directories.


Running Notebooks
The JupyterLab dashboard will have two panes:

A file browser pane on the left where you will see the files and folders in your HPC directory; and
A Launcher pane on the right with tiles for the available kernels (i.e., underlying software that will run the code in your Notebooks).


If you already have a Jupyter Notebook in your account, you can maneuver to the file in the file-browser pane, and double-click on the file name to open the Notebook.
However, if you want to create a new Notebook, go to the Launcher pane and click on the tile for the desired kernel (e.g., Python 3).
If you are more familiar with the classic Notebook environment, you can change the JupyterLab format by clicking on Help and select Launch Classic Notebook.

Extensions
We provide the following JupyterLab extensions:

plotly is an interactive plotting library
jupyterlab-dash renders Plotly Dash apps as a separate window in JupyterLab
jupyterlab-toc auto-generates a table of contents in the left area
voila allows you to convert a Jupyter Notebook into an interactive dashboard

FAQ
How to create custom JupyterLab kernels?
Please refer to our how-to.
How do I load a module in JupyterLab?
That is not possible if you request a JupyterLab session. Please refer to our how-to for a workaround."
rc-website-fork/content/userinfo/hpc/software/orca.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2021-05-21T00:00:00-05:00""
tags = [
  ""chem"",
  ""computational-chemistry"",
  ""multi-core"",
  ""mpi"",
  ""dft"",
]
draft = false
modulename = ""orca""
softwarename = ""ORCA""
title = ""ORCA and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
License and Permission
We have an ""ACADEMIA"" license. Usage is restricted to academic purposes. Please contact us if you need access to the software.
Slurm script template
Below is a Slurm script template. Please note that:

Loading the orca module will automatically load gcc and openmpi. Do not load the latter manually.
For your convenience we have defined the environment variable orcadir in the module.
The full path to the executable orca is thus $orcadir/orca.
Do not use srun/mpirun. The software will take care of MPI.

{{< pull-code file=""/static/scripts/orca_serial.slurm"" lang=""no-highlight"" >}}
Submit the job in the same directory as my.inp.
Multi-node
For larger calculations, you may run on multiple nodes. The following example will run on a total of 120 cores:
{{< pull-code file=""/static/scripts/orca_multinode.slurm"" lang=""no-highlight"" >}}
Important notes:
- The nprocs in *.inp should be equal to the total number of cores requested in your Slurm script.
- Do not run multi-node ORCA jobs from research standard storage /nv. (See here.) We recommend scratch.
References
For more information please visit:

ORCA Forum. You will need to create an account.
ORCA Input Library
"
rc-website-fork/content/userinfo/hpc/software/bioinformatics.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
  ""bioinformatics""
]
date = ""2019-06-23T08:37:46-05:00""
tags = [""bio"",""bioinformatics"",""computational-biology"",""docking"",""rosetta""
]
draft = false
shorttitle = ""Bioinformatics""
title = ""Bioinformatics and UVA HPC""
description = ""Bioinformatics Software in the HPC environment""
author = ""RC Staff""
+++
Overview
Many commonly used bioinformatics software packages on the HPC clusters are available as individual modules or as Python packages bundled in the bioconda modules. 
Please see our HowTo for more information about using this software on the HPC system.
Software Availability
If a particular package is not available, several options are available.  If it is sufficiently widely used, Research Computing staff will install it as a new module.  If we determine that it is too specialized, you can install it yourself. Please use permanent storage such as your home directory to install software.  If you have difficulty we can assist you to install the package.
Please see below for a full listing of available bioinformatics software.  If you do not find it there, please check the bioconda package before requesting that we install it.
Reference Genomes
Research Computing makes some standard reference genomes available. For a listing and information about how to copy them, please see our HowTo.
Full List of Bioinformatics Software Modules
Below is a list of software installed as separate modules. Other packages that are based on Python are available in the bioconda environment.  Please see the bioconda page for a listing of those packages.
{{< rivanna-software moduleclasses=""bio"" >}}"
rc-website-fork/content/userinfo/hpc/software/alphafold.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2025-05-01T00:00:00-05:00""
tags = [
  ""bio"",
  ""multi-core"",
  ""gpu""
]
draft = false
modulename = ""alphafold""
softwarename = ""AlphaFold""
title = ""AlphaFold and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions, run:
module spider {{< module-name >}}
For detailed information about a particular version, including the load command, run module spider <name/version>. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
AlphaFold 3
Model Parameters
The AlphaFold 3 model parameters are subject to the Terms of Use defined here. Our module does not contain the model parameters; instead, each user must submit their own request to DeepMind. Visit here for further instructions.
Upon approval you will receive a download url for the file af3.bin.zst (~1 GB). Place it in a directory that is not shared with others, e.g. ~/af3.
bash
DIR=~/af3
mkdir $DIR
cd $DIR
wget <your_download_url>
unzstd af3.bin.zst
The last command will extract the file into af3.bin.
Slurm Script
{{< pull-code file=""/static/scripts/alphafold3.slurm"" lang=""no-highlight"" >}}
If you put the model parameters in a different location, change the value of --model_dir. To see the complete list of flags run:
python $EBROOTALPHAFOLD/app/alphafold/run_alphafold.py --help
Refer to the official documentation for more information.
AlphaFold 2
Installation details
We prepared a Docker image based on the official Dockerfile with some modifications. 

AlphaFold does not use TensorFlow on the GPU (instead it uses JAX). See issue. Changed tensorflow to tensorflow-cpu.
There is no need to have system CUDA libraries since they are already included in the conda environment.
Switched to micromamba instead of Miniconda.

With a three-stage build, our Docker image is only 5.4 GB on disk (2.1 GB compressed on DockerHub), almost half the size using the official Dockerfile (10.1 GB).
For further details see here.
AlphaFold launch command
Please refer to run_alphafold.py for all available options.
Launch script run
For your convenience, we have prepared a launch script run that takes care of the Apptainer command and the database paths, since these are unlikely to change. If you do need to customize anything please use the full Apptainer command.
Explanation of Apptainer flags

The database and models are stored in $ALPHAFOLD_DATA_PATH.
A cache file ld.so.cache will be written to /etc, which is not allowed on the HPC system. The workaround is to bind-mount e.g. the current working directory to /etc inside the container. [-B .:/etc]
You must launch AlphaFold from /app/alphafold inside the container due to this issue. [--pwd /app/alphafold]
The --nv flag enables GPU support.

Explanation of AlphaFold flags

The default command of the container is /app/run_alphafold.sh.
As a consequence of the Apptainer --pwd flag, the fasta and output paths must be full paths (e.g. /scratch/$USER/mydir, not relative paths (e.g. ./mydir). You may use $PWD as demonstrated.
The max_template_date is of the form YYYY-MM-DD.
Only the database paths in mark_flags_as_required of run_alphafold.py are included because the optional paths depend on db_preset (full_dbs or reduced_dbs) and model_preset.

Slurm Script
Below are some Slurm script templates for version 2.3.
Monomer with full_dbs
{{< pull-code file=""/static/scripts/alphafold_monomer.slurm"" lang=""no-highlight"" >}}
Multimer with reduced_dbs
{{< pull-code file=""/static/scripts/alphafold_multimer.slurm"" lang=""no-highlight"" >}}
Notes


For users running large protein jobs: Version 2.3.2-dev is based on commit 020cd6d, about 2 years after the official 2.3.2 release. The reason for using a development version is that the package requirements are updated for compatibility on the H200 GPU. Users who have experienced out-of-memory errors for large protein calculations should request an H200 GPU (--gres=gpu:h200) and load the 2.3.2-dev version.


Before upgrading to a newer version, please always check the official repo for details, especially on any changes to the parameters, databases, and flags.


You may need to request 8 CPU cores due to this line printed in the output:
    Launching subprocess ""/usr/bin/jackhmmer -o /dev/null -A /tmp/tmpys2ocad8/output.sto --noali --F1 0.0005 --F2 5e-05 --F3 5e-07 --incE 0.0001 -E 0.0001 --cpu 8 -N 1 ./seq.fasta /share/resources/data/alphafold/mgnify/mgy_clusters.fa""

You must provide a value for --max_template_date. If you are predicting the structure of a protein that is already in PDB and you wish to avoid using it as a template, then max_template_date must be set to be before the release date of the structure. If you do not need to specify a date, by default you can set today‚Äôs date. For example, if you are running the simulation on August 7th 2021, set -‚Äìmax_template_date = 2021-08-07. See here.
You are not required to use the run wrapper script. You can always provide the full apptainer command.
"
rc-website-fork/content/userinfo/hpc/software/bioconda.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
  ""bioinformatics""
]
date = ""2023-12-15T00:00:00-05:00""
tags = [""bio""
]
draft = false
modulename = ""bioconda""
softwarename = ""Bioconda""
shorttitle = ""Bioconda""
title = ""The Bioconda Environment and UVA HPC""
description = ""Bioconda Software in the HPC environment""
author = ""RC Staff""
+++
Bioconda Python packages
Many bioinformatics Python packages are now maintained and available for the popular Anaconda Python distribution. Python packages for the Anaconda distribution are distributed through a variety of different bundles, called channels. The bioconda channel is specifically set up for the maintenance and distribution of popular bioinformatics packages.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
To view an up-to-date list of the Python packages provided by a particular bioconda module, load the bioconda module and run the conda list command. For example:
module load bioconda
conda list | grep bioconda
The environment contains a large number of packages, most of them to support the software of interest in bioinformatics. The grep command filters the Python package list to only show the Bioconda channel packages. The output may look like this:
```
packages in environment at /apps/software/standard/core/bioconda/py3.10:
bcftools                  1.17                 h3cc50cf_1    bioconda
cyvcf2                    0.30.22         py310hcf1fb4a_0    bioconda
deeptools                 3.5.2              pyhdfd78af_1    bioconda
deeptoolsintervals        0.1.9           py310h8472f5a_5    bioconda
entrez-direct             16.2                 he881be0_1    bioconda
hisat2                    2.2.1                hdbdd923_6    bioconda
hmmer                     3.3.2                hdbdd923_4    bioconda
homer                     4.11            pl5262h9f5acd7_8    bioconda
htseq                     2.0.3           py310h5aa3a86_1    bioconda
htslib                    1.17                 h81da01d_2    bioconda
k8                        0.2.5                hdcf5f25_4    bioconda
kallisto                  0.50.0               hc877fd6_0    bioconda
minimap2                  2.26                 he4a0461_1    bioconda
multiqc                   1.15               pyhdfd78af_0    bioconda
munkres                   1.0.7                      py_1    bioconda
ngmlr                     0.2.7                hdcf5f25_6    bioconda
nseg                      1.0.1                h031d066_4    bioconda
perl-...
py2bit                    0.3.0           py310h4b81fae_8    bioconda
pybigwig                  0.3.22          py310h79000e5_1    bioconda
pysam                     0.21.0          py310h41dec4a_1    bioconda
pyspoa                    0.0.10          py310h0dbaff4_0    bioconda
python-edlib              1.3.9           py310h0dbaff4_4    bioconda
recon                     1.08                 h031d066_6    bioconda
repeatmasker              4.1.5           pl5321hdfd78af_0    bioconda
repeatmodeler             1.0.8                         0    bioconda
repeatscout               1.0.6                hec16e2b_3    bioconda
rmblast                   2.14.0               h4565617_2    bioconda
salmon                    1.10.2               hecfa306_0    bioconda
samtools                  1.17                 hd87286a_1    bioconda
stringtie                 2.2.1                h6b7c446_4    bioconda
subread                   2.0.6                he4a0461_0    bioconda
svim                      2.0.0              pyhdfd78af_0    bioconda
trf                       4.09.1               h031d066_4    bioconda
twobitreader              3.1.7              pyh864c0ab_1    bioconda
```"
rc-website-fork/content/userinfo/hpc/software/sagemath.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""math""
]
date = ""Wed May  6 10:04:55 EDT 2020""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""sagemath""
softwarename = ""SageMath""
title = ""SageMath and UVA HPC""
author = ""RC Staff""
+++
Description
SageMath (previously Sage or SAGE, ""System for Algebra and Geometry Experimentation""[3]) is a computer algebra system with features covering many aspects of mathematics, including algebra, combinatorics, graph theory, numerical analysis, number theory, calculus and statistics. Ref: wikipedia.org
SageMath is a free open-source mathematics software system licensed under the GPL. It builds on top of many existing open-source packages: NumPy, SciPy, matplotlib, Sympy, Maxima, GAP, FLINT, R and many more. Access their combined power through a common, Python-based language or directly via interfaces or wrappers.
Its mission: Creating a viable free open source alternative to Magma, Maple, Mathematica and Matlab.  Ref: sagemath.org
Available Versions
To find the available versions and learn how to load them, run:
module spider sagemath/9.0
The sagemath software provides its own Jupyter notebook. To start sagemath, go to
fastx.hpc.virginia.edu
and select FastX Web. This will open a desktop environment. Then click the terminal icon in the top toolbar and enter:
module load apptainer sagemath
Read the on-screen instructions carefully to see how to start the Jupyter session in a browser.

Then after executing the command to start the sagemath Jupyter notebook, you should see

If you select the URL in the bottom line and right-click to select 'Open Link', a browser will open up to the following page:
"
rc-website-fork/content/userinfo/hpc/software/clara-parabricks.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
  ""bioinformatics""
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""lang"",
  ""programming"",
]
draft = false
modulename = ""clara-parabricks""
softwarename = ""clara-parabricks""
shorttitle = ""Clara Parabricks""
title = ""Nvidia Clara Parabricks and UVA HPC""
description = ""GPU-enabled bioinformatics and UVA HPC""
author = ""RC Staff""
+++
Overview
Nvidia Clara Parabricks is a GPU-accelerated software suite for performing secondary analysis of next generation sequencing (NGS) DNA and RNA data. It contains GPU-enabled versions of popular bioinformatics tools such as the aligners BWA-Mem and STAR.
Loading the container
On the HPC system, Clara Parabricks is available as an Apptainer container.  To load the clara-parabricks container module, you can type:
module load apptainer clara-parabricks
The load command will load a default version of Clara Parabricks, unless another version is specified.  To see the available versions, type:
module spider clara-parabricks
Running Clara Parabricks tools
The Clara Parabricks container on the HPC system includes many bioinformatics tools for genomics and transcriptomics. Each tool must be accessed using the Apptainer run command to activate the container, followed by the Clara Parabricks pbrun command to call the designated tool, followed by arguments specific to each tool. See below for an example using the fq2bam pipeline tool, which does a BWA-Mem alignment, sorts reads by coordinates, marks duplicate reads with GATK MarkDuplicates, and optionally generates a BQSR report. 
{{< pull-code file=""/static/scripts/parabricks_fq2bam.slurm"" lang=""no-highlight"" >}}
Notes on fq2bam Slurm script:

Replace <allocation> with your allocation name.
The apptainer flag --nv enables Nvidia GPU support inside the container.
The apptainer flag -B binds a directory into the container. 
In this case, we are binding the present working directory ($PWD) into both /workdir and /outputdir inside the container.


The variable $CONTAINERDIR is defined by the container module - you do not need to assign it a value. This line in the script points the apptainer run command to the appropriate .sif file to call the desired container.
The pbrun command tells Clara Parabricks you want to run the subsequent tool (in this case, fq2bam).
The arguments following pbrun fq2bam are specific to the Clara Parabricks tool being used. See the fq2bam reference for more detailed information on these arguments.
In this case, the reference fasta file (Homo_sapiens_assembly38.fasta) and fastq data files (sample_1.fq.gz and sample_2.fq.gz) were downloaded ahead of time and stored in the referenced subdirectories. You should change these paths and file names as needed to point to your specific reference fasta and data files.


This script should be saved in a file, called (for example) job.slurm.  To run your job, you would submit the script by typing sbatch job.slurm.
"
rc-website-fork/content/userinfo/hpc/software/fiji.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""viz""
]
date = ""2024-04-09T00:00:00-05:00""
tags = [
  ""image processing"",
]
draft = false
modulename = ""fiji""
softwarename = ""Fiji""
title = ""Image Processing with Fiji and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Interactive Use of Fiji via FastX
We recommend to launch the Graphical User Interface (GUI) of Fiji as an interactive job via the Open OnDemand Desktop interactive app.  You may request a core count and amount of memory through the text boxes on the Open OnDemand form.  Be sure to supply your allocation account where requested.
Once the Desktop is launched, open a terminal window.  Load the fiji module and start the application: 
module load fiji
ImageJ-linux64 --mem=32G &
Run a Fiji script as Slurm Job
To execute a Fiji script non-interactively on a compute node, you can use the following Slurm job script template.
{{< pull-code file=""/static/scripts/fiji.slurm"" lang=""no-highlight"" >}}


Adjust the --cpus-per-task, --mem and --time options as needed. Note that not all built-in Fiji functions or Fiji scripts are designed to utilize multiple cpu cores.


Replace <YOUR_ALLOCATION> with your allocation account.


Replace <FIJI_SCRIPT> and <SCRIPT_ARGS> with your custom Fiji script and add script arguments as required by the particular Fiji script.


Custom Plugins
Users can install their own plugins in their home directory. First create the directory via
bash
mkdir ~/.plugins
Then follow the instructions here, replacing the destination with your local plugin directory."
rc-website-fork/content/userinfo/hpc/software/r.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2023-12-20T00:00:00-05:00""
tags = [
  ""lang"",
  ""programming"",
]
draft = false
modulename = ""R""
softwarename = ""R""
shorttitle = ""R & RStudio""
title = ""R and RStudio and UVA HPC""
description = ""R and RStudio in the HPC environment""
author = ""RC Staff""
+++
Overview
R is a programming language that often is used for data analytics, statistical programming, and graphical visualization.
Loading the R module
On the UVA HPC system, R is available through our module system.¬† For example, to load R, you can type:
module load goolf R
Notice that we included goolf in the load command. There are two reasons why including goolf is important:


R was built with a compiler, an interface to OpenMPI, and other utilities.  The goolf module will ensure that each of these items is loaded.


R has many computationally-intensive packages that are built with C, C++, or Fortran. By including goolf, we ensure¬†that the same environment used for building R is loaded for any package installs.


The load command will load a default version of R, unless another version is specified.
To see the available versions of R, type:
module spider R
Loading the RStudio module
RStudio is a development environment for R. We recommend launching RStudio through our web-based portal to the HPC system. For instructions on how to access it, see RStudio Server on the HPC system.
For users who must launch RStudio from the commandline, start up a FastX or Open OnDemand Desktop session and run rstudio-launcher in the terminal. Then follow the instructions.
To use your local R packages in RStudio, run:
echo ""R_LIBS_USER=~/R/goolf/x.y"" > ~/.Renviron
where x.y is the major-minor version, e.g. 4.3.
Installing packages
Due to the amount and variability of packages available for R, Research Computing does not maintain R packages beyond the very basic.  If you need a package, you can install it in your account, using a local library.  For example, to install BiocManager, you can type:
```
$ module load goolf R
$ R
   .
   .
   .

install.packages('BiocManager')

```
If the R interpreter prompts you about creating a local library, type yes.  If it asks you to select a CRAN mirror, scroll down the list it provides and select one of the US sites.
Or, you can launch RStudio and install the packages as you would on your laptop.
Submitting a Single-Core Job to the Cluster
After you have developed your R program, you can submit it to the compute nodes by using¬†a Slurm job script¬†similar to the following:¬†
{{< pull-code file=""/static/scripts/r_job.slurm"" lang=""no-highlight"" >}}
This script should be saved in a¬†file, called (for example)¬†r_job.slurm.¬† To run your job, you would submit the script¬†by typing:
sbatch job.slurm
Submitting Multi-Core Jobs to the Cluster
R programs can be written to use multiple cores on a node.  You will need to ensure that both Slurm and your R code know how many cores they will be using.  In the Slurm script, we recommend using --cpus-per-task to specify the number of cores.  For example:
{{< pull-code file=""/static/scripts/r_multicore.slurm"" lang=""no-highlight"" >}}
For the R code, the number of cores can be passed in with a command-line argument, as shown in the above example with ${SLURM_CPUS_PER_TASK}.  The code will need to be designed to read in the command-line argument and establish the number of available cores.  For example:
cmdArgs <- commandArgs(trailingOnly=TRUE)
numCores <- as.integer(cmdArgs[1]) - 1
options(mc.cores=numCores)
Or, you if you do not want to use command-line arguments, you can use the function Sys.getenv() in the R code.  For example:
```
numCores <- as.integer(Sys.getenv(""SLURM_CPUS_PER_TASK"")) - 1
options(mc.cores=numCores)
```
Do not use the detectCores() function, which is often shown in tutorial examples.  It will detect the number of physical cores -- not how many core Slurm is allowing the program to use.
Submitting MPI Jobs to the Cluster
R programs can be distributed across multiple nodes with MPI (message passing interface) and the appropriate MPI packages.¬† To run a parallel R job that uses MPI, the Slurm script¬†would be similar to the following:
{{< pull-code file=""/static/scripts/r_mpi.slurm"" lang=""no-highlight"" >}}
The items to notice in this script are 
i)   the number of nodes; 
ii)  the number of tasks; 
iii) the parallel partition; and 
iv)  the mpirun before the command to run the R code.
Submitting Jobs to Rio
When running Slurm jobs on Rio, it is essential to ensure that all R packages and environment variables are configured correctly. Rio compute nodes can only run jobs from high-security research standard storage, so it's important to ensure that all necessary files and variables point to this location. 
Before installing any R packages, create an .Renviron file in a directory under your high-security research standard storage. For example:
touch /standard/ivy-xxx-xxxx/path/to/R/.Renviron
Next, create the .Rprofile file in the same directory as .Renviron and add the following content to the .Rprofile file:
```
R_VERSION <- paste(R.version$major, sub(""\..*"", """", R.version$minor), sep=""."")
lib_path <- file.path(""/sfs/ceph/hachi/standard/ivy-xxx-xxxx/path/to/R"", R_VERSION)
Create the directory for the R library if it doesn't exist
if (!dir.exists(lib_path)) {
    dir.create(lib_path, recursive = TRUE, showWarnings = FALSE)
}
Set R_LIBS_USER in this session
Sys.setenv(R_LIBS_USER = lib_path)
Update the .Renviron file with the correct R_LIBS_USER path
r_envpath <- normalizePath(""/sfs/ceph/hachi/standard/ivy-xxx-xxxx/path/to/R/.Renviron"", mustWork = FALSE)
writeLines(paste0(""R_LIBS_USER="", lib_path), ""/sfs/ceph/hachi/standard/ivy-xxx-xxxx/path/to/R/.Renviron"")
Reload the environment variable to apply changes immediately
readRenviron(r_envpath)
```
Replace the lines with 'ivy-xxx-xxxx' to the path in your filesystem where the R directory exists.
This will ensure that R packages are installed in the correct directory under high-security storage.The .Rprofile file also dynamically adjusts to whatever version of R you're working with (RStudio or different module R versions).
{{% callout %}}
It's important to ensure that the /standard/ivy-xxx-xxxx/path/to/R/X.Y (Eg 4.3) directory exists before trying to install any new packages. Otherwise, R will try to create a directory under /home. If this happens, you can just close R then try running again and it should install correctly since the path has then been created from the .Rprofile file. 
Additionally, if switching between R versions, make sure to load R then close it before actually running it on your code. For some reason, R_LIBS_USER gets set properly, but if packages are installed right after closing the previous version, they still install in the other /standard/ivy-xxx-xxxx/path/to/R/X.Y (Eg If using 4.4, packages will install in 4.3). You'll want to quit R, purge the module, then re-load to try again.
{{% /callout %}}
To ensure the environment variables persist across sessions, add the following to your ~/.bashrc file:
export R_PROFILE=""/standard/ivy-xxx-xxxx/path/to/R/.Rprofile""
export R_ENVIRON=""/standard/ivy-xxx-xxxx/path/to/R/.Renviron""
These variables will carry over from your virtual machine (VM) frontend to the compute node. If these variables are not set in ~/.bashrc, they can also be exported directly in your Slurm script or via the command line when using ijob:
```
export R_PROFILE=""/standard/ivy-xxx-xxxx/path/to/R/.Rprofile""
export R_ENVIRON=""/standard/ivy-xxx-xxxx/path/to/R/.Renviron""
module purge
module load R/4.3.1
``
{{% callout %}}
Keep in mind to replace/standard/ivy-xxx-xxxx/path/to/R` with the path to R in your storage share.
{{% /callout %}}
By following the above steps, you will ensure that your Slurm jobs are properly configured to run with the required R packages and environment settings under the high-security research standard storage system.
If you have questions about running your R code on the HPC system or would like a consultation to optimize or parallelize your code, contact hpc-support@virginia.edu."
rc-website-fork/content/userinfo/hpc/software/debuggers.md,"+++
type = ""rivanna""
date = ""2020-03-10T12:51:46-05:00""
tags = [
  ""rivanna"", ""software"", ""compiler"", ""debuggers"", ""profilers""
]
draft = false
title = ""Debuggers and Profilers""
description = ""Debuggers and Profilers and UVA HPC""
author = ""RC Staff""
+++
Debuggers
To use a debugger, it is necessary to rebuild your code with the -g flag added.  All object files must be removed anytime compiler flags change.  If you have a Makefile run make clean if it is available.  The program must then be run under the control of the debugger.  For example, if you are using gdb, you run
gdb ./myexec
Adding debugging flags generally disables any optimization flags you may have added, and can slow down the code.  Please remember to recompile with -g removed once you have found your bugs.
Gnu Debugger (gdb) and Profiler (gprof)
The Gnu Compiler Collection compilers are free, open-source tools. Additional tools included are the gdb debugger and the gprof performance profiler. For detailed documentation, visit the Gnu website.
The gdb and gprof tools are included with the gcc compiler suite and are loaded with the gcc module.
module load gcc
Both gdb and gprof are text-based, command-line tools.  
{{< module-versions module=""gcc"" >}}
Intel
To load the Intel compilers and associated tools, run
module load intel
The Intel debugger for versions 16.0 and beyond is a modified gdb and is used in a similar manner. After the Intel module is loaded, it can be accessed as gdb-ia.

Available Intel Compilers
{{< module-versions module=""intel"" >}}
PGI Compiler
The PGI Server Compilers and tools are licensed for Linux systems.
module load pgi

Available PGI and NVIDIA Compilers
{{< module-versions module=""pgi"" >}}
PGI provides a very capable debugger, pgdbg. In its default mode, it is graphical, and it requires that an X server run on the user's local desktop machine. It may be run in command-line mode with the -text option; see the manpage for a full list of options. As with all debuggers, the user's program must be compiled with the -g flag in order to enable debugging. If you wish to use the graphical debugger and do not have or want to install an X11 server, you can also use FastX. 
{{< module-versions module=""nvhpc"" >}}
The PGI compiler is transitioning to the NVIDIA HPC SDK tools.  The NVHPC debugger is called cuda-gdb.
Totalview
The most powerful debugger available on the HPC system for OpenMP and MPI codes is Totalview.  It is also an excellent general-purpose debugger.  It has a command line interface but is not easy to use in that mode; it is nearly always used through its graphical user interface, which is highly intuitive.  For short runs with few processes it can be used on a frontend through FastX; longer or otherwise more demanding debugging runs can occur by running an Open OnDemand Desktop.
Valgrind
Valgrind is a framework for dynamic analysis tools. The most widely used tool is probably memcheck for detecting memory leaks. Build your code as usual with -g, then run it as
valgrind --leak-check=yes ./myprog arg1 arg2 > valgrind.out
The output from memcheck can be voluminous.  Running under memcheck will also be much slower than a normal run and can require more memory.
Profilers
Profilers are tools for locating areas where the code can be sped up.  Most profilers function by interrupting the code frequently and randomly, determining what subprogram is executing, and counting the time required for a subprogram to execute, how many times it is called, and so forth.
Gnu Profiler (gprof)
The Gnu profiler is a command-line, text-oriented tool.  The code must be recompiled with -g -p flags.  Run the code and usual.  This will produce a binary file called gmon by default.  Then run the profiler
gprof > gprof.out
Intel Tools
VTune Profiler
VTune is a powerful profiler for code built with Intel compilers.  A getting-started guide is available from Intel.
Intel Trace Analyzer
The Intel Trace Analyzer, which is focused on MPI codes, is included with our Intel compiler licenses.  Intel provides a tutorial here.  Some modifications are needed to run under Slurm.  Add a flag -bootstrap=slurm to the mpirun command, and use `-n $SLURM_NTASKS} rather than hard-coding a value.  
Open|SpeedShop
Open|SpeedShop is a profiler capable of operating in several different modes, for different ""experiments."" Unlike many profilers, it requires that only the -g flag be enabled.
Building on the HPC system
For general information about building your code on the HPC system, please see our howto"
rc-website-fork/content/userinfo/hpc/software/ansys.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2021-06-04T08:37:46-05:00""
tags = [
  ""cae"",
  ""multi-core"",
  ""mpi""
]
draft = false
modulename = ""ansys""
softwarename = ""ANSYS""
title = ""ANSYS and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
Local support is minimal; users should make an account at the student forum through the {{% software-name %}} website for technical support and for obtaining detailed information.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Licensing
The current general UVA license can be used for research but is limited in the size of the models it can use, and some of the more advanced features are not available.  Users who have their own research licenses with greater capabilities must specify that license.  To use such a research license on the UVA HPC system, before running ANSYS set the following environment variable
no-highlight
export ANSYSLMD_LICENSE_FILE=1055@myhost.mydept.virginia.edu
You may also need
no-highlight
export ANSYSLI_SERVERS=2325@myhost.mydept.virginia.edu
You must obtain the full names of the hosts and the port numbers from your group's license administrator.  The numbers in the above lines are the standard ANYSYS ports, but it is possible they may differ for some license servers; consult your license administrator for specific values. The ANSYSLI_SERVERS environment variable is generally not necessary if the default port is used, but ANSYSLMD_LICENSE_FILE will always be required.
These environment variables must be set in each shell and in every Slurm script that invokes ANSYS.
Using ANSYS Workbench
If you wish to run jobs using the Workbench, you need to edit the ~/.kde/share/config/kwinrc file and add the following line:
FocusStealingPreventionLevel=0
The workbench application, runwb2, should be executed in an interactive Open OnDemand Desktop session.
When you are assigned a node, launch the desktop, start a terminal, load the desired module and start the workbench with the runwb2 command.
module load ansys
unset SLURM_GTIDS
runwb2
Be sure to delete your Open OnDemand session if you finish before your requested time expires.
Multi-Core Jobs
It is possible to run multicore jobs through Open OnDemand. In a terminal, load the ansys module and then run the appropriate package frontend: for general ANSYS applications, including CFX, that is the Workbench; for Fluent run fluent to start its graphical interface.  Choose the ""Parallel Options"" tab to set up a run.  Be sure to use only the number of cores you requested when you launched the OOD Desktop.
For longer jobs, and for all multinode jobs, you should run in batch mode using  a Slurm script.  Please refer to ANSYS documentation for instructions in running from the command line.  These examples use threading to run on multiple cores on a single node.
ANSYS Slurm Script:
{{< pull-code file=""/static/scripts/ansys.slurm"" lang=""no-highlight"" >}}
CFX Slurm Script:
{{< pull-code file=""/static/scripts/cfx.slurm"" lang=""no-highlight"" >}}
Multi-Node MPI Jobs
For Fluent specify -mpi=intel along with the flag -srun to dispatch the MPI tasks using Slurm's task launcher. If more than the default memory per core is required, it is generally better with ANSYS and related products to request a total memory over all processes rather than using --mem-per-cpu, because a process can exceed the allowed memory per core.  Please refer to our documentation for current information about default memory per core in each partition.
These examples also show the minimum number of command-line options; you may require more for large jobs.

Fluent Slurm Script:
{{< pull-code file=""/static/scripts/fluent.slurm"" lang=""no-highlight"" >}}
The syntax for CFX is different and includes a ""start-method."" We recommend Intel MPI. Please refer to documentation for other options that may be required.
CFX Slurm script:
{{< pull-code file=""/static/scripts/cfx_mpi.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/picard.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""picard""
softwarename = ""Picard""
title = ""Picard and UVA HPC""
author = ""RC Staff""
+++
Description
{{< module-description >}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/totalview.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2021-06-04T08:37:46-05:00""
tags = [
  ""debugger"",
  ""multi-core"",
  ""mpi"",
  ""programming""
]
draft = false
modulename = ""totalview""
softwarename = ""TotalView""
title = ""Code Debugging and UVA HPC""
author = ""RC Staff""
+++
TotalView
TotalView is a full-featured, source-level, graphical debugger for applications written in C, C++, Fortran (77 and 90/95/2003), assembler, and mixed source/assembler codes. It is a multiprocess, multithread debugger that supports multiple parallel programming paradigms including MP and OpenMP. The University has a near-site license (256 tokens) for Totalview on all versions of Linux. Visit the TotalView website for detailed documentation.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Getting started with TotalView.
Your code must be compiled appropriately to use Totalview. For most Unix compilers, the debug flag -g must be added to the compilation options, just as it would be for other debuggers such as gdb. Optimization should also generally be suppressed, since optimization can change the code in ways that make it difficult for the debugger to interpret. Once the code has been recompiled and an executable generated, you are ready to invoke Totalview.
To start TotalView, execute the following command:
module load totalview
totalview
Totalview is normally used with its X11-based graphical user interface and to use it directly, you must have an X server running on your local system or use FastX. Computers running Linux will automatically have an X server available.

On Mac OS X you will need to install XQuartz.
Windows users must also install an X server; we recommend XminGW.

The recommended way to run X applications remotely is to enable X11 port forwarding in your ssh client (SecureCRT, PuTTY, etc.) and run the X server in the background (passively). Another option is the FastX client which can be installed on the user's local system to open a desktop on the cluster frontend.
Using Totalview to Debug MPI Codes
One of the most powerful features of Totalview is its ability to debug parallel codes.
Using the Debugger Through FastX
You can use TotalView via FastX. If your debugging work is sufficiently small to run on the frontend, start TotalView like any other X11 application from the command line:
module load totalview
totalview &
Using the Client on Compute Nodes
If you have a long debugging job or you want to debug an MPI application, you should run an interactive job through Slurm using the Open OnDemand Desktop interactive application.  When the Desktop is launched, start a terminal window and type the above commands as usual."
rc-website-fork/content/userinfo/hpc/software/amber.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
  ""amber""
]
draft = false
modulename = ""amber""
softwarename = ""Amber""
title = ""Amber and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website
Local support is not available.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/complete-list.md,"+++
type = ""rivanna""
date = ""2019-04-23T08:37:46-05:00""
tags = [
  ""rivanna"",
  ""lmod"",
  ""software"",
  ""hpc""
]
draft = false
title = ""UVA HPC Software List""
description = ""List of all Software Modules on the UVA HPC System""
author = ""RC Staff""
categories = [""userinfo""]
+++
{{< rivanna-software moduleclasses=""all"" >}}"
rc-website-fork/content/userinfo/hpc/software/berkeleygw.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2022-03-10T00:00:00-05:00""
tags = [
  ""multi-core"",
  ""phys"",
  ""gpu""
]
draft = false
modulename = ""berkeleygw""
softwarename = ""BerkeleyGW""
title = ""BerkeleyGW and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Local support is not available. For detailed documentation and tutorials, visit the {{% software-name %}} website. The user forum can be found here.
Software Category: {{% module-category %}}
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Users may build their own versions of BerkeleyGW if they wish to use a different compiler+MPI combination, or to choose individual optional packages. Please consult the official documentation.
Example Slurm script
We built BerkeleyGW with GPU support. It can only run on V100 and A100 GPUs. Please use the following Slurm script as a template.
{{< pull-code file=""/static/scripts/berkeleygw.slurm"" lang=""no-highlight"" >}}
We highly recommend running a benchmark to decide how many CPU cores and/or GPU devices you should use."
rc-website-fork/content/userinfo/hpc/software/sas.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""math""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""sas""
softwarename = ""SAS""
title = ""SAS and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Note: SAS scripts may be run on the HPC system through the Slurm queueing system in batch mode, but production interactive jobs on the frontend are not permitted."
rc-website-fork/content/userinfo/hpc/software/yambo.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2022-03-10T00:00:00-05:00""
tags = [
  ""multi-core"",
  ""phys"",
  ""gpu""
]
draft = false
modulename = ""yambo""
softwarename = ""Yambo""
title = ""Yambo and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Local support is not available. For detailed documentation and tutorials, visit the {{% software-name %}} website. The user forum can be found here.
Software Category: {{% module-category %}}
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Users may build their own versions of Yambo if they wish to use a different compiler+MPI combination, or to choose individual optional packages. Please consult the official documentation.
Example Slurm script
We built Yambo with GPU support. It can only run on V100 and A100 GPUs. Please use the following Slurm script as a template.
{{< pull-code file=""/static/scripts/yambo.slurm"" lang=""no-highlight"" >}}
We highly recommend running a benchmark to decide how many CPU cores and/or GPU devices you should use."
rc-website-fork/content/userinfo/hpc/software/intel.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""intel""
]
draft = false
modulename = ""intel""
softwarename = ""Intel""
title = ""Intel and UVA HPC""
description = ""Intel compilers and tools""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
{{< highlight >}}
The 2024.0 version is experimental. Most users should load an older version for the time being.
{{< /highlight >}}
Compiler
For general information on building code using compilers, please see our How-To pages:

Building Your Code on UVA HPC
Building and Running MPI Code

The Intel compilers are:
{{< table class=""table table-striped"" >}}
||<2024|>=2024|
|---|---|---|
C| icc | icx |
C++| icpc | icpx |
Fortran| ifort | ifort |
{{< /table >}}
Tools
Intel Trace Analyzer and Collector
The Intel Trace Collector is a low-overhead tracing library that performs event-based tracing in applications. The Intel Trace Analyzer provides a convenient way to monitor application activities gathered by the Intel Trace Collector through graphical displays. 
To use this tool:
module load intel
source $EBROOTINTEL/parallel_studio_xe_20**.*.***/bin/psxevars.sh}}
Fill in the * with the actual path.
In your Slurm script, replace srun myprog with
```
SBATCH -n 
...
mpirun -trace -bootstrap slurm -n ${SLURM_NTASKS} myprog
``
The Slurm variable${SLURM_NTASKS}will expand to the` that you specify in the SBATCH directive.
This will write a trace file (*.stf) that you can analyze with traceanalyzer. You will need to run this on FastX Web MATE desktop environment (recommended) or add the -Y flag when you ssh into the HPC system."
rc-website-fork/content/userinfo/hpc/software/singularity.md,"+++
categories = [""userinfo""]
type = ""rivanna""
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""docker"", ""containers"", ""singularity""
]
draft = false
title = ""Software Containers""
description = ""Software Containers""
author = ""RC Staff""
+++
{{< highlight >}}
[Deprecated] On Dec 18, 2023 Singularity has been upgraded to Apptainer, a continuation of the Singularity project.
{{< /highlight >}}
Overview

Singularity is a container application targeted to multi-user, high-performance computing systems. It interoperates well with Slurm and with the Lmod modules system. Singularity can be used to create and run its own containers, or it can import Docker containers.
Creating Singularity Containers
To create your own image from scratch, you must have root privileges on some computer running Linux (any version).  Follow the instructions at the Singularity site.  If you have only Mac or Windows, you can use the Vagrant environment.  Vagrant is a pre-packed system that runs under several virtual-machine environments, including the free Virtualbox environment.  Singularity provides instructions for installing on Mac or installing on Windows.  Once you have installed Vitrualbox, you install Singularity's Vagrant image, which contains the prerequisites to author images.  You can then follow the instructions for Linux to author your image.
How to use a Docker image on the HPC System?
You will need to convert the Docker image into Singularity. Please visit our how-to page for instructions.
If you do not have the ability to create your own image for the HPC system or to use a Docker image, contact us for assistance.
Singularity on the HPC system
Singularity is available as a module. The RC staff has also curated a library of pre-prepared Singularity container images for popular applications as part of the shared software stack.  Descriptions for these shared containers can be found via the module avail and module spider commands.
module load singularity
module avail
Loading any of these container modules produces an on-screen message with instructions on how to copy the container image file to your directory and how to run the container.
What is Inside the Container?
Use the shell command to start up a shell prompt and navigate (more later).
For containers built with Singularity, you can use the run-help command to learn more about the applications and libraries:
singularity run-help /path/to/sif
For containers built with Docker, use the inspect --runscript command to find the default execution command. Using the TensorFlow module as an example:
```bash
$ module load singularity tensorflow/2.10.0
$ singularity inspect --runscript $CONTAINERDIR/tensorflow-2.10.0.sif
!/bin/sh
OCI_ENTRYPOINT='""python""'
...
```
This shows that python will be executed when you run (more later) the container.
Running non-GPU Images
If your container does not require a GPU, all that is necessary is to load the singularity module and provide it with a path to the image.
module load singularity
singularity <CMD> <OPTIONS> <IMAGEFILE> <ARGS>
CMD defines how the container is used. There are three main commands:

run: Executes a default command inside the container. The default command is defined at container build time.
exec: Executes a specific application/command inside the container as specified with ARGS. This provides more flexibility than the run command.
shell: Starts a new interactive command line shell inside the container.

OPTIONS define how the singularity command is executed.  These can be omitted in most cases.
IMAGEFILE refers to the single Singularity container image file, typically with a .sif or .simg extension.
ARGS define additional arguments passed inside the container.  In combination with the exec command they define what command to execute inside the container.
singularity run
containerdir = ~mst3k
singularity run $containerdir/myimage.sif
This executes a default application or set of commands inside the container.  The default application or set of commands to execute is defined in the image build script and cannot be changed after the container is built.  After execution of the default command, the container is closed.
singularity exec
singularity exec $containerdir/myimage.sif python myscript.py
This is similar to singularity run but more versatile by allowing the specification of the particular application or command to execute inside the container.  In this example it launches the python interpreter and executes the myscript.py script, assuming that Python was installed into the container image.  After execution of the command, the container is closed.
singularity shell
singularity shell $containerdir/myimage.sif
This opens a new shell inside the container, notice the change of the prompt:
Singularity>
Now you can execute any command or application defined in the container, for example ls to list all files in the current directory:
Singularity> ls
You can navigate the container file system, including /scratch and /nv, and run any application that is installed inside the container. To leave the interactive container shell, type exit:
Singularity> exit
Running GPU Images
Singularity can make use of the local NVIDIA drivers installed on the host.  To use a GPU image, load the singularity module and add the --nv flag when executing the singularity shell, singularity exec, or singularity run commands.
module load singularity
singularity <CMD> --nv <IMAGE_FILE> <ARGS>
Example:
module load tensorflow/2.10.0.sif
singularity run --nv $CONTAINERDIR/tensorflow-2.10.0.sif myscript.py
In the container build script, python was defined as the default command to be executed and singularity passes the argument(s) after the image name, i.e. myscript.py, to the Python interpreter. So the above singularity command is equivalent to
singularity exec --nv $CONTAINERDIR/tensorflow-2.10.0.sif python myscript.py
This image was built to include CUDA and cuDNN libraries that are required by TensorFlow.  Since these libraries are provided by the container, we do not need to load the CUDA/cuDNN libraries available on the host.
Running Images Interactively
Start an ijob:
ijob  -A mygroup -p gpu --gres=gpu -c 1
salloc: Pending job allocation 12345
salloc: job 12345 queued and waiting for resources
salloc: job 12345 has been allocated resources
salloc: Granted job allocation 12345
If your image starts a graphical user interface or otherwise needs a display, you should use the Open OnDemand Desktop rather than a command-line ijob.  Once the Desktop is launched, start a terminal window and type the commands as in any other shell.
module purge
module load singularity
singularity shell --nv /path/to/sif
Running Image Non-Interactively as Slurm jobs
Example script:
```
!/usr/bin/env bash
SBATCH -J tftest
SBATCH -o tftest-%A.out
SBATCH -e tftest-%A.err
SBATCH -p gpu
SBATCH --gres=gpu:1
SBATCH -c 1
SBATCH -t 00:01:00
SBATCH -A mygroup
module purge
module load singularity tensorflow/2.10.0
singularity run --nv $CONTAINERDIR/tensorflow-2.10.0.sif tensorflowtest.py
```
Interaction with the Host File System
Each container provides its own file system.  In addition, directories of the host file system can be overlayed onto the container file system so that host files can be accessed from within the container.  These overlayed directories are referred to as bind paths or bind points.  The following system directories of the host are exposed inside a container:

/tmp
/proc
/sys
/dev

In addition, the following user directories are overlayed onto each container by default on the HPC system:

/home
/scratch
/nv
/project

Due to the overlay these directories are by default the same inside and outside the container with the same read, write, and execute permissions.  This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits.  The /nv and /project directories refer to leased storage locations that may not be available to all users.
Disabling the Default Bind Paths
Under some circumstances this default overlay of the host file systems is undesirable.  Users can disable the overlay of /home, /scratch, /nv, /project by adding the -c flag when executing the singularity shell, singularity exec, or singularity run commands.
For example,
containerdir=~mst3k
singularity run -c $containerdir/myimage.sif
Adding Custom Bind Paths
Users can define custom bind paths for host directories via the -B/--bind option, which can be used in combination with the -c flag.
For example, the following command adds the /scratch/$USER directory as an overlay without overlaying any other user directories provided by the host:
singularity run -c -B /scratch/$USER $containerdir/myimage.sif
To add the /home directory on the host as /rivanna/home inside the container:
singularity run -c -B /home:/rivanna/home $containerdir/myimage.sif"
rc-website-fork/content/userinfo/hpc/software/bowtie2.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""bowtie2""
softwarename = ""Bowtie2""
title = ""Bowtie2 and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.

Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}

Bowtie2 Example
The following example demonstrates how to run a Bowtie sequence alignment on multiple cpu cores on a single HPC node. More details information about the aligner can be found here.
Note that Bowtie cannot be executed across multiple nodes. 
Create a temporary directory
We start by creating a temporary directory and copying the Bowtie2 example files into it.  
module load gcc bowtie2
mkdir -p /scratch/$USER/bowtie_temp
cp -r $EBROOTBOWTIE2/example /scratch/$USER/bowtie_temp

The $USERvariable will expand to your computing ID so you'll be using your personal scratch directory.  
The EBROOTBOWTIE2 environment variable is set to the Bowtie2 installation directory after you load the bowtie2 module.  
Note that you have to preload the gcc module before loading bowtie2.

The Slurm Job Script
The Slurm script defines the HPC resources needed to run the Bowtie2 indexing and alignment. Bowtie2 can utilize multiple cpu cores on a single compute node. It does not support execution on multiple nodes.  
Let's create a textfile that serves as our job script, alignment.slurm, with the following content:
{{< pull-code file=""/static/scripts/bowtie2.slurm"" lang=""no-highlight"" >}}


You need to replace <YOUR_ALLOCATION> with your own HPC allocation name.


The $USER variable will expand to your computing ID so you'll be using your personal scratch directory.  


The $SLURM_CPUS_PER_TASK variable is set by the job scheduler to match the --cpus-per-task directive, in this case 8 cpus core per task (job run).


Submitting the Job
To run the above script, type:
sbatch alignment.slurm
This will return a message like this:
Submitted batch job 3184590
Check the Job Status
To check the status of the job, run
squeue -u <username>
where <username> is your UVA computing ID that you used to log in.
To see a history of your jobs, run:
sacct
Output
Because of parallel processing, the aligned reads might appear in the output SAM file in a different order than they were in the input FASTQ. You can add the --reorder flag to your command so that the order does not change, but it is typically not necessary.
Troubleshooting
Caution: If you create the Slurm job script on a Windows computer and then upload it to the HPC system, you‚Äôll probably get an error when you run it with sbatch that says:
sbatch: error: Batch script contains DOS line breaks (\r\n)
sbatch: error: instead of expected UNIX line breaks (\n).
To fix this, run
dos2unix alignment.slurm
This will remove unwanted \r from text files."
rc-website-fork/content/userinfo/hpc/software/matlab.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2019-06-23T08:37:46-05:00""
tags = [
  ""lang"",
  ""matlab"",
  ""parallel"",
  ""programming""
]
draft = false
shorttitle = ""MATLAB""
softwarename = ""MATLAB""
modulename = ""matlab""
title = ""Matlab and UVA HPC""
description = ""Matlab in the HPC environment""
author = ""RC Staff""
toc = true
+++
MATLAB is an integrated technical computing environment from the MathWorks that combines array-based numeric computation, advanced graphics and visualization, and a high-level programming language. Separately licensed toolboxes provide additional domain-specific functionality.
Mathworks provides MATLAB examples and tutorials for all experience levels here.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions. To load the most recent version of {{% software-name %}}, at the terminal window prompt run:
module load matlab
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{% module-versions %}}
You can work in the MATLAB desktop on the UVA HPC frontend nodes; we recommend FastX for this application.  However, the time and memory that a job can use on the frontends are limited, so for longer jobs you should submit your job to compute nodes through Slurm.
If your Matlab job requires user interactions via the Matlab interface, you should use Open OnDemand as described in the next section.
If you will be running MATLAB through the command line but still wish to use an interactive job, you can create an ijob.
Interactive Sessions through Open OnDemand
Starting an Interactive Session
To launch an instance of {{% software-name %}}, you will begin by connecting to our Open OnDemand portal. You need to specify required resources, e.g. node partition, time, your UVA HPC allocation, etc.. If you are new to UVA HPC, you may want to read the Getting Started Guide to learn more about the partitions.

Open a web browser and go to URL:  https://ood.hpc.virginia.edu.
Use your Netbadge credentials to log in. This will open the Open OnDemand web portal.
On the top banner of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on {{% software-name %}}.
After connecting to {{% software-name %}} through Open OnDemand, a form will appear where you can fill in the resources for {{% software-name %}}.
When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times.
It may take a few minutes for the system to gather the resources for your instance of {{% software-name %}}. When the resources are ready a Connect to {{% software-name %}} button will appear. Click on the button to start {{% software-name %}}.

Using {{% software-name %}}
When {{% software-name %}} opens in your web browser, it will appear just like the {{% software-name %}} that you have on your laptop or desktop.
Closing the Interactive Session
When you are done, quit the Matlab application.  The interactive session will be closed and the allocated resources will be released. If you leave the session open, your allocation will continue to be charged until the specified time limit is reached.
Running a Matlab Batch Jobs on the HPC System
The HPC system uses the Slurm resource manager to schedule and run jobs on the cluster compute nodes. The following are example Slurm scripts for submitting different types of Matlab batch jobs to the cluster.
Submitting a batch job using a single core of a compute node.
Once your program is debugged, we recommend running in batch mode when possible.
This runs the job in the background on a compute node. Write a Slurm script similar to the following:
{{< pull-code file=""/static/scripts/matlab_serial.slurm"" lang=""no-highlight"">}}
The option -nodisplay suppresses the Desktop interface and any attempt to run a graphical display. Some MATLAB functions are capable of running on multiple cores. If your code uses linear algebraic operations, those can be multithreaded across multiple cores, so you would need to request the additional cores in your slurm script.  Unless you are sure you can use multiple cores effectively it's generally best to restrict your job to one core.
The ;exit is important to ensure that the job terminates when the computation is done. The example code pcalc2.m is shown below. Note that passing the SLURM_JOB_ID variable allows the function to save
output to a job-specific filenames.
```
function  [a]=pcalc2(nloop,ndim,jobid)
% Example using the parfor construct to calculate the maximum eignevalue
% of a random ndim by ndim matrix nloops times
% nloop: Number of times parfor loop should run
% ndim: Dimension of the square matrix to create
% jobid: Slurm job id number for unique file name
if ischar(nloop) % checking data type on first input
    nloop=str2num(nloop);
end
if ischar(ndim) % checking data type on second input
    ndim=str2num(ndim);
end
% preallocate output array
a=zeros(nloop, 1);
% TIME CONSUMING LOOP
tic;
parfor i=1:nloop  % parallelized for loop
                  % parfor only used if parallel pool open
    a(i)=FunctionTakesLongTime(ndim);
    % print progress of the parfor loop
    if mod(i,10)==0
        fprintf('Iteration number = %d of %d total \n',nloop-i,nloop)
    end
end
time=toc;
% output timing information and host
stringOut1= ...
sprintf('time = %f,nloop = %d on host %s \n',time,nloop,getenv('HOSTNAME'));
% save output to a file
fid = fopen(['pcalc_' jobid '.out'],'wt');
fprintf(fid, '%s', stringOut1);
fclose(fid);
end
function max_eig=FunctionTakesLongTime(ndim);
% Computation intensive calculation dependent on matrix size
 max_eig=max(abs(eig(random('Exponential',ndim,ndim))));
end
```
Submitting a batch job using multiple cores on a compute node
If you have a Matlab job that can be structured to run across multiple cores, you can greatly speed up the time to your results. The linear algebraic libraries in Matlab are multithreaded and will make use of multiple cores on a compute node. The Parallel Computing Toolkit allows you to distribute for loops over multiple cores using parfor and other parallel constructs in MATLAB. For more information on using the Parallel Computing Toolbox in MATLAB see the.
MathWorks documentation .
The example function pcalc2.m above uses a  parallel for loop (parfor)
in MATLAB. To run your parallel MATLAB code across multiple cores on one compute
node, you can use a slurm script similar to the following:
{{< pull-code file=""/static/scripts/matlab_multicore.slurm"" lang=""no-highlight"" >}}
The Matlab script setPool1.m creates a local pool of matlab workers on the cores of the compute node.
```
% Script setPool1.m
% create a local cluster object
pc = parcluster('local');
% explicitly set JobStorageLocation to job-specific temp directory
mkdir(strcat('/scratch/',getenv('USER'),'/slurmJobs/',getenv('slurm_ID')));
pc.JobStorageLocation = ...
     strcat('/scratch/', getenv('USER'),'/slurmJobs/', getenv('slurm_ID'));
% start the matlabpool with the available workers
% control how many workers by setting ntasks in your sbatch script
parpool(pc, str2num(getenv('numWorkers')))
```
Matlab Jobs using Slurm Job Arrays
The Slurm has a mechanism for launching multiple independent jobs with one
job script using the --array directive.
Array of Multicore Parallel Matlab Jobs
The following Slurm script uses job arrays to submit multiple parallel Matlab
jobs, each running on a nodes of the standard queue.
{{< pull-code file=""/static/scripts/matlab_job_array.slurm"" lang=""no-highlight"" >}}
Parallel Matlab on Multiple Compute Nodes
To run Matlab parallel jobs that require more cores than are available on one compute node (e.g. > 40), you can launch the Matlab desktop on one of the HPC login nodes. The following procedure will create the cluster profile for your account on UVA HPC:
For version R2023a or newer, use the Discover Clusters function in the drop-down of the Parallel menu to create a cluster profile for Rivanna as described in the following link.
Discover Clusters and Use Cluster Profiles
parfor example
```
% Create a cluster object based on the profile
pc = parcluster('Rivanna_cluster'); % This must correspond to the matlab
     % version you are using
% Add additional properties related to slurm job parameters
pc.AdditionalProperties.AccountName = 'hpc_build' % account to charge job to
pc.AdditionalProperties.QueueName = 'parallel' % queue to submit job to
pc.AdditionalProperties.WallTime = '1:00:00' % amount of wall time needed
pc.saveProfile % save settings
pc.AdditionalProperties  % confirm above properties are set
% Additional configuration commands
% email address for Slurm to send email
pc.AdditionalProperties.EmailAddress ='teh1m@virginia.edu'
% send email when job ends and specify number of nodes and processes per node
pc.AdditionalProperties.AdditionalSubmitArgs = ...
'--mail-type=end --nodes=2 --ntasks-per-node=4'
% specify the total number of processes
procs=8;
```
Once this configuration is complete you can submit jobs to the cluster using
the following commands:
```
% Launch Matlab parallel code across two compute nodes
j=pc.batch(@pcalc2,1,{400,400,'myOutput1'},'pool',procs-1);
% Arguments of c.batch in order
% Handle of function containing parallel code to run
% Number of function outputs
% Cell array of function inputs
% Setting of a pool of matlab workers
% Number of Matlab workers. There are 8 cores on two nodes
% so use 7 for workers and one for master
% Get the state of the job
j.State
% Don't return command prompt until job finishes
j.wait
% Get output from the job
j.diary
% Command to use to get back debug information if a job crashes
% For parallel jobs (i.e. calling batch with pool>0)
% j.Parent.getDebugLog(j)
```
spmd example
The previous example distributes the iterations of a parfor loop across
multiple compute nodes. The following example shows how to solve a linear system
of equations (Ax=b) across multiple compute nodes using distributed arrays.
```
% create a cluster object
pc = parcluster('Rivanna_cluster'); % This must correspond to the matlab
     % version you are using
pc.AdditionalProperties.AccountName = 'hpc_build'
pc.AdditionalProperties.WallTime = '04:00:00';
pc.AdditionalProperties.QueueName = 'parallel';
% email address for Slurm to send email
pc.AdditionalProperties.EmailAddress ='teh1m@virginia.edu'
% send email when job ends and specify number of nodes and processes per node
pc.AdditionalProperties.AdditionalSubmitArgs = ...
'--mail-type=end --nodes=2 --ntasks-per-node=4'
% specify the total number of processes
procs=8;
% specify additional submit arguments
pc.AdditionalProperties.AdditionalSubmitArgs='--mem-per-cpu=30000';
% start the matlabpool with available workers
% control how many workers by setting ntasks in your sbatch script
j=pc.batch(@solver_large1,2,{20000,'myOutput2'},'Pool',procs-1);
wait(j);
j.State
j.diary
% Command to use to get back debug information if a job crashes
% For parallel jobs (i.e. calling batch with pool>0)
% j.Parent.getDebugLog(j)
```
The function solver_large1 looks like the following:
```
function [ firstTenX, errChk2 ] = solver_large1( N, jobid)
% This function is a simple test of a LU linear solver
% Since b is sum of columns, x should always be a vector
% of ones.
tic;
spmd
    A = codistributed.rand(N, N);
    b = sum(A, 2);
% solve Ax=b
x = A\b;

% Check error
errChk = normest(A * x - b);

end
time=toc;
x2 = gather(x);
fprintf('norm of the error for the solution \n')
errChk2=gather(errChk)
fprintf('first 10 elements of solution vector (should be all ones)\n')
firstTenX=x2(1:10)
time
save(['solver_large1_' num2str(jobid) '.out'], ...
      'time','errChk2','firstTenX');
end
```
Utilizing GPUs with Matlab
General guidelines on requesting GPUs on the HPC system
Once your job has been granted its allocated GPUs, you can use the gpuDevice function to initialize a specific GPU for use with Matlab functions that can utilize the architecture of GPUs. For more information see the MathWorks documentation on GPU Computing in Matlab.
The following slurm script is for submitting a Matlab job that uses 4 GPUs in a parfor loop. For each GPU requested, the script requests one cpu (ntasks-per-node).
{{< pull-code file=""/static/scripts/matlab_gpu.slurm"" lang=""no-highlight"" >}}
The function gpuTest1 looks like the following. For further information, see https://www.mathworks.com/help/parallel-computing/examples/run-matlab-functions-on-multiple-gpus.html
```
function  [a_cpu]=gpuTest1(ndim,nloop,jobid)
% Example to test gpu nodes
% ndim: dimension of matrix
% jobid: Slurm job id  for save saving output to job specific file
% nloop is number of time for parallel loop to run
% Check to see if you have a GPU device on your machine:
disp(['number of gpus ', num2str(gpuDeviceCount)])
% Create pool of workers across the gpus
parpool('local',gpuDeviceCount);
% Display the gpus
spmd
   gpuDevice
end
% preallocate output gpu array
a=zeros(nloop, 1,'double','gpuArray');
tic
parfor k=1:nloop
% create random 2-D array on gpu, perform 2-D fft and get maximum value
a(k) = max(max(abs(fft2((rand(ndim,'double','gpuArray'))))));
end
%Wait for the transfer to complete:
wait(gpuDevice)
%Gather the results from the GPU back to the CPU:
a_cpu = gather(a);
firstTen=a_cpu(1:10)  % Display first 10 elements of output array
delete(gcp('nocreate')); % delete parallel pool
tGPU = toc;
disp(['Total time on GPU: ' num2str(tGPU)])
disp(['ndim = ', num2str(ndim), '   nloop = ', num2str(nloop)])
% save output to file
save(['gpuTest1_' jobid '.out'],...
    'tGPU','ndim','nloop','firstTen','-ascii');
end
```
Matlab Parallel Computing Resources


Parallel Computing Toolbox Documentations


Parallel and GPU Computing tutorials


Performance and Memory


Parallel Computing Toolbox ‚Äî Examples

"
rc-website-fork/content/userinfo/hpc/software/texlive.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
  ""chem""
]
draft = false
modulename = ""texlive""
softwarename = ""TeX Live""
title = ""TeX Live and UVA HPC""
author = ""RC Staff""
+++
Description
TeX Live is TeX Live is intended to be a straightforward way to get up and running with the TeX document production system. It provides a comprehensive TeX system with binaries for most flavors of Unix, including GNU/Linux, macOS, and also Windows. It includes all the major TeX-related programs, macro packages, and fonts that are free software, including support for many languages around the world.
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Add Local Package
We do not support user-requested packages in our module. If you need a package that is not included, please install it locally by following these instructions.
module load texlive
mkdir -p ~/texmf/tex/latex
The directory structure of ~/texmf/tex/latex should be the same as
ls $EBROOTTEXLIVE/texmf-dist/tex/latex
Each package should reside in its own directory. Create a subdirectory for your package under ~/texmf/tex/latex and copy the *.sty file from ctan.org into the subdirectory. (If the package does not provide a *.sty, please follow the installation instructions. Typically, latex *.dtx or latex *.ins should produce the *.sty file.)
Then run:
$ texhash ~/texmf
texhash: Updating /home/mst3k/texmf/ls-R...
texhash: Done.
You should now be able to use the new package locally.
dvipng
If you encounter this error:
FileNotFoundError: [Errno 2] No such file or directory: 'dvipng'
please use our dvipng container. Go through all the steps in https://hub.docker.com/r/uvarc/dvipng."
rc-website-fork/content/userinfo/hpc/software/mathematica.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""math""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""mathematica""
softwarename = ""Mathematica""
title = ""Mathematica and UVA HPC""
author = ""RC Staff""
+++
Description
Mathematica is an integrated technical computing environment that combines numeric and symbolic computation, advanced graphics and visualization, and a high-level programming language.
There are several website resources with Mathematica tutorials and parallel Mathematica training sessions.
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/software/tensorflow.md,"+++
type = ""rivanna""
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""machine learning""
]
draft = false
title = ""TensorFlow and UVA HPC""
description = ""TensorFlow and UVA HPC""
author = ""RC Staff""
+++
Overview
TensorFlow is an open source software library for high performance numerical computation.  It has become a very popular tool for machine learning and in particular for the creation of deep neural networks.  The latest TensorFlow versions are now provided as prebuilt Apptainer containers on the HPC system.  The basic concept of running Apptainer containers on the HPC system is described here.
TensorFlow code is provided in two flavors, either with or without support of general purpose graphics processing units (GPUs).  All TensorFlow container images provided on the HPC system require access to a GPU node.  Access to GPU nodes is detailed in the sections below.
TensorFlow and Keras
Keras is a high-level neural networks application programming interface (API), written in Python and capable of running on top of TensorFlow, CNTK, or Theano.  Since version 1.12.0, TensorFlow contains its own Keras API implementation as described on the TensorFlow website.
What is inside the TensorFlow containers?
The TensorFlow modules on the HPC system include common Python packages such as Matplotlib and OpenCV. See https://hub.docker.com/r/uvarc/tensorflow for details.
TensorFlow Jupyter Notebooks
Jupyter Notebooks can be used for interactive code development and execution of Python scripts and several other codes. A few TensorFlow kernels are available.
Accessing the JupyterLab Portal

Open a web browser and go to URL:  https://ood.hpc.virginia.edu
Use your Netbadge credentials to log in.
On the top right of the menu bar of the Open OnDemand dashboard, click on ‚ÄúInteractive Apps‚Äù.
In the drop-down box, click on JupyterLab.

Requesting access to a GPU node
To start a JupyterLab session, fill out the resource request webform.  To request access to a GPU, verify the correct selection for the following parameters:

Under Rivanna Partition, choose ""GPU"".
Under Optional GPU Type, choose a GPU type or leave it as ""default"" (first available).
Click Launch to start the session.

Review our Jupyter Lab documentation for more details..
Editing and Running the Notebook
Once the JupyterLab instance has started, you can edit and run your notebook as described here.
TensorFlow Slurm jobs
Apptainer can make use of the local NVIDIA drivers installed on a host equipped with a GPU device.  The Slurm script needs to include the #SBATCH -p gpu and #SBATCH --gres=gpu directives in order to request access to a GPU node and its GPU device.  Please visit the Jobs Using a GPU section for details.
To run commands in an GPU-enabled container image, load the apptainer module and add the --nv flag when executing the apptainer run or apptainer exec commands.
For example:
module load apptainer tensorflow/2.10.0
apptainer run --nv $CONTAINERDIR/tensorflow-2.10.0.sif tf_example.py
In the container build script, python is defined as the default command to be executed and Apptainer passes the argument(s) after the image name, i.e. tf_example.py, to the Python interpreter. So the above apptainer command is equivalent to
apptainer exec --nv $CONTAINERDIR/tensorflow-2.10.0.sif python tf_example.py
The TensorFlow container images were built to include CUDA and cuDNN libraries that are required by TensorFlow.  Since these libraries are provided within each container, we do not need to load the CUDA/cuDNN libraries available on the host.
Example Slurm Script:
{{< pull-code file=""/static/scripts/tensorflow.slurm"" lang=""no-highlight"" >}}
TensorFlow Interactive Jobs (ijob)
Start an ijob.  Note the addition of -p gpu --gres=gpu to request access to a GPU node and its GPU device.
ijob -A mygroup -p gpu --gres=gpu -c 1
Console output""
salloc: Pending job allocation 12345
salloc: job 12345 queued and waiting for resources
salloc: job 12345 has been allocated resources
salloc: Granted job allocation 12345
Now you can load the apptainer module and execute commands provided by the container. For example:
module purge
module load apptainer tensorflow/2.13.0
apptainer run --nv $CONTAINERDIR/tensorflow-2.13.0.sif tf_example.py
Interaction with the Host File System
The following user directories are overlayed onto each container by default on the HPC system:

/home
/scratch
/nv
/project

Due to the overlay, these directories are by default the same inside and outside the container with the same read, write, and execute permissions.  This means that file modifications in these directories (e.g. in /home) via processes running inside the container are persistent even after the container instance exits.  The /nv and /project directories refer to leased storage locations that may not be available to all users.
TensorBoard
Request a Desktop session under Interactive Apps via Open OnDemand. Fill out the form to submit the Slurm job. Launch the session and open a terminal in the desktop. Enter these commands:
$ module load apptainer tensorflow/2.13.0
$ apptainer shell --nv $CONTAINERDIR/tensorflow-2.13.0.sif
Apptainer> python -m tensorboard.main --logdir=logdir
Open the resulting URL (of the form http://localhost:xxxx/) in Firefox.
Can I install my own TensorFlow (that works on a GPU)?
Yes, you may either pull the official TensorFlow Docker image or create your own environment. We shall use TensorFlow 1.14 as an example.
Docker


Go to https://hub.docker.com/r/tensorflow/tensorflow/tags and search for the desired version. Use the -gpu variant.


Note the provided pull command (docker pull tensorflow/tensorflow:1.14.0-gpu) and change it into Apptainer. The differences are underlined by ^:
    bash
    apptainer pull docker://tensorflow/tensorflow:1.14.0-gpu
    ^^^^^^^^^      ^^^^^^^^^


You will find the Apptainer image tensorflow_1.14.0-gpu.sif in your current directory. Consult the instructions in the previous sections. Remember to replace $CONTAINERDIR/tensorflow-2.13.0.sif with the actual path to your own Apptainer image.


Pip
Please read the manual for instructions. Especially note the [and-cuda] part of the pip install comand.
You may consider creating a conda environment (see miniforge) for your local installation."
rc-website-fork/content/userinfo/hpc/software/code-server.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""lang"",
]
draft = false
shorttitle = ""Code Server""
modulename = ""code-server""
softwarename = ""Code Server""
title = ""Code Server and UVA HPC""
author = ""RC Staff""
+++
Description
{{< module-description >}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Interactive Sessions through Open Ondemand
Interactive sessions of {{% software-name %}} can be launched through the HPC web portal, Open OnDemand.
If you are new to HPC, you may want to read the Getting Started Guide to learn more about the partitions.
Python Setup
Python users should install the ms-python extension and select an appropriate interpreter:

Press F1
Search for ‚Äúpython: select interpreter‚Äù
Choose the miniforge python or any other python in your custom environments.

Closing the Interactive Session
When you are done, delete the {{% software-name %}} session from the ""My Interactive Sessions"" tab and the allocated resources will be released."
rc-website-fork/content/userinfo/hpc/software/libraries.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""lib"",
]
draft = false
shorttitle = ""Libraries""
title = ""Libraries and UVA HPC""
description = ""Libraries in the HPC environment""
author = ""RC Staff""
+++
Available Software Libraries
To get an up-to-date list of the installed software libraries, log on to UVA HPC and run the following command in a terminal window:
module keyword lib
To get more information about a specific module version, run the module spider command, for example:
module spider hdf5

List of Software Library Modules
{{< rivanna-software moduleclasses=""numlib,lib""  >}}"
rc-website-fork/content/userinfo/hpc/software/smrtlink.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""smrtlink""
softwarename = ""SmrtLink""
title = ""SmrtLink and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Using SmrtLink
On the HPC system, Smrtlink tools can be executed only in non-interactive mode without any graphical user interface.  Several Smrtlink tools support code execution on multiple cpu cores. Some of the common tools include blasr, ngmlr, pbalign, and the pbsv (pbsv align and pbsv call) commands.
In the Slurm job scripts the number of requested cpu cores (per task) is stored in the environment variable SLURM_CPUS_PER_TASK.
PacBio BAM files
The BAM format is a binary compressed format for raw or aligned sequence reads. The associated SAM format is a text representation of the same data (specifications for BAM/SAM).
PacBio-produced BAM files are a fully compatible extension of the BAM specification. In addition to the typical BAM headers, the PacBio BAM header includes @RG (read group) entries with the following tags: ID, PL, PM, PU, DS.  This means that any of the Smrtlink tools that require a PacBio BAM file will not accept any generic BAM files (i.e. non-PacBio BAM files).
A more detailed description of the PacBio BAM format can be found here.
Running blasr
blasr is a read mapping program that maps reads to positions in a genome by clustering short exact matches between the read and the genome, and scoring clusters using alignment. The matches are generated by searching all suffixes of a read against the genome using a suffix array. When suffix array index of a genome is not specified, the suffix array is built before producing alignment. This may be prohibitively slow when the genome is large (e.g. Human). It is best to precompute the suffix array of a genome using the program sawriter, and then specify the suffix array on the command line using -sa genome.fa.sa. Global chaining methods are used to score clusters of matches.
Command line arguments
The only required inputs to blasr are a file of reads (Fasta, PacBio BAM, bax.h5) and a reference genome (Fasta format). Although reads may be input in FASTA format, the recommended input is PacBio BAM files because these contain quality value information that is used in the alignment and produce higher quality variant detection.

--out : specifies the output file. Although alignments can be output in various formats, the recommended output format is PacBio BAM using the --bam option.
--nproc  : sets the number of threads used and is matched to the number of cpu cores requested for the Slurm job.

To get a more complete description of all available command line options run this command:
blasr --help
Slurm script example
{{< pull-code file=""/static/scripts/smrtlink_blasr.slurm"" lang=""no-highlight"" >}}
Running pbalign
pbalign maps PacBio sequences to references using predefined and selectable alignment algorithms (options are blasr, bowtie, or gmap).  The input can be a fasta, pls.h5, bas.h5 or ccs.h5 file or a fofn (file of file names).  The output can be in SAM or BAM format.  If the output is in BAM format, the aligner has to be blasr and QVs will be loaded automatically.
Command line arguments
pbalign expects a minimum of three command line arguments: the name of the sequence input file (set of subreads or unaligned sequences in PacBio BAM format), the path to the reference files (Fasta format), and a filename for the computed alignment results. The blasr algorithm is used by default.

--nproc : Is used to specify how many cpu cores are available for the alignment computation. This value is matched to the number of requested cpu core via the SLURM_CPUS_PER_TASK environment variable.

To get a more complete description of all available command line options run this command:
pbalign --help
Slurm script example
{{< pull-code file=""/static/scripts/smrtlink_pbalign.slurm"" lang=""no-highlight"" >}}
Running ngmlr
ngmlr is a long-read mapper designed to align PacBio or Oxford Nanopore reads to a reference genome. It is optimized for structural variation detection.
Command line arguments
ngmlr requires these command line arguments:

-r :  Specifies the path to the reference genome (FASTA/Q, can be gzipped)
-q :  Specifies the path to the read file (FASTA/Q) [/dev/stdin]
-o :  Specifies the path to output file [stdout]
-t :  Is used to specify how many cpu cores are available for the alignment computation. This value is matched to the number of requested cpu core via the SLURM_CPUS_PER_TASK environment variable.

To get a more complete description of all available command line options run this command:
ngmlr --help
Slurm script example
{{< pull-code file=""/static/scripts/smrtlink_ngmlr.slurm"" lang=""no-highlight"" >}}
Running sawriter
sawriter creates a suffix array from a single or list Fasta input files for a reference genome. The suffix array provides an additional index that increases the performance during the mapping step (e.g. via blasr). This is particularly useful when handling large reference files (i.e. larger than bacterial genomes).
Command line arguments
sawriter expects at least two command line arguments. The first one specifies the output file, e.g. sa_outputfile, the remaining ones specify the input files in Fasta format. At least one Fasta file has to be provided. Multiple input files can be specified by providing additional Fasta files separated by a whitespace at the end of the command.
To get a more complete description of all available command line options run this command:
sawriter --help
Slurm script example
{{< pull-code file=""/static/scripts/smrtlink_sawriter.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/blender.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2019-06-23T08:37:46-05:00""
tags = [
  ""lang"",
]
draft = false
shorttitle = ""Blender""
modulename = ""blender""
softwarename = ""Blender""
title = ""Blender and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Interactive Sessions through Open Ondemand
Interactive sessions of {{% software-name %}} can be launched through the HPC system's web portal, Open OnDemand.
To launch an instance of {{% software-name %}}, you will begin by connecting to our Open OnDemand portal. Your {{% software-name %}} session will run on a Rivanna/Afton GPU node. In addition, you need to specify required resources, e.g. time, your HPC allocation, etc. If you are new to HPC, you may want to read the Getting Started Guide to learn more about the partitions.
Starting an Interactive Session

Open a web browser and go to URL:  https://ood.hpc.virginia.edu.
Use your Netbadge credentials to log in. This will open the Open OnDemand web portal.
On the top banner of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on {{% software-name %}}.
After connecting to {{% software-name %}} through Open OnDemand, a form will appear where you can fill in the resources for {{% software-name %}}. {{% software-name %}} supports GPUs and should be run in the GPU partition.
When done filling in the resources, click on the blue Launch button at the bottom of the form. Do not click the button multiple times.
It may take a few minutes for the system to gather the resources for your instance of {{% software-name %}}. When the resources are ready a Launch {{% software-name %}} button will appear. Click on the button to start {{% software-name %}}.

Using {{% software-name %}}
When {{% software-name %}} opens in your web browser, it will appear just like the {{% software-name %}} that you have on your laptop or desktop.
Closing the Interactive Session
When you are done, quit the {{% software-name %}} application. The interactive session will be closed and the allocated resources will be released."
rc-website-fork/content/userinfo/hpc/software/gnupg.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2020-03-06T00:00:00-05:00""
tags = []
draft = false
shorttitle = ""GnuPG""
modulename = ""gnupg""
softwarename = ""GnuPG""
title = ""GnuPG and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Generate a key
To generate a key, execute the following command:
gpg --full-generate-key
and follow the on-screen instructions.
If it ends abruptly with this message:
gpg: agent_genkey failed: No pinentry
Key generation failed: No pinentry
please follow these steps:

Kill your current gpg-agent
gpgconf --kill gpg-agent
Start the agent with pinentry
gpg-agent --daemon --pinentry-program /usr/bin/pinentry
Run GnuPG
gpg --full-generate-key
It will ask you to ‚Äúperform some other action‚Äù but just wait. After a few seconds you will be asked to create a passphrase in a popup window. Again, wait for a few seconds and retype your passphrase in another popup window. At the end you should see something like this:
```
gpg: /home/mst3k/.gnupg/trustdb.gpg: trustdb created
gpg: key ** marked as ultimately trusted
gpg: directory '/home/mst3k/.gnupg/openpgp-revocs.d' created
gpg: revocation certificate stored as '/home/mst3k/.gnupg/openpgp-revocs.d/*.rev'
public and secret key created and signed.

pub   rsa4096 2020-03-06 [SC] [expires: 2020-03-07]
      ******
uid                      MST3K mst3k@virginia.edu
sub   rsa4096 2020-03-06 [E] [expires: 2020-03-07]
```"
rc-website-fork/content/userinfo/hpc/software/gromacs.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
]
date = ""2025-05-28T00:00:00-05:00""
tags = [
  'container',
  ""chem""
]
draft = false
modulename = ""gromacs""
softwarename = ""GROMACS""
title = ""GROMACS and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Usage on GPU
This module is built with CUDA support. A Slurm script template is provided below.
{{< pull-code file=""/static/scripts/gromacs_gpu.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/perl.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-02-14T00:00:00-05:00""
tags = [
  ""programming""
]
draft = false
modulename = ""perl""
softwarename = ""Perl""
shorttitle = ""Perl""
title = ""Perl and UVA HPC""
description = ""Perl in the HPC environment""
author = ""RC Staff""
+++
Overview
Perl is a general-purpose interpreted programming language, originally developed for text manipulation and now used for a wide range of tasks including system administration, web development, network programming, GUI development, and bioinformatics.
Available Versions
The default Perl is required for system purposes and is generally too old for applications. We offer more recent versions of Perl as modules. To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
CPAN Modules
Users can install their own Perl modules from CPAN via the cpanm command. For instance:
cpanm Test::More
Run cpanm --help for further help.
Example Slurm Script
```
!/bin/bash
SBATCH -N 1
SBATCH -n 1
SBATCH -t 01:00:00
SBATCH -o output_filename
SBATCH -p standard
SBATCH -A mygroup
module load gcc perl
perl myscript.pl
```"
rc-website-fork/content/userinfo/hpc/software/containers.md,"+++
categories = [""userinfo""]
type = ""rivanna""
date = ""2023-05-08T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""docker"", ""containers"", ""singularity""
]
draft = false
title = ""Software Containers""
description = ""Software Containers""
author = ""RC Staff""
+++
Overview
Containers bundle an application, the libraries and other executables it may need, and even the data used with the application into portable, self-contained files called images. Containers simplify installation and management of software with complex dependencies and can also be used to package workflows. 
Please refer to the following pages for further information.

Singularity (before Dec 18, 2023)
Apptainer (after Dec 18, 2023)
Short course: Software Containers for HPC

Container Registries for UVA Research Computing
Images built by Research Computing are hosted on Docker Hub (and previously Singularity Library).
Singularity Library
Due to storage limits we can no longer add Singularity images to Singularity Library. There will be no more updates to this registry.
Docker Hub
In the summer of 2020, we switched to Docker Hub. A complete list of images along with their Dockerfiles can be found in our rivanna-docker GitHub repository. These images may or may not be installed as modules on the HPC system.
We do not use the latest tag. Specify the exact version when pulling an image. For example:
module load apptainer
apptainer pull docker://uvarc/pytorch:1.5.1
Images that contain ipykernel can be added to your list of Jupyter kernels. To verify:
apptainer exec <container_name>.sif python -m pip list | grep ipykernel
If this returns ipykernel    <version>, proceed here.
You are welcome to use/modify our Dockerfiles. Some form of acknowledgment/reference is appreciated."
rc-website-fork/content/userinfo/hpc/software/samtools.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2021-05-14T08:37:46-05:00""
tags = [
  ""multi-core"",
]
draft = false
modulename = ""samtools""
softwarename = ""Samtools""
title = ""Samtools and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}

Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{% module-name %}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Build Your Own Version
Users may build their own versions of {{% software-name %}} if they wish to use a different compiler or software version. Instructions are available on the {{% software-name %}} website.
Convert SAM to BAM with Samtools
samtools view can convert human-readable SAM files to binary BAM files. Below is a simple command to convert SAM files to BAM files. The -S option specifies that the input is in SAM format, while the -b option outputs to a BAM file:
samtools view -bS example.sam > example.bam
To preview the first five lines of the new BAM file:
samtools view example.bam | head
Most downstream analyses require your BAM files to be sorted, which can be achieved by:
samtools sort example.bam -o example_sorted.bam
If you would like to visualize your BAM file using some viewer like IGV, you will need to create an index file
samtools index example_sorted.bam
Finally, samtools flagstat is a good way to get simple statistics from a BAM file including QC, duplicates, mapped reads, and many others
samtools flagstat example_sorted.bam
Slurm Script Example
To run {{% software-name %}} on the HPC system, a script similar to the following can be used.
{{< pull-code file=""/static/scripts/samtools.slurm"" lang=""no-highlight"" >}}
To speed up your code, use multiple cpus per task. Here, we ask for 8 with the --cpus-per-task option, but only specify 7 in our samtools command to leave one for the manager process:
{{< pull-code file=""/static/scripts/samtools_threaded.slurm"" lang=""no-highlight"" >}}"
rc-website-fork/content/userinfo/hpc/software/machine-learning.md,"+++
type = ""rivanna""
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""machine-learning""
]
draft = false
title = ""Machine Learning and UVA HPC""
description = ""Machine Learning and UVA HPC""
author = ""RC Staff""
+++
Overview
Many machine learning packages can utilize general purpose graphics processing units (GPGPUs).  If supported by the respective machine learning framework or application, code execution can be manyfold, often orders of magnitude, faster on GPU nodes compared to nodes without GPU devices.
The HPC system has several nodes that are equipped with GPU devices.  These nodes are available in the GPU partition.  Access to a GPU node and its GPU device(s) requires specific Slurm directives or command line options as described in the Jobs using a GPU Node section.
Applications
Several machine learning software packages are installed on the UVA HPC system.  The most commonly used ones are:

TensorFlow
PyTorch

Other less frequently used software packages include:

Dragonn
LightGBM
XGBoost
"
rc-website-fork/content/userinfo/hpc/software/math-statistics.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2024-01-02T00:00:00-05:00""
tags = [
  ""statistics"",
  ""math""
]
draft = false
shorttitle = ""Math & Statistics""
title = ""Math & Statistics and UVA HPC""
description = ""Math & Statistics Software in the HPC environment""
author = ""RC Staff""
+++
Overview
Many popular math and statistics software packages are available on the HPC system.
General considerations
Available Math & Statistics Software
To get an up-to-date list of the installed math applications, log on to UVA HPC and run the following command in a terminal window:
module keyword math
To get more information about a specific module version, run the module spider command, for example:
module spider mathematica

List of Math & Statistics Software Modules
{{< rivanna-software moduleclasses=""math"" >}}
Using a Specific Software Module
To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module.
After loading a module, you are ready to run the application(s) provided by the module. For example:
module load mathematica"
rc-website-fork/content/userinfo/hpc/software/miniforge.md,"+++
title = ""Miniforge and UVA HPC""
description = """"
author = ""RC Staff""
images = [""""]
date = ""2024-10-23T00:00:00-05:00""
categories = [""userinfo""]
tags = [
    ""HPC"",
    ""software""
]
modulename = ""miniforge""
softwarename = ""Miniforge""
draft = false
+++
Overview
Miniforge provides the Conda and Mamba package managers, with the default channel being conda-forge.
(Mamba is a reimplementation of the Conda package manager in C++ that uses a state-of-the-art library ""libsolv"" for much faster dependency solving.)
We have transitioned from Anaconda to Miniforge on Oct 15, 2024. See here for details.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Installing packages
Packages could be installed via the pip, conda, or mamba package managers
Using pip
Open the bash terminal, and type:

module load miniforge
pip search package_name (search for a package by name)
pip install --user package_name (install a package)
pip update package_name --upgrade (upgrade the package to the latest stable version)
pip list (list all installed packages)

{{% callout %}}
Do not upgrade pip. If you see the following message asking you to upgrade your pip version, it is usually safe to ignore it.
You are using pip version x.x.x, however version y.y.y is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Doing so may result in broken dependencies.
(As of 01/10/2020, this error message is suppressed.)
{{% /callout %}}
However, if you must upgrade pip, please do so in a virtual environment, such as conda.
Using conda {#package-installation-with-conda}
You can specify which version of Python you want to run using conda. This can be done on a project-by-project basis, and is part of what is called a ""Virtual Environment"". A Virtual Environment is simply your isolated copy of Python in which you maintain your own version of files and directories. It enables you to keep other projects unaffected. With projects that have similar dependencies, you can freely install different versions of the same package without worry on two different Virtual Environments. In order to jump between two VE's, you simply activate or deactivate your environment. Follow the steps below:


Set up your Virtual Environment:
conda create -n your_env_name_goes_here (default Python version: use conda info to find out)
OR 
conda create -n your_env_name_goes_here python=version_goes_here (This command will automatically upgrade pip to the latest version in the environment. To find specific Python versions, use conda search ""^python$"".)


If it asks you for y/n, hit y to proceed. It will start the installation

Activate your newly created environment source activate your_env_name_goes_here

Install a package in your activated environment
conda install -n your_env_name_goes_here your_package_name_goes_here
OR 
conda install -n your_env_name_goes_here \ your_package_name_goes_here=version_goes_here
OR (even better)
In your home directory or Conda installation directory, create a file called .condarc (if not already there) Inside the file write the following:
create_default_packages
    - your_package_name_goes_here
    - your_package_name_goes_here
    - your_package_name_goes_here
    ...
Now everytime you create a new environment, all those packages listed in .condarc will be installed.
1. To end the current environment session:
conda deactivate
1. Remove an environment:
conda remove -n your_env_name_goes_here -all
1. To create a JupyterLab tile for your conda environment:
Install ipykernel inside your activated environment:
    conda install -c conda-forge ipykernel
Then, create a new kernel:
    python -m ipykernel install --user --name=MyEnvName
Your new kernel will show up as a tile when you select File-> New Launcher in JupyterLab.


To see all available environments, run conda env list.
{{% callout %}}
Tip: use mamba instead of conda. Conda can take a long time to resolve environment dependencies and install packages. A new tool, mamba, has been developed to speed up this process considerably. Simply replace conda with mamba in any commands used to build an environment or install packages. Then you can still call your environment using source activate <env>.
{{% /callout %}}
Python and MPI
The most widely used MPI library for Python is mpi4py. We recommend creating an environment for it.  When installing on the cluster, please do not use conda install since this will install prebuilt binaries, including a version of MPICH that does not communicate correctly with our Slurm resource manager.  The best practice is to install from the conda-forge channel using their external versions of an MPI library.  These are simply bindings to an MPI package provided by the underlying system.  For our system we will use OpenMPI.
First load a compiler and a corresponding version of OpenMPI.
module load gcc openmpi
For the above example, with the versions of gcc available on our system, this is gcc 11.4.0. Now check the version of OpenMPI.
bash
module list
In this example it is OpenMPI 4.1.4.  Be aware that specific version numbers will change with time.
Now view the available external versions of OpenMPI from conda-forge
bash
conda search -f openmpi -c conda-forge
Install the bindings
bash
conda install -c conda-forge ""openmpi=4.1.4=external_*""
Now you can install mpi4py
bash
conda install -c conda-forge mpi4py
Example Slurm script
Non-MPI
{{< pull-code file=""/static/scripts/python_serial.slurm"" lang=""no-highlight"" >}}
MPI
{{< pull-code file=""/static/scripts/python_mpi.slurm"" lang=""no-highlight"" >}}
Using Conda Environments on Rio
When running Slurm jobs on Rio, it is essential to ensure that all packages and environments installed with miniforge are configured correctly. Rio compute nodes can only run jobs from high-security research standard storage, so it‚Äôs important to ensure that all necessary files and variables point to this location.
The following environment variables should be exported in your ~/.bashrc file to install conda packages and environment into a specific directory (/standard/ivy-xxx-xxxx/path/to/.conda/):
export HTTPS_PROXY=http://figgis-s.hpc.virginia.edu:8080 
export HTTP_PROXY=http://figgis-s.hpc.virginia.edu:8080
export CONDA_PKGS_DIRS=""/standard/ivy-xxx-xxxx/path/to/.conda/pkgs""
export CONDA_ENVS_PATH=""/standard/ivy-xxx-xxxx/path/to/.conda/envs"" 
export CONDA_CACHE_DIR=""/standard/ivy-xxx-xxxx/path/to/.conda/cache"" 
export CONDA_ROOT=""/standard/ivy-xxx-xxxx/path/to/.conda"" 
export XDG_CACHE_HOME=""/standard/ivy-xxx-xxxx/path/to/.conda/cache""
{{% callout %}}
Keep in mind to replace /standard/ivy-xxx-xxxx/path/to/.conda with the path to your .conda
directory in your storage share.
Additionally, You'll want to make sure all of the sub directories exist under .conda (Eg pkgs, envs, and cache)
{{% /callout %}}
Conda environments will need to be created with the --prefix flag. Eg
```
module purge
module load miniforge
conda create --prefix /standard/ivy-xxx-xxxx/path/to/.conda/envs/my_env python=3.11
```
To access the environment on the compute nodes you'll want to export all of the previous commands and  active the full file path to the environment:
source activate /standard/ivy-xxx-xxxx/path/to/.conda/envs/my_env
More Information
Please visit the official website."
rc-website-fork/content/userinfo/hpc/software/_index.md,"+++
date = ""2020-04-15T08:37:46-05:00""
categories = [""userinfo""]
tags = [
  ""rivanna"",
  ""hpc"",
  ""research"",
  ""software"",
  ""containers"",
]
draft = false
title = ""UVA HPC Software""
description = ""High Performance Computing""
author = ""RC Staff""
type = ""rivanna""
layout = ""single""
+++
Overview
{{< rawhtml >}}
Research Computing at UVA offers a variety of standard software packages for all UVA HPC users. We also install requested software based on the needs of the high-performance computing (HPC) community as a whole. Software used by a single group should be installed by that group‚Äôs members, ideally on leased storage controlled by the group. Departments with a set of widely-used software packages may install them to the lsp_apps space. The Research Computing group also provides limited assistance for individual installations.
For help installing research software on your PC, please contact Research Software Support at res-consult@virginia.edu.
{{< /rawhtml >}}
Software Modules and Containers
Software on the HPC system is accessed via environment modules or containers.
{{< rawhtml >}}
Learn about Modules ¬†
Learn about Containers
{{< /rawhtml >}}
Software by Category
Popular Software Packages and Libraries by Domain

Biology & Bioinformatics
Chemistry
Data Science & Machine Learning
Engineering
Image Processing & Scientific Visualization
Math & Statistics

{{< rawhtml >}}
Full list of all software and library modules
{{< /rawhtml >}}
Programming Languages & Tools

Python
Perl
Julia
Matlab
Mathematica
sageMath
R & RStudio
Jupyter Lab
Compilers
Parallel & MPI
Libraries
Debuggers and Profilers
Containers
IDEs and Editors
Workflow Managers

Access & File Transfer Tools

Open OnDemand
FastX
MobaXterm
SSH
Data Transfer
Globus
"
rc-website-fork/content/userinfo/hpc/software/nvhpc.md,"+++
type = ""rivanna""
date = ""2019-04-23T08:37:46-05:00""
tags = [
  ""rivanna"", ""software"", ""compiler"",""gpu""
]
draft = false
title = ""NVHPC""
description = ""Compiling GPU Applications and UVA HPC""
author = ""RC Staff""
+++
Compiling for a GPU
Using a GPU can accelerate a code, but requires special programming and compiling.  Several options are available for GPU-enabled programs.
OpenACC
OpenACC is a standard
Available NVIDIA CUDA Compilers
{{< module-versions module=""cuda"" >}}
{{< module-versions module=""nvhpc"" >}}
GPU architecture
According to the CUDA documentation, ""in the CUDA naming scheme, GPUs are named sm_xy, where x denotes the GPU generation number, and y the version in that generation."" The documentation contains details about the architecture and the corresponding xy value. The compute capability is x.y.
Please use the following values when compiling CUDA code on the HPC system.
| Type       | GPU       | Architecture | Compute Capability | CUDA Version |
|------------|-----------|--------------|--------------------|--------------|
| Datacenter | V100      | Volta        | 7.0                | 9+           |
|            | A100      | Ampere       | 8.0                | 11+          |
|            | A40       | Ampere       | 8.6                | 11+          |
|            | H200      | Hopper       | 9.0                | 11.8+        |
| RTX        | A6000     | Ampere       | 8.6                | 11+          |
| GeForce    | RTX2080Ti | Turing       | 7.5                | 10+          |
|            | RTX3090   | Ampere       | 8.6                | 11+          |

As an example, if you are only interested in V100 and A100:
-gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80"
rc-website-fork/content/userinfo/hpc/software/abinit.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""chemistry""
]
date = ""2019-06-22T08:37:46-05:00""
tags = [
  ""mpi"",
]
draft = false
modulename = ""abinit""
softwarename = ""Abinit""
title = ""Abinit and UVA HPC""
author = ""RC Staff""
+++
Description
{{< module-description >}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
The current installation of {{% software-name %}} incorporates the most popular packages. To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}
Build Your Own Version
Users may build their own versions of {{% software-name %}} if they wish to use a different compiler/MPI combination. Instructions are available on the {{% software-name %}} website. If using the Intel compiler, you need to add the -heap-arrays flag to the Fortran compiler options.
Slurm Script Example
To run {{% software-name %}} on the HPC system, a script similar to the following can be used. {{% software-name %}} has many options so only a basic example is shown.
{{% pull-code file=""/static/scripts/abinit.slurm"" lang=""no-highlight"" %}}
For this example, the text file files has the following content:
gw.in
gw.out
gwi
gwo
gwt
../pseudo/sc-sg15.oncvpsp.psp8
../pseudo/se-sg15.oncvpsp.psp8
../pseudo/mo-sg15.oncvpsp.psp8
../pseudo/w-sg15.oncvpsp.psp8"
rc-website-fork/content/userinfo/hpc/software/cellranger-atac.md,"+++
type = ""rivanna""
categories = [
  ""HPC"",
  ""software"",
  ""bio""
]
date = ""2024-01-02T00:00:00-05:00""
draft = false
modulename = ""cellranger-atac""
softwarename = ""Cell Ranger ATAC""
title = ""Cell Ranger ATAC and UVA HPC""
author = ""RC Staff""
+++
Description
{{% module-description %}}
Software Category: {{% module-category %}}
For detailed information, visit the {{% software-name %}} website.
Available Versions
To find the available versions and learn how to load them, run:
module spider {{< module-name >}}
The output of the command shows the available {{% software-name %}} module versions.
For detailed information about a particular {{% software-name %}} module, including how to load the module, run the module spider command with the module's full version label. For example:
module spider {{% module-firstversion %}}
{{< module-versions >}}"
rc-website-fork/content/userinfo/hpc/ood/fileexplorer.md,"+++
description = """"
title = ""Open OnDemand: File Explorer""
draft = false
date = ""2019-06-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""gpu"",""allocations"",""queues""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
Open OnDemand provides an integrated file explorer to browse and manage small files. Rivanna and Afton have multiple locations to store your files with different limits and policies. Specifically, each user has a relatively small amount of permanent storage in his/her home directory and a large amount of temporary storage (/scratch) where large data sets can be staged for job processing. Researchers can also lease storage that is accessible on Rivanna. Contact Research Computing or visit the storage website for more information.
The file explorer provides these basic functions:

Renaming of files
Viewing of text and small image files
Editing text files
Downloading & uploading small files

To see the storage locations that you have access to from within Open OnDemand, click on the Files menu. The drop-down list will show your Rivanna/Afton /home directory and possibly other leased storage volumes like /project that your group has access to. Clicking on any of them will open a new tab with a file browser for that storage location. The user interface is divided into two panes:

The left pane shows the directory tree at the current storage location.
The right pane shows the content of a particular directory that has been selected in the left window pane. Above the right window pane are rows of buttons that allow you to execute specific file operations.

Renaming Files
To rename an existing file or directory, click on it in the right window pane and click the Rename button. In the pop-up window, modify the file/directory name and click OK.
Viewing Files
To view existing text and image files (.png, .tif, etc.), select the file in the right window pane and clicking on the View button. The file content or image is shown in a pop-up window.
Editing Files
To edit an existing text file, e.g. source code file or a job script, select the file in the right window pane and click the Edit button. The file is opened in a new browser tab labelled as Editor. The editor shows line numbers and supports syntax highlighting for common programming languages (e.g. Python, R, Matlab, XML, markdown, Bash, etc.). A specific syntax highlighting can be chosen under the Mode drop-down menu. To save any changes, click on the Save button in the top left corner of the Editor tab.
Opening Files in Terminal
Clicking on the Open Files in Terminal button opens a terminal window in a new web browser tab. The current directory is set to the location directory or file that was selected in the File Explorer's left or right window pane.  Note that this terminal is not able to start graphical applications such as the Matlab desktop; for applications such as those you must use FastX.
Navigating to other Storage Locations
To navigate to other file locations on Rivanna/Afton, you can use the Go To button to enter a specific storage volume and directory path. For example, if you are in your home directory and want to go to your /project directory, enter /project/ and click OK. This will show a list of all project directories including those of other research groups.  You can also enter the full path to your Project storage, e.g. /project/my-storage, to go straight to your group's storage. To find out about the full path of all your leased storage locations, run the hdquota command in a Rivanna terminal window.
You can only access project directories associated with your leased storage and MyGroup.
File Transfer
The Open OnDemand file explorer should only be used to transfer small files such as your source code and job scripts.
File Upload to the HPC System
To upload files from your current workstation (i.e. the computer that the web browser runs on) to the HPC system, choose the directory on Rivanna/Afton to which the files should be uploaded. Then click on the Upload button at the top of the File Explorer window. This will produce a small pop-up window with a Choose Files button. Clicking the Choose Files button opens a file browser window showing the storage locations accessible on your local workstation.  Select a single file or multiple files and click the Choose or OK button. This will initiate the file transfer and close the file browser window on your local workstation. The uploaded files will appear as new or updated files in the current directory shown in the right pane of the Open OnDemand File Explorer.
File Download from the HPC System
To download files from the HPC system to your local workstation, select the files or directories from download in the main (right) window pane of the Open OnDemand File Explorer and click the Download button. The selected files and directories are immediately downloaded in the usual way depending on your Web browser; typically it will copy the files to your Download directory.
Alternative File Transfer Tools
Large files (> 2GB) should be transferred with scp/sftp either from a standard shell or from an application such as MobaXterm (Windows), or Cyberduck or Filezilla (Windows, Mac, Linux), or via the Globus transfer tool. Globus is a browser-based file transfer tool optimized for fast, fault-tolerant file transfers that run in the background once started. To use Globus with the HPC system please follow the instructions at our Globus documentation page. A link to the Globus web application can be found on the top of the file explorer window."
rc-website-fork/content/userinfo/hpc/ood/desktop.md,"+++
type = ""rivanna""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
]
date = ""2019-06-23T08:37:46-05:00""
tags = [
  ""lang"",
]
draft = false
shorttitle = ""Open OnDemand Desktop""
title = ""Open OnDemand Desktop""
description = ""Desktop environment for compute-intensive applications with graphical user interface (GUI)""
author = ""RC Staff""
+++
Overview
The Open OnDemand Desktop app provides a full Linux Desktop environment launched on user-specified allocated hardware resources which may include a compute node equipped with graphical processing units (GPUs). 
{{% callout %}}
This is the preferred mechanism to start compute intensive applications that require a graphical user interface (GUI) on the HPC system.
{{% /callout %}}
Accessing the Desktop App
To access the app and start a desktop session, connect to our Open OnDemand portal:

Open a web browser and go to https://ood.hpc.virginia.edu.
Use your Netbadge credentials to log in.
On the top right of the menu bar of the Open OnDemand dashboard, click on Interactive Apps.
In the drop-down box, click on Desktop.


Requesting an Instance
Your instance of the Desktop app will run on a HPC compute node. So it will need a list of resources, such as partition, time, and allocation. If you are new to UVA HPC, you may want to read the HPC User Guide to learn more about the partitions.


After connecting to JupyterLab through Open OnDemand, a form will appear where you can fill in the resources for the Desktop session.


Partition: UVA HPC has different types of compute nodes that are organized in partitions based on the type of processing they can do.  Most of the time you will select the Standard or Dev partition.  If you are running machine or deeplearning models that support GPUs, you will want to use the GPU partition.
Number of hours: The number of hours defines the amount of time that your session will be active.  Beware--when time runs out the session will end without warning.
Allocation (SUs): An allocation is a special Grouper (requires VPN connection) group that holds the service units you may use for your computation.  You may be a member of multiple allocation groups.

When done filling in the resources, click on the blue ‚ÄúLaunch‚Äù button at the bottom of the form.


It may take some time for the system to find and allocate the requested resources.  When the resources are ready a Launch Desktop button will appear. Click on the button and the Desktop session will open in a new tab.



The Desktop Environment 


"
rc-website-fork/content/userinfo/hpc/ood/jobcomposer.md,"+++
description = """"
title = ""Open OnDemand: Job Composer""
draft = false
date = ""2019-06-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""gpu"",""allocations"",""queues""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
Open OnDemand allows you to submit Slurm jobs to the cluster without using shell commands.
The job composer simplifies the process of:

Creating a script
Submitting a job
Downloading results

Submitting Jobs
We will describe creating a job from a template provided by the system.


Open the Job Composer tab from the Open OnDemand Dashboard.


Go to the New Job tab and from the dropdown, select From Template. You can choose the default template or you can select from the list.


Click on Create New Job. You will need to edit the file that pops up, so click the light blue Open Editor button at the bottom. Replace your allocation with your group name and click Save.


Open OnDemand creates a unique directory for each job. In most cases, you will need to upload or move files into the job directory, so when you have finished editing the script, return to the Job Composer tab and click the darker blue Open Dir button at the bottom of the page.


You may now use the File Explorer to upload or move the files you will need to run the job.


When you have finished preparing your job, click the Submit button. Your job will be submitted. Any errors will appear at the top of the page.


The Job Composer main panel allows you to monitor your job status. When your job has completed, select the job from the list so that it is highlighted. Click the Open Dir button again to enter the directory. There you may view or download your result files."
rc-website-fork/content/userinfo/hpc/ood/_index.md,"+++
description = """"
title = ""Open OnDemand""
draft = false
date = ""2019-06-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""gpu"",""allocations"",""queues"",""ood""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
layout = ""single""
+++
Overview
Open OnDemand is a graphical user interface that allows access to UVA HPC via a web browser. Within the Open OnDemand environment users have access to a file explorer; interactive applications like JupyterLab, RStudio Server & FastX Web; a command line interface; and a job composer and job monitor.
Logging in to UVA HPC
The HPC system is accessible through the Open OnDemand web client at https://ood.hpc.virginia.edu. Your login is your UVA computing ID and your password is your Netbadge password. Some services, such as FastX Web, require the Eservices password. If you do not know your Eservices password you must change it through ITS by changing your Netbadge password (see instructions).


Open OnDemand can be accessed from off Grounds without the UVA VPN client, but FastX Web requires it.

The Dashboard
Once you log in, you are automatically redirected to the Open OnDemand dashboard. The dashboard may contain information about upcoming changes and maintenance work that can affect your jobs, so please read all announcements carefully. Keep the dashboard tab open until you are ready to end your session and log out.

The dashboard provides access to all Open OnDemand services and applications. These include

File Explorer
Interactive Applications
Command Line Interface (Shell)
Job Composer & Job Monitor

These services and applications are accessible through drop-down boxes on the menu bar. When you click on any of the drop-down options, a new tab will open in your browser.
File Explorer
File Explorer makes browsing and managing small files easy. Rivanna and Afton have multiple locations to store your files with different limits and policies. Specifically, each user has a relatively small amount of permanent storage in his/her home directory and a large amount of temporary storage (/scratch) where large data sets can be staged for job processing. Users can also lease storage that is accessible on Rivanna. Contact Research Computing or visit the storage page for more information.
The file explorer provides these basic functions:

Renaming of files
Viewing of text and small image files
Editing of text files
Downloading & uploading of small files

Visit our File Explorer guide for detailed instructions.
Interactive Applications
Open OnDemand provides access to interactive applications that provide a full graphical HPC desktop environments, JupyterLab for running Jupyter notebooks, RStudio Server, Matlab, a simple terminal shell, and a variety of other research apps.
Desktop
The Desktop app provides a full Linux Desktop environment launched on user-specified allocated hardware resources which may include a compute node equipped with graphical processing units (GPUs). This is the preferred mechanism to start compute intensive applications that require a graphical user interface (GUI).
Please read the Open OnDemand Desktop documentation for detailed instructions on how to specify resources and start a desktop session.
FastX Web
FastX Web enables users to start an X11 desktop environment on a remote system. When launched through Open OnDemand, FastX Web provides access to a HPC frontend. The FastX Web desktop environment can be used to open conventional shell terminals or launch applications with a graphical user interface. 
Please read our FastX Web documentation for a detailed description of this remote desktop environment.
FastX Web sessions are not suitable for running compute intensive applications or code--Open OnDemand Desktop is intended for such purpose.
JupyterLab
JupyterLab provides an environment that has become popular for interactive code development and debugging. JupyterLab sessions run on user-specified allocated hardware resources which may include compute nodes equipped with graphical processing units (GPUs).
Please read the JupyterLab documentation for detailed instructions on how to start JupyterLab sessions and specify hardware resource requests.
After starting a JupyterLab session, you're taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner.
RStudio Server
RStudio provides an environment specifically designed for interactive R script development and debugging.
Please read the RStudio Server documentation for detailed instructions on how to start RStudio sessions and specify hardware resource requests.
After starting an RStudio session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner.
Matlab
The Matlab interface provides an environment for interactive Matlab script development and debugging.
Please read the Matlab documentation for detailed instructions on how to start Matlab sessions and specify hardware resource requests.
After starting an interactive Matlab session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner.
Blender
Through this interface users can run interactive Blender sessions on a dedicated GPU node.
Please read the Blender documentation for detailed instructions on how to start Blender sessions and specify hardware resource requests.
After starting an interactive Blender session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner.
ParaView
Through this interface users can run interactive ParaView sessions on a dedicated GPU node.
Please read the ParaView documentation for detailed instructions on how to start ParaView sessions and specify hardware resource requests.
After starting an interactive ParaView session, you are automatically taken to the ""My Interactive Sessions"" page. To return to the Open OnDemand dashboard, click on the Home link in the top left corner of the site just below the Research Computing banner.
Command Line Interface (Shell)
To open a conventional command line terminal window, click on the Clusters drop-down menu and select Rivanna Shell Access. A new tab opens that provides a Bash command line environment.  This is similar to logging in through ssh but with the limitation that it cannot start graphical (X11) applications.  You must use FastX for X11 applications such as the Matlab desktop.
Utilities
In addition to interactive apps, UVARC offers several static applications found in the 'Utilities' dropdown menu on Open OnDemand. These applications serve as wrappers for existing bash commands and scripts users can run in the shell.
Disk Usage
The Disk Usage app provides information on the remaining storage in each project directory available to the user, as well as in their /home and /scratch directories. Acting as a wrapper for the hdquota command, it displays the size and available storage in each quota.
Queue Status
The Queue Status app provides detailed information on jobs queued and running in each partition. It acts as a wrapper for the qlist command and displays data on the total cores, free cores, jobs running, jobs pending, time limits, and SU charges for each partition.
Check Scratch For Purge
According to UVA RC policy, files in the /scratch directory that have not been accessed for over 90 days will be permanently deleted or 'purged'. The Check Scratch For Purge app allows you to see which files are at risk of being purged and download a list of their filenames. It displays the output of the command check-scratch-for-purge, showing a list of files ordered from the oldest last accessed to the most recently accessed.
Slurm Script Generator
The Slurm Script Generator helps users write   Slurm scripts for their jobs. It is available on Open OnDemand and the RC Website. This tool assists users in creating a Slurm script tailored to the specifications of a given job. It also calculates the estimated amount of Standard Units (SUs) needed to run the job.
Job Composer
The Job Composer is an easy way to submit general-purpose jobs.  You can copy pre-existing templates and modify them for your application, then submit a job at the click of a few buttons.  It works with the File Explorer to allow you to upload or move files you need for your job, and to download your results.
Visit our Job Composer documentation for details."
rc-website-fork/content/userinfo/hpc/logintools/fastx.md,"+++
description = """"
title = ""FastX Web Portal""
draft = false
date = ""2019-06-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""parallel-computing"",""gpu"",""allocations"",""queues""]
categories = [""userinfo""]
images = [""""]
author = ""Staff"" 
type = ""rivanna""
+++
Overview
FastX is a commercial solution that enables users to start an X11 desktop environment on a remote system. It is available on the UVA HPC frontends. Using it is equivalent to logging in at the console of the frontend.
Using FastX for the Web
We recommend that most users access FastX through its Web interface. To connect, point a browser to:
https://fastx.hpc.virginia.edu
{{< off-campus >}}

Login Screen

After entering your computing ID and Netbadge password, you will see a launch screen.

Launch
In this example, we have no pre-existing sessions so we must create one. Click the Launch Session button. This will bring up a screen showing the options.


Launch MATE
Most users will choose the MATE desktop. 

Click on the green MATE icon. Text showing the choice will appear in the box below it.
Click the Launch button to start your session. 

If you are running a popup blocker in your browser, a request will appear that you unblock this site. Once you do so, you can click the button to continue to your session. After a short wait, your desktop will appear.


Desktop
The toolbar at the top controls FastX behavior. If the desktop does not automatically expand to the browser screen, the user can click the double arrow. The pushpin pins the toolbar to the screen.


Logout
When you are done, you can log out by selecting Logout from the System menu. This will terminate your FastX Web session.


Resume
If you close the browser tab with the desktop, your session will be suspended rather than terminated. You can go back to the launch tab and click the thumbnail of your desktop. To resume the session, click the arrow (play) button.


Terminate Session
To terminate the session, either select Terminate from the Actions dropdown menu, or click the close symbol. Please terminate sessions if you do not plan to use them in the near future."
rc-website-fork/content/userinfo/hpc/logintools/cl-data-transfer.md,"+++
date = ""2022-02-14T11:45:12-05:00""
tags = [
        ""data-transfer"",
        ""sftp""
        ]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
description = """"
title = ""Command Line Data Transfer""
draft = false
+++
Standard Linux tools can efficiently transfer a small to moderate quantity of data to or from Rivanna/Afton.

scp
scp uses the secure shell (SSH) protocol to transfer files between your local machine and a remote host, or between two remote hosts.
The following syntax enables copying from local to remote or vice versa. In both cases we are starting from the local system.
By default, scp works from the level of the directory in which it is invoked.

Copying from local to remote: scp source_file mst3k@hostaddress:target_file
Copying from remote to local: scp mst3k@hostaddress:source_file target_file

The following examples detail how to transfer data between your local computer and /project storage on Rivanna/Afton. In these examples

my_file is the file you would like to transfer
mst3k is your computing ID
mygroup_name is the name of your /project directory.
my_directory is the directory to which you wish to copy the file.

To copy a file
From your computer to /project storage:
scp my_file mst3k@login.hpc.virginia.edu:/project/mygroup_name
From /project storage to my_directory on your computer:
scp mst3k@login.hpc.virginia.edu:/project/mygroup_name/my_file /my_directory
scp accepts wildcards.  In this example, the mycode directory must exist in your scratch directory.
scp *cxx mst3k@login.hpc.virginia.edu:/scratch/mst3k/mycode
scp Options
The -r option recursively copies directories.
From your computer to /project storage:
scp -r my_directory mst3k@login.hpc.virginia.edu:/project/mygroup_name
From /project storage to your computer:
scp -r mst3k@login.hpc.virginia.edu:/project/mygroup_name /target_directory
The -p option preserves modification time, access time, and ownership from the original file.
The -q option suppresses the progress and debugging messages. Useful for scripts.
sftp
Secure FTP or sfpt is an interface built on top of scp to mimic the behavior of ftp.
To connect to UVA HPC with sftp, execute the following in the command line interface:
sftp mst3k@login.hpc.virginia.edu
When prompted, enter your password. Once the connection succeeds, you will see the sftp prompt:
sftp>
Navigating Directories
You can access both your local and remote file systems with sftp. The following table lists how to execute the following commands for both your local and remote systems.
| Action                    |On Remote System  | On Local System |
| :-------                  |:----------:      |   :-----:       |
|Print Working Directory    |    pwd           |     lpwd        |
|List Contents of Directory |    ls            |     lls         |
|Change Directory           |    cd            |     lcd         |
File Transfer from Local to Remote
To transfer files from your computer to the Rivanna/Afton file system, use the put command:
sftp> put my_file
To transfer a folder from your computer to Rivanna/Afton, use put -r. A folder with the same name must also exist on Rivanna. An example is shown below:
sftp> mkdir /project/mygroup_name/my_folder
sftp> cd /project/mygroup_name
sftp> put -r my_folder
File Transfer from Remote to Local
To transfer files from Rivanna/Afton to your computer, use the get command:
sftp> get my_file
To transfer a folder from Rivanna/Afton to your computer, use get -r:
sftp> get my_folder
Terminating the Connection
To terminate the sftp connection, use exit.
sftp> exit
rsync
Remote sync is a powerful tool for copying files.  It is most widely used to transfer multiple files and/or directories.
In this example, we have a local directory ldir and a remote directory rdir and we wish to copy the contents of ldir to rdir. 
rsync -r ldir/ mst3k@login.hpc.virginia.edu:rdir
The trailing / after ldir is important.  Without it, ldir and its contents would be placed under rdir.
Unlike scp, if the target directory does not exist, rsync will create it.  
It is more common to use the -a (archive) option.  This option preserves symbolic links, special files, ownership, permissions, and timestamps.
rsync -a ldir/ mst3k@login.hpc.virginia.edu:rdir
Show a progress bar and keep partially transferred files
rsync -Pa ldir/ mst3k@login.hpc.virginia.edu:rdir
Delete files not present on the source directory if they are present on the target directory
rsync -Pa --delete ldir/ mst3k@login.hpc.virginia.edu:rdir
Have rsync print the list it will transfer without carrying out the transfers.  Especially important when using --delete.
rsync -Pa --delete --dry-run ldir/ mst3k@login.hpc.virginia.edu:rdir
AWS CLI
The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services, including data transfer.
Learn more about the AWS CLI tools.
Globus CLI
The Globus command-line interface can also be used to orchestrate the transfer of large datasets, or to script regular transfers in or out of systems.
Read more about the Globus CLI.
Usage from Off Grounds
{{< off-campus >}}"
rc-website-fork/content/userinfo/hpc/logintools/rivanna-ssh.md,"+++
description = """"
title = ""ssh on UVA HPC""
draft = false
date = ""2022-08-30T11:45:12-05:00""
tags = [""rivanna"",""login"",""hpc"",""ssh"",""cli""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
The secure shell ssh is the primary application used to access the HPC system from the command line.
Connecting to a Remote Host
For Windows, MobaXterm is our recommended ssh client; this package also provides an SFTP client and an X11 server in one bundle.
Mac OSX and Linux users access the cluster from a terminal through OpenSSH, which are preinstalled on these operating systems. Open a terminal (on OSX, the Terminal application) and type
ssh -Y mst3k@login.hpc.virginia.edu
where mst3k should be replaced by your user ID. You will generally need to use this format unless you set up your user account on your Mac or Linux system with your UVA ID. 
Please note that ssh will not echo your password or move your cursor as you type.
Mac users will need to install XQuartz in order to use graphical applications through a shell (the -Y option will permit this).
Passwordless ssh using keys
Sometimes you will need to enable passwordless ssh. We allow passwordless ssh to frontend nodes from UVA IP addresses. Key 
authentication works by matching two halves of an encrypted keypair. The ""public"" key is placed within your home directory on the 
remote server and the ""private"" key is kept safely on your own workstation. You should treat private keys as securely as you would
any password.
Windows
In MobaXterm, click the Tools icon or menu and select MobaKeyGen. Keep it as RSA and leave the passphrase blank. Save the public key under a name of your choice. MobaXterm will display the public key. Copy this key to your clipboard. Continue as for ""All Operating Systems.""
Mac OSX and Linux
Open a terminal and type
ssh-keygen
Accept all defaults. When it asks for a passphrase, hit Enter to keep it blank. Open the file id_rsa.pub and copy its contents to your clipboard.
Graphical Installation, All Operating Systems
Log in to UVA HPC,
cd .ssh
Note the period in front of ssh. Then, using a text editor, open the file authorized_keys. Append the key you copied previously. Use the middle mouse button or scroll wheel to paste it into the authorized_keys file if you are using MobaXterm. Otherwise right-click and select paste.  Be sure there are no line breaks in the key.
Command-Line Transfer (Mac and Linux)
Transfer the id_rsa.pub file to the HPC system with scp:
scp ~/.ssh/id_rsa.pub mst3k@login.hpc.virginia.edu:~/.ssh/mykey.pub
Log in to UVA HPC through a terminal, then type
cat ~/.ssh/mykey.pub >> ~/.ssh/authorized_keys
Passwordless ssh Between Nodes
If you are permitted to use passwordless ssh between HPC compute nodes, such as for ANSYS, follow the instructions for Mac and Linux but generate the key directly on a UVA HPC frontend. Use the cat command to append the key to your authorized_keys file.
Troubleshooting


When you log in to a new host, ssh will ask whether you wish to accept the host key. You must answer yes explicitly in order to proceed.


When off Grounds, you must use the UVA Anywhere client in order to connect to on-Grounds resources. If you do not, your attempt to use ssh will hang with no messages.


A relatively short period of inactivity may cause ssh connections to time out.  Mac OSX and Linux users can reduce this by setting a configuration value. At the terminal change to your ~/.ssh directory
cd ~/.ssh
Use a text editor to create a file called config. Place the following lines in it:
Host *
   ServerAliveInterval 60
There should be one or more spaces at the beginning of the second line.


MobaXterm users should see the documentation for instructions to enable KeepAlive.


When in doubt, you can obtain more information by running ssh with the -v (verbose) flag.
ssh -v -Y mst3k@login.hpc.virginia.edu

A common error message from ssh is when a host key changes, such as after an upgrade. This will appear as a message containing lines such as
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
MobaXterm will typically detect this and ask whether you want to change the host key; you may answer yes. On Mac OSX or Linux, from a terminal go to your ~/.ssh directory and use a text editor to open the file known_hosts. Remove all lines that might refer to UVA HPC. Alternatively, just delete the entire file; it will be recreated as you log in to different hosts. If you are unfamiliar with using a command line on Mac, you must
cd 
cd .ssh
After that you must either edit the known_hosts file with a text editor to remove the invalid lines, either through a command-line editor or with
open known_hosts
(The above command only works on Macs, not Linux.)
"
rc-website-fork/content/userinfo/hpc/logintools/mobaxterm.md,"+++
description = """"
title = ""MobaXterm""
draft = false
date = ""2019-05-28T17:45:12-05:00""
tags = [""hpc"",""rivanna"",""supercomputer"",""login"",""ssh"",""mobaxterm"",""sftp""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
MobaXterm is the recommended login tool for Windows users.  It bundles a tabbed ssh client, a graphical drag-and-drop sftp client, and an X11 window server for Windows, all in one easy-to-use package.  Some other tools included are a simple text editor with syntax coloring and several useful Unix utilities such as cd, ls, grep, and others, so that you can run a lightweight Linux environment on your local machine as well as use it to log in to a remote system.
Download
To download MobaXterm, click the link below. Select the ""Home"" version, ""Installer"" edition, 
Download MobaXterm
Run the installer as directed.
Connecting


When you start MobaXterm you can create a new session, restart a saved one, or attach to an existing one. To start a new one, begin with ssh. 



Remember to connect to login.hpc.virginia.edu with your Eservices username and password. SSH key authentication is also supported.


You can prevent premature ssh timeouts by accessing Settings->SSH and making sure the box labeled ""SSH keepalive"" is checked.



When you start an ssh session, MobaXterm will automatically start a sftp session with a file browser.  You can double-click files on the remote host and they will open if the appropriate application is found on your local computer.



MobaXterm by default automatically allows X11 to pass through ssh; no special options are required when you log in.  You can run individual X11 (graphical) applications simply by starting them.  Remember to type an ampersand (&) at the end of the command.  For example, to edit a simple text file using gedit, type
gedit &



This can be slow, especially off Grounds, so for extensive work with graphical applications you may prefer FastX.


When you are logged in to a Unix system (like the UVA HPC system), MobaXterm will utilize the Unix X11 convention for cut and paste.  Highlighting text with the left mouse button selects it automatically.  Clicking the middle mouse button pastes it.  If you do not have a middle mouse button or scroll wheel, such as on a laptop, clicking both left and right buttons simultaneously emulates a middle mouse button.


Access from Off Grounds
{{< off-campus >}}"
rc-website-fork/content/userinfo/hpc/logintools/filezilla.md,"+++
date = ""2022-02-14T11:45:12-05:00""
tags = [
        ""data-transfer"",
        ""sftp"",
        ""filezilla"",
        ""rivanna""
        ]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
description = """"
title = ""Filezilla""
draft = false
+++
Filezilla is a cross-platform data transfer tool.  The free version supports FTP, FTPS, and SFTP.  Only SFTP can be used with UVA HPC.
Download
Download Filezilla
Connecting to the HPC System and File Transfer


Launch FileZilla. After launching FileZilla, the user interface will open. In the left panel, you should see your local file system and files listed in the left side panels. You will enter your login credentials in the fields highlighted in the figure below.



Enter Your Credentials. Fill in the Host, Username, Password, and Port fields.
Host: login.hpc.virginia.edu
    Username: your computing ID
    Password: your Eservices password
    Port: 22
When completed, click Quickconnect.


Click OK on Warning. When connecting for the first time, a warning like the one shown below. Check the box next to ‚ÄúAlways trust this host, add this key to the cache‚Äù, and then click OK.



After successfully connecting, your home directory and files on the HPC system should appear in the right-side panels, as shown below. To transfer a file, simply double-click the filename in the panel or right-click on the file and select the Upload option. To transfer a folder, right-click on the folder and select Upload.



Access from Off Grounds
{{< off-campus >}}"
rc-website-fork/content/userinfo/hpc/logintools/graphical-sftp.md,"+++
description = """"
title = ""Graphical SFTP/SCP Transfer Tools""
draft = false
date = ""2020-01-22T09:54:12-05:00""
tags = [""hpc"",""rivanna"",""supercomputer"",""data"",""scp"",""sftp""]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
type = ""rivanna""
+++
Several options are available to transfer data files between a local computer and the HPC system through user-friendly, graphical methods.
{{< off-campus >}}
MobaXterm

MobaXterm is a Windows application that combines an ssh client for logging in, a graphical secure-copy client for easy drag-and-drop file transfer, and an X11 server for displaying graphical applications.
MobaXterm

FileZilla

FileZilla is a cross-platform application for drag-and-drop data transfer.  It is available for Windows, Mac OSX, and Linux.
FileZilla

Cyberduck

Cyberduck is another cross-platform application for drag-and-drop data transfer.  It is available for Mac OSX and Windows.
Cyberduck

Open OnDemand

The File Explorer in Open OnDemand can be used to upload and download small files.  Open OnDemand can be accessed from any Web browser through Netbadge, and does not require the installation of an application, but the number and size of files that can be transferred is limited.
Open OnDemand
Remote Mounting for Research Project and Research Standard Storage Shares
Leased storage (Research Project and Research Standard) can be mounted as remote drives to Windows and Mac OSX computers."
rc-website-fork/content/userinfo/hpc/logintools/cyberduck.md,"+++
date = ""2022-02-14T11:45:12-05:00""
tags = [
        ""data-transfer"",
        ""sftp"",
        ""cyberduck"",
        ""rivanna""
        ]
categories = [""userinfo""]
images = [""""]
author = ""Staff""
description = """"
title = ""Cyberduck""
draft = false
+++
Cyberduck is a transfer tool for Windows and Mac. It supports a large number of transfer targets and protocols.  Only SFTP can be used with Rivanna/Afton.  The free version will pop up donation requests.

Download
Download Cyberduck
Connecting to the HPC System and File Transfer


Launch Cyberduck. After launching Cyberduck, the user interface will open. To initiate a connection to UVA HPC, click the Open Connection button.



Enter Your Credentials. From the drop-down menu, select SFTP (SSH File Transfer Protocol). Then enter the appropriate information in the following fields:


Host: login.hpc.virginia.edu
    Username: your computing ID
    Password: your UVA HPC password
    Port: 22
When completed, click Connect.



After successfully connecting to UVA HPC, the contents of your UVA HPC home directory will appear in the user interface.


Navigate to the directory to which you would like to transfer the files.To move to the higher level directories, use the highlighted drop-down menu. Transfer your local file or directory to Rivanna/Afton by dragging and dropping it to the Cyberduck user interface.



There are two ways to transfer files from Rivanna/Afton to your local host.
--Method 1: Drag and Drop
Simply drag your desired file or directory from the Cyberduck user interface to the target directory on your local machine. 
--Method 2: Download To‚Ä¶
    Right-click on the file or directory that you want to transfer.
    Select the Download To‚Ä¶ option.
    Select the target directory on your machine where you want to download the file. 


Once your transfer is initiated, a pop-up window will then appear to inform you of the status of your transfer.
Access from Off Grounds
{{< off-campus >}}"
rc-website-fork/content/userinfo/howtos/ivy/secure-globus-transfer.md,"+++
type = ""howto""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""ivy"",
  ""data"",
  ""globus"",
  ""howto""
]
date = ""2020-03-26T08:37:46-05:00""
tags = ['howtos']
draft = false
shorttitle = ""Secure DTN""
title = ""Highly Sensitive Data Transfer with Globus""
description = ""Using the secure DTN to transfer data to Ivy""
author = ""RC Staff""
+++


Login In to Globus and Choose Ivy Collection 


Disconnect your High Security VPN ‚Äì it will interfere with the high security network that Globus uses.

Open https://www.globus.org/ and sign in through the University of Virginia using NetBadge.
Once you are on the Transfer Files page, at the first ""Collection"" type ivy to search.  Find the uva#ivy-DTN collection and select it.
You will then be asked to authenticate.

Once signed in, simply click through the names of your shares until you find the source or destination for your file transfers.


Navigate to Your Personal Collection


In another pane repeat the process of searching for a collection, but use the name of your personal collection.
  NOTE: Ivy is designed for security for research on sensitive data. Sensitive data should never be stored on or transmitted from personal laptops.

Once you have moved your data to High-Security Research Standard Storage using Globus, you may need to copy it to your Ivy VM Storage. Double-click the Desktop icon labeled ‚ÄúHigh-Security Research Standard Storage‚Äù to see the files you transferred with Globus (a file explorer window will appear). Double-click the Desktop icon labeled ‚ÄúVM Storage‚Äù to see the files that currently exist in your VM storage. To copy data from High-Security Research Standard Storage to the VM, you can drag files from the High-Security Research Standard Storage file explorer window to the VM storage file explorer window, or you can use traditional copy+paste functions. The decision to work off of your data on the local hard drive versus on the High-Security Research Standard Storage drive should be driven by the size of your data, and how much your research needs to read from and write to it.

For More Information
To learn more about setting up and using a Globus personal collection, please see our documentation."
rc-website-fork/content/userinfo/howtos/ivy/_index.md,"+++
type = ""howto""
date = ""2020-02-21T15:12:46-05:00""
tags = [
  ""howto"",
  ""ivy"",
]
categories = [""howto""]
draft = false
title = ""How To Guides for Ivy Users""
description = ""How Tos for Ivy""
author = ""RC Staff""
layout = ""single""
+++

Transfer Files to or from Ivy Using Globus
"
rc-website-fork/content/userinfo/howtos/general/redis.md,"+++
type = ""howto""
date = ""2023-06-12T00:00:00-05:00"" 
tags = [ ""Rivanna"", ""databases"", ""howto"", ""redis"", ""data"", ""nosql"" ]
category = [""howto""]
draft = true 
title = ""Redis - A Key/Value Store"" 
description = ""Basic usage of Redis"" 
author = ""RC Staff""
+++
¬´ Return to Databases
{{% callout %}}


redis is an in-memory, key/value store. Think of it as a dictionary with any number of keys, each of which has a value
that can be set or retrieved. However, Redis goes beyond a simple key/value store as it is actually a data structures server, 
supporting different kinds of values. Some fundamental concepts:


Can be used as a database, cache, or message broker
  Supports multiple data types and structures
  Built-in replication
  Keys can be up to 512MB in size

{{% /callout %}}
{{< ref ""/userinfo/howtos/general/databases.md"" >}}
Getting Started
HPC nodes can connect to external Redis databases hosted in Kubernetes or a public cloud (AWS, Azure, GCP, etc.)
To use Redis from the command-line, use the redis-cli. In the HPC system, this is a module:
$  module load redis-cli
You can now create a connection to the server. Use port 6379 (the default port). No password is required:
$  redis-cli -h redis.uvarc.io
Basic Operations
As a dictionary, Redis allows you to set and retrieve pairs of keys and values. Think of a ""key"" as a unique
identifier (string, integer, etc.) and a ""value"" as whatever data you want to associate with that key. Values
can be strings, integers, floats, booleans, binary, lists, arrays, dates, and more.
```
KEY                 VALUE

hello               world
1234                5678
1a2b3c              /path/to/file.csv
124a                AGCCCCTCAGGAGTCCGGCCACATGGAAACTCC
```
{{% callout %}}
Note! The ""value"" half of a Redis key/value pair can be quite large - 512MB.
This is considerably larger than other popular NoSQL databases such as DynamoDB or MongoDB.
{{% /callout %}}
To view all keys (once you have established a connection to the Redis server):
redis.uvarc.io:6379> keys *
1) ""hello""
2) ""1234""
3) ""1a2b3c""
4) ""124a""
Then use a specific key to fetch its value:
redis.uvarc.io:6379> get hello
""world""
To set a new key/value:
redis.uvarc.io:6379> set herman melville
OK
Set an expiring key/value (EX in seconds, PX in milliseconds)
redis.uvarc.io:6379> set jane eyre EX 30
OK
Delete a key/value:
redis.uvarc.io:6379> del herman
OK
Working Alongside Other Users
Redis allows for the creation and management of multiple databases, called ""indexes"". By default new connections are attached
to index 0 but this can be changed to the integer of another index. Keys/values stored in one index are unavailable to another
index. Use select to move between indexes. There are 64 total indexes in this implementation.
redis.uvarc.io:6379> select 0
OK
redis.uvarc.io:6379> set hello world
OK
redis.uvarc.io:6379> get hello
""world""
redis.uvarc.io:6379> select 1
OK
redis.uvarc.io:6379[1]> get hello
(nil)
Indexes need not be created in order. We suggest you select a high arbitrary number (0 to 63) for a private index. Populate and empty it
as you find necessary. However, in the standard security environment remember that your keys/values are visible to other UVA HPC
users.
To connect to the Redis endpoint and specify an index other than 0, use the -n flag with the integer of the index. The redis-cli
prompt will indicate when you are using a non-zero index:
$ redis-cli -h redis.uvarc.io -n 17
redis.uvarc.io:6379[17]>
Advanced Operations

Data Types & Structures
Values are not constrained
Lists
Sets
Incremental Counters
Command Repetition
Random Keys

Data Types & Structures
In addition to strings and integers, Redis supports the following data types and data manipulations:

Lists
Sets
Hashes
Increments
Command repetition
Random Keys
Sorted sets
Secondary indexes
Scripts

Values are not constrained
Remember that the ""value"" half of a key/value pair does not have to contain only a single value. It can essentially be populated
with multiple, separated values, so long as you can anticipate the order, and identity of those values. In this way a
key/value is akin to a ""row"" of a comma-separated data file.
To implement this functionality, you have two options:


Use a hash. Hashes in Redis store multiple objects within the same key, i.e. sets of key/value pairs within a single key/value pair.
Hashes are named, then subkeys and their values are defined:
redis.uvarc.io:6379> hset hash-key subkey1 value1 subkey2 value2
OK
Then fetch all values:
redis.uvarc.io:6379> hgetall hash-key
1) ""subkey1""
2) ""value1""
3) ""subkey2""
4) ""value2""
Fetch a specific field:
redis.uvarc.io:6379> hget hash-key subkey2
""value2""


Store your payload as JSON. Redis will store your JSON data as one long string, which you can then parse:
redis-cli -h redis.uvarc.io --raw
redis.uvarc.io:6379> set json_key '{""eventType"": ""purchase"", ""amount"": 5, ""item_id"": ""XXX""}' 
OK
redis.uvarc.io:6379> keys *
""json_key""
redis.uvarc.io:6379[5]> get json_key
{""eventType"": ""purchase"", ""amount"": 5, ""item_id"": ""XXX""}
Note the use of the --raw`` flag when invoking theredis-cli` tool. This ensures that response data is
decoded back to UTF-8 instead of bytes.


Lists
Create a list by pushing a value into it:
redis.uvarc.io:6379> LPUSH dbs redis 
(integer) 1 
redis.uvarc.io:6379> LPUSH dbs mongodb 
(integer) 2 
redis.uvarc.io:6379> LPUSH dbs mysql 
(integer) 3 
redis.uvarc.io:6379> LPUSH dbs mysql 
(integer) 4
Pushing a new value into a list gives the new value the 0 position of the list. (To add new values
to the end of the list use the RPUSH command.) List values can be duplicated within the list.
Get a list range back by defining the min and max indices you want:
redis.uvarc.io:6379> LRANGE dbs 0 10
1) ""mysql"" 
2) ""mysql""
3) ""mongodb"" 
4) ""redis""
redis.uvarc.io:6379> LRANGE dbs 0 1
1) ""mysql""
2) ""mysql""
You can also LPOP, LPUSH, and LTRIM as well as RPOP, RPUSH, and RTRIM with Redis lists.
Sets
You can populate a set within a single key. Set members already present cannot be duplicated within the set:
redis.uvarc.io:6379> sadd set1 bananas
(integer) 1
redis.uvarc.io:6379> sadd set1 apples
(integer) 1
redis.uvarc.io:6379> sadd set1 grapes
(integer) 1
redis.uvarc.io:6379> sadd set1 bananas
(integer) 0
Then retrieve the set members:
redis.uvarc.io:6379> smembers set1
1) ""grapes""
2) ""apples""
3) ""bananas""
Incremental Counters
Use Redis as a counter or tracker:
redis.uvarc.io:6379> set counter 1
OK
redis.uvarc.io:6379> incr counter
(integer) 2
redis.uvarc.io:6379> incr counter
(integer) 3
Increment by integers other than 1:
redis.uvarc.io:6379> get counter
1
redis.uvarc.io:6379> incrby counter 3
(integer) 4
redis.uvarc.io:6379> incrby counter 6
(integer) 10
Command Repetition
If you need the same command to be repeated N times, simply preface your command with that integer:
redis.uvarc.io:6379> set counter 1
OK
redis.uvarc.io:6379> 5 incr counter
(integer) 2
(integer) 3
(integer) 4
(integer) 5
(integer) 6
Random Keys
Using a database populated with keys and values, some workflows could make use of this as a queue for jobs
or batches to be processed when order does not matter. Your process can fetch a random key:
redis.uvarc.io:6379> randomkey
""herman""
redis.uvarc.io:6379> get herman
""melville""
Working with redis in Code
Redis has many available SDKs for most modern languages. Every operation available via the cli is available in those SDKs.
Some popular choices:

Python
C++
R
MATLAB
Perl
Go
Others

Use redis in Your Research
We are frequently asked by researchers how to incorporate databases into their work. Here are four suggestions for how Redis might help your research::

Queue - Have a list of datafiles or batches that need processing? Redis supports queues in two ways:
Load a Redis index with identifiers and let jobs retrieve single values at a time. Each job, when complete, removes that key from the table, working its way until the queue is empty.
For less demanding processes, write your HPC job to loop through values in a Redis index to fetch identifiers and process them in series as part of one SLURM job.
Use Redis as a simple Pub/Sub message broker. This model de-couples message producers from message receivers, and allows for multiple of each.


Cache - Store interim results or data for use in later computation. This is a faster and more scalable replacement for temporary text files.
Dictionary - Use an extended key/value store as an in-memory lookup resource for reference values. Where you may have previously stored reference values in a text file or relational DB table, Redis would likely outperform that pattern. Transactions with Redis are also atomic, which means multiple keys can be set, retrieved, or modified at the same time without risking data concurrency.

Other Resources

Redis Documentation
Try Redis Online
Redis Cheatsheet
"
rc-website-fork/content/userinfo/howtos/general/sshkeys.md,"+++
type = ""howto""
date = ""2020-03-20T00:00:00-05:00"" 
tags = [ ""Rivanna"", ""login"", ""howto"" ] 
category = [""howto""]
draft = false 
title = ""SSH Keys"" 
description = ""Authentication with SSH Keys"" 
author = ""RC Staff""
+++
{{% callout %}}
Users can authenticate their SSH sessions using either a password or an ssh key. The instructions below describe how to create a key and use it for password-less authentication to your Linux instances.
{{% /callout %}}
About SSH Keys
SSH keys are a pair of encrypted files that are meant to go together. One half of the pair is called the ‚Äúprivate‚Äù key, and the other half is the ‚Äúpublic‚Äù key. When users use the private key to connect to a server that is configured with the public key, the match can be verified and the user is signed in. Or, put it more simply, when data is encrypted using one half of the key, it can be decrypted using the other half.
The most important thing to remember about SSH key pairs is to NEVER share or distribute the private half. That should remain safely and securely with you. Anyone with possession of that key can potentially sign in to other systems as you.
Public keys, by contrast, can be shared widely.
Create an SSH keypair
From a terminal or command prompt (Linux and macOS) issue this command:
ssh-keygen
If you receive an error, you may need to install the openssl package.
This command will prompt you for a name and location of the key pair. By default, the key is usually named id_rsa and is placed within a hidden .ssh folder within your personal directory.
When creating the key, you will be asked if you would also like to secure it with a password. This is optional, but should be used in high security environments.
After key generation you will find two new files in your .ssh directory:
id_rsa
id_rsa.pub
The .pub file is your public key. Note that the private key has restricted permissions,
-rw------- (600).
Authenticate SSH using keys
To use your SSH keypair for authentication, you need to do two things:
Copy the public key to your destination server - First, cat out your public key, and copy it to your clipboard. Then SSH into your destination server using a password as normal. Within the .ssh/ folder on the remote server, you should find a file named authorized_keys. (If you do not, create one.) And then paste your public key into that file. Be sure the key is entirely on only one line. Then log out.
Use key authentication for SSH connections - Second, when you invoke the ssh client from your local workstation, use the -i flag to specify your identity file (i.e. ssh key). So while a normal SSH connection looks like this (prompting you for a password):
ssh foo9b@login.hpc.virginia.edu
You should now instead use something like this (that requires no password):
ssh -i ~/path/to/file foo9b@login.hpc.virginia.edu
You can add an alias in as a new line in your .bashrc file for easy logins, for example:
rivanna='ssh -i ~/path/to/file foo9b@login.hpc.virginia.edu'
Key Expiration
One risk of SSH keys is that they have no expiration date or specific lifespan. Be sure to rotate out older keys on a regular basis. We suggest swapping out keys every 90-180 days.
Windows Users


GitBash - Download and install GitBash 1, which allows you to run Linux-style commands such as ssh (for secure shell connections) and ssh-keygen to generate keypairs.


SSH in Chrome Browser - Run an SSH client within the Chrome browser. Install the extension here 2 and launch. Works with Linux-compatible SSH keys, and profiles can be saved.


PuTTY - Download the full PuTTy package and generate an RSA keypair at least 2048 bits in size using the PuTTYGen. The files it creates are in a unique format but work perfectly well when used with the PuTTY SSH client.

"
rc-website-fork/content/userinfo/howtos/general/mysql.md,"+++
type = ""howto""
date = ""2023-06-10T00:00:00-05:00"" 
tags = [ ""rivanna"", ""databases"", ""howto"", ""rdbms"", ""data"", ""mysql"" ]
category = [""howto""]
draft = true 
title = ""MySQL - A basic relational database"" 
description = ""Basic usage of MySQL"" 
author = ""RC Staff""
+++
¬´ Return to Databases
{{}}
{{% callout %}}


MySQL is an open-source relational database management system. It supports standard SQL syntax and models. Some important
concepts are:

Tables
  Rows
  Keys
  Schemas
  Data types
  Selects
  Joins
  Indexes
  Other CRUD operations


{{% /callout %}}
Getting Started
After submitting a request for a MySQL database, a username and password be created for you, this information along with your endpoint name will be sent
to you via our ticketing system. Store this information somewhere secure, and do not share this information with others.
User: <your-db-username>
Pass: <your-db-password>
Host: <mysql-shared-endpoint-name>
Port: 3306
The MySQL service is backed by a pair of servers in HA replication mode. One serves as the primary for READS and WRITES, and
the read-replica can be used for READ queries only. These endpoints are available only within the HPC networks and cannot be accessed 
from elsewhere in the UVA WAN. You cannot use MySQL tools remotely (from University offices, labs, or home offices over VPN).
To use MySQL from the command-line, use the mysqlclient module on the HPC system:
$  module load mysqlclient
Or use the appropriate library for the language you are coding in to establish a connected client.
You can now create a connection to the server. Use port 3306 (the default port):
$  mysql -h <mysql-endpoint-name> -u <your-username> -p
Password: ***********"
rc-website-fork/content/userinfo/howtos/general/docker-basics.md,"+++
type = ""howto""
date = ""2020-03-09T00:00:00-05:00"" 
tags = [ ""software"", ""containers"", ""howtos"" ] 
categories = [""howto""]
draft = false 
title = ""Docker - The Basics"" 
description = ""Docker - The Basics"" 
author = ""RC Staff""
+++
{{% callout %}}
Note that Docker requires sudo privilege and therefore it is not supported on the HPC system. To use a Docker image you will need to convert it into Apptainer. More information can be found here on our website.
{{% /callout %}}
What Is Docker?
""Docker is a set of platform-as-a-service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating-system kernel and are thus more lightweight than virtual machines. The service has both free and premium tiers. The software that hosts...""
Continue reading on Wikipedia
Click to watch on YouTube:

Install Docker
Docker is available for Windows, Mac, and Linux. Download the appropriate Docker Edition for your platform directly from Docker. We suggest the CE ‚ÄúCommunity Edition.‚Äù
Finding Containers
There are thousands of pre-built containers already available for common use cases. If you need a web server, a database instance, or portions of a genomics pipeline, there is probably a container ready for you to use.
Here are some good places to search for container images or docker files.

Docker Hub
BioContainer
GitHub

Running Containers
If you have found a container you would like to try, download it (using the nginx web server as an example):
docker pull nginx
View a list of all container images you have pulled:
```
docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
whalesay            latest              188e03692c84        25 hours ago        277 MB
rocker/rstudio      latest              919e13c956b8        2 weeks ago         990 MB
nginx               latest              6b914bbcb89e        3 weeks ago         182 MB
hello-world         latest              48b5124b2768        2 months ago        1.84 kB
docker/whalesay     latest              6b362a9f73eb        22 months ago       247 MB
```
Run a container image:
docker run -d nginx
This runs the container as a daemon (service). But you may want to expose the container to a specific port locally, so that you can interact with it. For example, if you wanted to expose nginx locally over port 80, enter this:
docker run -d -p 8080:80 nginx
The -p 8080:80 flag publishes your local computer‚Äôs port 8080 with the container‚Äôs port 80.
Another useful flag for runtime is a volume mapping, so that your running container can read or write to portions of your local computer‚Äôs filesystem. So, extending the earlier command:
docker run -d -p 8080:80 -v /User/local/dir:/var/www/html nginx
View all running containers:
```
docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS                    NAMES
1d17f542be53        rocker/rstudio      ""/init""                  18 hours ago        Up 18 hours               0.0.0.0:8787->8787/tcp   elegant_banach
```
You can also run containers interactively (i.e. logging in) instead of running as a service. This allows you to explore the structure, features, or configuration of a container, or modify how it works:
docker run -it nginx /bin/bash
This runs the container interactively (-i) in a pseudo-TTY (-t), and instantiates a shell for your session to use. Once you are done, simply exit the shell and you will leave the container and return to your local computer‚Äôs shell. If you have made any changes to the container, be sure to save it using docker commit (see here for more info).
https://asciinema.org/a/108394
Creating Containers
If you cannot find just the right container, you can always build your own. There are two ways to do this:


Pull Images and Customize - Download a container image, run it and log into it, and customize as if it were your own custom virtual machine. Then, save the container for later deployment. Instructions for interactively logging into a container can be found above.


Pull a base container you want to start with, such as Ubuntu, CentOS, Amazon Linux, Yocto, etc.


Run the container interactively so that you can install packages and code, and customize the image from within.


Finally, when you exit the container and stop it, save it using the docker commit command. At this point your updated container is versioned (much like a git repository) and can be pushed to Docker Hub if you want to share or store it.




Write your own Dockerfile - Alternatively, you can write a custom Dockerfile and build the container from scratch, using docker build. More on Docker files and builds can be found at https://docs.docker.com/reference/dockerfile/. This allows Dockerfiles to be shared as snippets of code rather than as full container images, comparable to a bootstrapping script you might use when instantiating a virtual server instance.

Step 1 - Create a text file called Dockerfile with contents such as:



```
Use an official Python runtime as a base image
FROM python:3.7-slim
Set the working directory to /app
WORKDIR /app
Copy the current directory contents into the container at /app
ADD . /app
Install any needed packages specified in requirements.txt
RUN pip install -r requirements.txt
Make port 80 available to the world outside this container
EXPOSE 80
Define environment variable
ENV NAME World
Run app.py when the container launches
CMD [""python"", ""app.py""]
```
- **Step 2** - Then build your container based on your Dockerfile:

docker build -t mycontainer .

Tutorials
Play with Docker Classroom - Hands-on labs
Docker for Beginners - Covers the basics of container management, execution, modification, etc.
Docker Training - Docker documents this process in great detail, and provides a step-by-step overview of their container system.


Next Steps

Learn about Docker Swarms for deploying containers in high availability.
Design Docker Stacks for complex solutions of services.
Learn how to convert Docker images into Apptainer to run on the HPC system.
"
rc-website-fork/content/userinfo/howtos/general/_index.md,"+++
type = ""howto""
date = ""2020-02-21T15:12:46-05:00""
tags = []
categories = [""howto""]
draft = false
title = ""General How To Guides""
description = ""Tips and Tricks""
author = ""RC Staff""
layout = ""single""
+++

User Guides



Docker - Basics
Authentication with SSH Keys
"
rc-website-fork/content/userinfo/howtos/rivanna/bioinfo-on-rivanna.md,"+++
type = ""howto""
author = ""Staff""
description = """"
title = ""Bioinformatics Resources and UVA HPC""
date = ""2020-11-17T09:48:06-05:00""
draft = false
tags = [""bioinformatics"",""genomics"", ""rivanna"",""tools""]
categories = [""howto""]
images = [""""]
+++
{{% lead %}}
The UVA research community has access to numerous bioinformatics software installed directly or available through the bioconda Python modules.
Click here for a comprehensive list of currently-installed bioinformatics software.
{{% /lead %}}
Popular Bioinformatics Software
Below are some popular tools and useful links for their documentation and usage:











Tool
Version
Description
Useful Links




BEDTools
2.26.0
BEDTools utilities allow one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF.

Homepage
Tutorial



BLAST+
2.7.1
BLAST+ is a suite of command-line tools that offers applications for BLAST search, BLAST database creation/examination, and sequence filtering.

Web BLAST
Manual



BWA
0.7.17
BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM

Homepage
Manual



Bowtie2
2.2.9
Bowtie2 is a memory-efficient tool for aligning short sequences to long reference genomes.

Homepage
Manual


FastQC
0.11.5
FastQC is a Java application that generates a comprehensive quality control report for raw sequencing data.

Homepage
Documentation



GATK
4.0.0.0
The Genome Analysis Toolkit provide tools for variant discovery. In addition to SNP and INDEL identification in germline DNA and RNAseq data, GATK tools include somatic short variant calling, as well as tackle copy number and structural variation.

User Guide


Picard
2.1.1
Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.

Homepage
Documentation



SAMTools
1.7
SAMTools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format.

Homepage
Manual


SPAdes
3.10.1
SPAdes provide pipelines for assembling genomes from Illumina and IonTorrent reads, as well as hybrid assemblies using PacBio, Oxford Nanopore and Sanger reads. It supports paired-end reads, mate-pairs and unpaired reads. 

Homepage
Manual



STAR
2.5.3a
Spliced Transcripts Alignment to a Reference (STAR) is a RNA-seq aligner based on an algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure.

Homepage


vsearch
2.7.1
VSEARCH (stands for Vectorized Search) is a toolkit for nucleotide sequence analyses, including database search and clustering algorithms. It supports clustering, chimera detection, database searching, merging of paired-end reads, and other sequence manipulation tools.

Homepage






Bioinformatics Modules
To get an up-to-date list of the installed bioinformatics applications, log on to UVA HPC and run the following command in a terminal window:
module keyword bio
If you know which package you wish to use, you can look for it with
module spider <software>
For example,
module spider bcftools
This returns
```

bcftools:
Description:
  SAMtools is a suite of programs for interacting with high-throughput
  sequencing data. BCFtools - Reading/writing BCF2/VCF/gVCF files and
  calling/filtering/summarising SNP and short indel sequence variants

 Versions:
    bcftools/1.3.1
    bcftools/1.9


For detailed information about a specific ""bcftools"" module (including how to
load the modules) use the module's full name.
  For example:
 $ module spider bcftools/1.9


```
Available versions may change, but the format should be the same.
To obtain more information about a specific module version, including a list of any prerequisite modules that must be loaded first, run the module spider command with the version specified; for example:
module spider bcftools/1.3.1
Using a Specific Software Module
To use a specific software package, run the module load command. The module load command in itself does not execute any of the programs but only prepares the environment, i.e. it sets up variables needed to run specific applications and find libraries provided by the module.
After loading a module, you are ready to run the application(s) provided by the module. For example:
module load bcftools/1.3.1
bcftools --version
Output:
bcftools 1.3.1
Using htslib 1.3.1
Copyright (C) 2016 Genome Research Ltd.
License GPLv3+: GNU GPL version 3 or later
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
You will need to include the appropriate module load commands into your Slurm script.
General Considerations for Slurm Jobs
Most bioinformatics software packages are designed to run on a single compute node with varying support for multi-threading and utilization of multiple cpu cores.  Many can run on only one core.  In that case, please request only a single task.
Some software is multithreaded.  Usually it communicates the number of threads requested through a command-line option.  In this case the Slurm job scripts should contain the following two SBATCH directives:
```
SBATCH -N 1                    # request single node
SBATCH --cpus-per-task=     # request multiple cpu cores
``
Replace` with the actual number of cpu cores to be requested. Requesting more than 8 cpu cores does not provide any significant performance gain for many bioinformatics packages. This is a limitation due to code design rather than a UVA HPC constraint.
Please be certain that the number of cores you request matches the number you communicate to the software.  To be certain, you can often use the environment variable SLURM_CPUS_PER_TASK.  For example,
biofoo -n ${SLURM_CPUS_PER_TASK}
You should only deviate from this general resource request format if you are absolutely certain that the software package supports execution on more than one compute node.
Reference Genomes on the HPC system {#reference-genomes-on-hpc-system}
Research Computing provides a set of ready-to-use reference sequences and annotations for commonly analyzed organisms in a convenient, accessible location on Rivanna: 
/project/genomes/

The majority of files have been downloaded from Illumina's genomes repository (iGenomes), which contain assembly builds and corresponding annotations from Ensembl, NCBI and UCSC. Each genome directory contain index files of the whole genome for use with aligners like BWA and Bowtie2. In addition, STAR2 index files have been generated for each of Homo Sapiens (human) and Mus musculus (mouse) genomic builds. 
Click the radio button for the genome of your choice, then click the clipboard icon to copy it.  On Rivanna please use the right click method to paste.
{{% reference-genomes %}} 
"
rc-website-fork/content/userinfo/howtos/rivanna/docker-images-on-rivanna.md,"+++
type = ""howto""
date = ""2020-02-21T15:12:46-05:00"" 
tags = [ ""rivanna"", ""software"", ""containers"" ] 
categories = [""howto""]
draft = false 
title = ""Docker Images on the HPC System"" 
description = ""How to use Docker images on the HPC System"" 
author = ""RC Staff""
+++
Docker requires sudo privilege and therefore it is not supported on the HPC system. To use a Docker image you will need to convert it into Apptainer.
Convert a Docker image
There are several ways to convert a Docker image:

Download a remote image from Docker Hub
Build from a local image cached in Docker daemon
Build from a definition file (advanced)

Instructions are provided in each of the following sections.
Docker Hub
Docker images hosted on Docker Hub can be downloaded and converted in one step via the apptainer pull command:
module load apptainer
apptainer pull docker://account/image
Use the exact same command as you would for docker pull.
Docker daemon
(Taken from Apptainer 3.5 User Guide)
You can convert local Docker images into Apptainer (e.g. on your personal computer).
Suppose you have the godlovedc/lolcow:latest image cached by Docker:
$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
godlovedc/lolcow    latest              577c1fe8e6d8        16 months ago       241MB
You can build an Apptainer image from it via:
$ apptainer build lolcow_from_docker_cache.sif docker-daemon://godlovedc/lolcow:latest
INFO:    Starting build...
Getting image source signatures
Copying blob sha256:a2022691bf950a72f9d2d84d557183cb9eee07c065a76485f1695784855c5193
 119.83 MiB / 119.83 MiB [==================================================] 6s
Copying blob sha256:ae620432889d2553535199dbdd8ba5a264ce85fcdcd5a430974d81fc27c02b45
 15.50 KiB / 15.50 KiB [====================================================] 0s
Copying blob sha256:c561538251751e3685c7c6e7479d488745455ad7f84e842019dcb452c7b6fecc
 14.50 KiB / 14.50 KiB [====================================================] 0s
Copying blob sha256:f96e6b25195f1b36ad02598b5d4381e41997c93ce6170cab1b81d9c68c514db0
 5.50 KiB / 5.50 KiB [======================================================] 0s
Copying blob sha256:7f7a065d245a6501a782bf674f4d7e9d0a62fa6bd212edbf1f17bad0d5cd0bfc
 3.00 KiB / 3.00 KiB [======================================================] 0s
Copying blob sha256:70ca7d49f8e9c44705431e3dade0636a2156300ae646ff4f09c904c138728839
 116.56 MiB / 116.56 MiB [==================================================] 6s
Copying config sha256:73d5b1025fbfa138f2cacf45bbf3f61f7de891559fa25b28ab365c7d9c3cbd82
 3.33 KiB / 3.33 KiB [======================================================] 0s
Writing manifest to image destination
Storing signatures
INFO:    Creating SIF file...
INFO:    Build complete: lolcow_from_docker_cache.sif
Note that this requires sudo privilege. You can then transfer the image to the HPC system.
Definition file
If you are building from a definition file, you can bootstrap from a Docker base container.
Bootstrap: docker
From: account/image:version
...
Use a Docker image
After you have obtained the converted *.sif Apptainer image, you can use it by:
module load apptainer
apptainer run|exec|shell image.sif
Please visit this page for more information on run, shell, and exec."
rc-website-fork/content/userinfo/howtos/rivanna/load-module-in-jupyter.md,"+++
type = ""howto""
date = ""2024-11-01T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""jupyter""
]
categories = [""howto""]
draft = false
title = ""Loading Module in Jupyter""
description = ""How to load a module in Jupyter""
author = ""RC Staff""
+++
Users cannot load modules inside a JupyterLab session. If you need access to modules, please request a desktop session instead of JupyterLab. Fill out the form as you normally would for JupyterLab. After you get to a desktop, open a terminal (next to Firefox in the top bar) and type these commands:
module load jupyterlab
module load ... # your modules here
jupyter-lab
This should start up Firefox shortly. If you accidentally close the window, right-click on the link in the terminal and choose ""open link"" to restart.
An example of using LaTeX inside a JupyterLab session is shown in the screenshot below.

Note: While you can load modules in the native terminal window within JupyterLab, it only applies to the terminal tab and has no effect on the notebook tab."
rc-website-fork/content/userinfo/howtos/rivanna/make.md,"+++
type = ""howto""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
  ""howto""
]
date = ""2022-02-14T08:37:46-05:00""
tags = [""compilers"",""software"",""hpc""]
draft = false
shorttitle = ""Make""
title = ""The Make Tool""
description = ""Compiling and linking programs with make""
author = ""RC Staff""
+++
Overview
Make is a program used primarily on Unix systems to manage compiling and linking (building) programs written in C, C++, Fortran, or other compiled languages.  Make operates on targets using rules to create those targets.  It has a set of built-in rules but users may write their own or override the default rules.   Make scans the dependencies of each target looking for files newer than the target.  If it finds them, it recreates the target.   Targets may and usually do depend on other targets; make will work its way through the chain to rebuild the final target, which is typically an executable.
To utilize the program, the user types
make
Make looks first for a file called makefile.  If it does not find that it will look for Makefile.  If neither is present, it will attempt to use its default rules, which is seldom successful.  Users can name their file other than makefile or Makefile but then must invoke make with the -f option
make -f filename
The name Makefile (capital M) is most frequently used on Unix because it will stand out in a directory where other files are entirely or mostly lower case.
Make is often used in conjunction with autoconf.  Autoconf uses a script called configure to generate a Makefile.  In most cases, the configure script will be provided by the developer of a particular program.   The configure script usually takes several optional arguments, including an option --prefix=/path/to/installation which will allow users to install to a location other than the default, which is usually /usr/bin and is not writeable by ordinary users.
Another popular build system is cmake.  Cmake is more similar to autoconf than to make, since on Unix it creates a Makefile which must then be executed.
Basics of Make
The Makefile must follow a rigid format.  The target must start in the first column of a line and must be terminated with a colon (:).  Any dependencies, i.e. files required to create this target, must follow the colon as a space-separated list on a single line.  The rules required to create the target from the dependencies must follow on separate lines and each rule line must begin with a tab character.
Example:
myexec: file1.o file2.o
<tab>g++ file1.o file2.o
file1.o: file1.cxx
<tab> g++ -c file1.cxx
file2.o: file2.cxx
<tab> g++ -c file2.cxx
Because some patterns occur repeatedly, make supports suffix rules, which describe how to create targets from certain files.  For example, a suffix rule to compile any Fortran file ending in .f90 would be written
```
.SUFFIXES .f90:
.f90.o:
gfortran -c $<
``
The$<` special variable stands for the current target.
Make supports variables.  Normally collected at the top of the Makefile, these are conventionally written in all capitals.
F90=gfortran
Our suffix rule could then be expressed as
.f90.o:
<tab> $(F90) -c $<
Variables make it easy to
Make on the HPC system
Users who write their own code and need to generate a Makefile can start with the makemake script.  It is local to the HPC system and should automatically be in the path so it is sufficient to type
makemake
This will create a skeleton Makefile.  The user must at minimum assign a value to the PROG variable
PROG=myexec
Usually it will also be necessary to change the compiler names to that actually used, especially for Fortran programs.
The version of makemake installed on the HPC system attempts to create a Makefile valid for C, C++, and Fortran programs.  Any lines in the Makefile not pertinent to the user's application (such as C++ for a Fortran program or vice versa) may be deleted.
It is important to note that makemake is not intelligent.  It simply collects all files it finds in a directory that end in the suffices .f, .f90, .c, .cxx, and a few others.  Any of those files that are not compilable, for example because they are included into another source file, must be removed from the SRCS and OBJS lists.
The makemake script also creates a special target, called a dummy, clean.  Typing make clean removes the executable and all object (.o) files, as well as any .mod files for Fortran.
Users should make clean every time compiler options are changed.
Special note for Fortran Programs
Modern Fortran programs typically use modules.  Make is not very good at determining correct dependency chains with modules and may not rebuild when modules are changed.  If this happens it will be necessary to make clean when module files are altered."
rc-website-fork/content/userinfo/howtos/rivanna/mpi-howto.md,"+++
type = ""howto""
date = ""2020-02-21T15:12:46-05:00""
tags = [
  ""rivanna"", ""software"", ""mpi""
]
categories = [""howto""]
draft = false
title = ""Building and Running MPI Code""
description = ""MPI on the HPC System""
author = ""RC Staff""
+++
Building an MPI Code
All implementations provide wrappers around the underlying compilers that simplify compilation.   As it is very important to use the headers that correspond to a given library, users are urged to make use of the wrappers whenever possible.   For OpenMPI and MVAPICH2 these are:

mpicc (C)
mpicxx (C++)
mpif90 (Fortran free or fixed format)

For Intel MPI these use gcc/g++/gfortran by default, which is generally not recommended; to use the Intel compilers the corresponding wrappers are:

mpiicc
mpiicpc
mpiifort

Note: At this time, we recommend MPI users build with Intel 18.0 and IntelMPI 18.0
no-highlight
module load intel/18.0
module load intelmpi/18.0
Most MPI programs are distributed for Linux with a Makefile or a means to create a Makefile, such as configure or cmake.  If the Makefile can be edited directly, it usually contains variables such as CC, CXX, FC or F90, or similar that are set to the compiler to be used.  It is only necessary to use the appropriate wrapper as the compiler.  For configure or cmake, it may be necessary to export environment variables, e.g.
export CC=mpicc
before running the command.  Users should refer to the installation documentation for the code they wish to build.
The same wrappers should also be used to link the program, since they automatically link the correct MPI library for the chosen compiler and implementation.  When using the wrappers as the linker, any Makefile variables such as MPILIB should be left blank.
Users who have difficulty building an MPI code not already present on the system can contact RC for assistance.
Running MPI Codes
MPI programs are invoked under the control of an executor; without invoking an executor only a single process will be instantiated, so it is equivalent to running a serial executable.
Running on Uncontrolled Systems
On a system not controlled by Slurm, the executors are usually mpirun or mpiexec (these are typically equivalent). They take a number of options, including the number of processes and a file containing a list of the hostnames on which to run them.
Users are permitted to run short (approximately 10 minutes or less), small (4-8 processes) testing/debugging runs on the UVA HPC frontends.  Multi-node frontend runs are not permitted.  It is not necessary to specify a hostfile if all processes are run on the same system.  To run a test job on a frontend with four processes, first load the appropriate modules and then type
mpirun -np 4 ./mycode
On the frontends the processes will not be assigned to specific cores and may be competing with other processes, so performance may be poor.
To use a debugger with an MPI program, compile with the -g flag as for a serial code.  We provide the Totalview graphical debugger for MPI and OpenMP applications. Totalview requires that the mpiexec executor be in your path before you invoke it.  If you need to debug for a longer time, with a large number of cores, or with multiple nodes, you can use Totalview through the Open OnDemand Desktop. Please request all cores for the node whether you use them or not, because Totalview cannot use the mpirun command as the executor.
Running Under Slurm
When running with Slurm, the mpirun command must be used as the executor.  Load the appropriate modules in your script, then invoke
mpirun ./mycode
Do not specify the number of processes or the list of hosts since mpirun will obtain that information from your request to Slurm and will distribute your processes on the nodes and cores to which your job was assigned.
This example is a Slurm job command file to run a parallel (MPI) job using the OpenMPI implementation:
```
!/bin/bash
SBATCH --nodes=2
SBATCH --ntasks-per-node=16
SBATCH --time=12:00:00
SBATCH --output=output_filename
SBATCH --partition=parallel
SBATCH -A mygroup
module load gcc
module load openmpi
mpirun ./parallel_executable
``
In this example, the Slurm job file is requesting two nodes with sixteen tasks per node for a total of 32 processes.  Both OpenMPI and IntelMPI are able to obtain the number of processes and the host list from Slurm, so these are not specified.  In general, MPI jobs should use all of a node so we'd recommend--ntasks-per-node=20` on the parallel partition, but some codes cannot be distributed in that manner so we are showing a more general example here.
Please see our MPI software page for examples of Slurm scripts for more complex situations, including running hybrid MPI/OpenMP codes.
For more detailed instructions on building and running compiled codes on the HPC system, please see our online tutorial."
rc-website-fork/content/userinfo/howtos/rivanna/mpi-tutorial.md,"+++
type = ""howto""
date = ""2020-02-21T15:12:46-05:00""
tags = [
  ""rivanna"", ""software"", ""mpi""
]
categories = [""howto""]
draft = false
title = ""A Short MPI Tutorial""
description = ""Beginning MPI""
author = ""RC Staff""
+++
Tutorials and books on MPI
A helpful online tutorial is available from the Lawrence Livermore National Laboratory. The following books can be found in UVA libraries:

Parallel Programming with MPI by Peter Pacheco.
Using MPI : Portable Parallel Programming With the Message-Passing Interface by William Gropp, Ewing Lusk, and Anthony Skjellum.
Using MPI-2: Advanced Features of the Message-Passing Interface by William Gropp, Ewing Lusk, and Rajeev Thakur.
MPI: The Complete Reference : The MPI Core by Marc Snir, Steve Otto, Steven Huss-Lederman, David Walker, and Jack Dongarra.
MPI: The Complete Reference : The MPI-2 Extensions by William Gropp, Steven Huss-Lederman, Andrew Lumsdaine, Ewing Lusk, Bill Nitzberg, and Marc Snir. A free HTML version of the first edition of the Complete Reference is available at Netlib.

Example Code
The MPI interface consists of nearly two hundred functions but in general most codes use only a small subset of the functions. Below are a few small example MPI programs to illustrate how MPI can be used. The first code prints a simple message to the standard output:
```
include 
include ""mpi.h""
int main(int argc,char argv[]) {
    MPI_Init(&argc, &argv);
    printf(""hello world\n"");
    MPI_Finalize();
    return 0;
}
The Fortran version:
program MPI_hello
use mpi
implicit none
integer ierr
call MPI_Init(ierr)
WRITE(6,)'Hello World'
call MPI_Finalize(ierr)
end program MPI_hello
When the code is executed with four processes the following output is produced:
hello world
hello world
hello world
hello world
The file `mpi.h` holds all the declarations for the MPI functions and must be included. The function `MPI_Init(&argc,&argv)` must be called before any other MPI functions and the function `MPI_Finalize()` should be called after the last MPI function has been called. In the above example these are the only two MPI functions used. `MPI_Init` is called with the same arguments as the main function to allow the system to perform any special initializations for the MPI library. When executed using four processes, four copies of the code are executed. Each process has an integer associated with it called its rank which can be used to identify it. The processes are numbered sequentially from zero. In the next code we use a MPI function call to find out the rank of each process.
include 
include ""mpi.h""
int main(int argc,char argv[]) {
    int my_rank; MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    printf(""Hello world from process %d \n"",my_rank);
    MPI_Finalize();
    return 0;
}
The Fortran version:
program MPI_Hello
use mpi
implicit none
integer ierr
integer status(MPI_STATUS_SIZE)
integer my_rank
call MPI_Init(ierr)
call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, ierr)
WRITE(6,)'Hello world from process ',my_rank
call MPI_Finalize(ierr)
end program MPI_Hello
We have added two lines to the code. The integer `my_rank` is used to store the rank of each process. The function `MPI_Comm_rank(MPI_COMM_WORLD, &my_rank)` retrieves the rank of the processes and stores it in my_rank. `MPI_COMM_WORLD` is called a communicator and defines a collection of processes that can send messages to each other. The user can define different communicators to hold separate groups of processes but in general only one communicator is needed and it is already defined for the user as `MPI_COMM_WORLD` in `mpi.h`. The output from the above code when run across four processes is:
Hello world from process 0
Hello world from process 2
Hello world from process 1
Hello world from process 3
When the above code is executed each process acts independently of each other. The order in which they complete is random; executing the code a number of times will demonstrate this. If we wish to order the output it is necessary to synchronize the separate processes. The next example shows how to use MPI functions to order the output. Each process will send their rank to the process with rank zero which will print the ranks in turn. This is an example of message passing, each process will send a message containing an integer representing its rank to the ‚Äòroot‚Äô process.
include 
include ""mpi.h""
int main(int argc,char argv[]) {
    int my_rank;
    int num_proc;
    int dest= 0;
    int tag= 0;
    int tmp,i; MPI_Status status;
    MPI_Init(&argc, &argv);
    / get my_rank /
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    / find out how many processes there are /
    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);
    if( my_rank != 0 ) {
        MPI_Send(&my_rank,1,MPI_INT,dest,tag,MPI_COMM_WORLD);
    } else {
        for(i=1; i < num_proc; i++) {
            MPI_Recv(&tmp,1,MPI_INT,i,tag,MPI_COMM_WORLD,&status);
            printf(""hello world from processes %d \n"",tmp);
        }
    }
    MPI_Finalize();
    return 0;
}
The Fortran version:
program MPI_Hello
use mpi
implicit none
integer :: ierr
integer :: status(MPI_STATUS_SIZE)
integer :: my_rank
integer :: p
integer :: source, dest, tag
integer :: size
integer :: tmp
integer :: number=1
call MPI_Init(ierr)
call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, ierr)
call MPI_Comm_size(MPI_COMM_WORLD, p, ierr)
if ( my_rank /= 0 ) then
call MPI_Send(my_rank,1, MPI_INTEGER, + dest,tag,MPI_COMM_WORLD,ierr)
endif
if ( my_rank == 0 ) then
do source=1, p-1
call MPI_Recv(tmp, 1, MPI_INTEGER, source, + tag, MPI_COMM_WORLD, status, ierr)
WRITE(6,)'Hello World from ',tmp
enddo
ENDIF
call MPI_Finalize(ierr)
end program MPI_Hello
MPI_Status is a structure defined in `mpi.h` and used by MPI to hold information such as error status; it is rarely used directly by the programmer. The function `MPI_Comm_size(MPI_COMM_WORLD, &num_proc)` returns the total number of processes that are running in the communicator `MPI_COMM_WORLD` and stores it in num_proc. The `if` block can be interpreted as follows:
if( my_rank != 0 ) {
    I am not 'root' process:: send my_rank to root }
else {
    I am 'root' process:: receive the rank of each process in turn and print it
}
``
TheMPI_Send(&my_rank,1,MPI_INT,dest,tag,MPI_COMM_WORLD)` function can be understood as follows:
| Argument | Interpretation |
|---|---|
| &my_rank | Address of value to be sent. |
| 1 | Number of values to be sent; by setting this number greater than one, arrays can be sent.
| MPI_INT   | ‚ÄúType‚Äù of value to be sent. |
| dest | Process the data is to be sent to, in this case process 0. |
| tag   | An identifier for the message‚Äîa process may be expecting a number of messages fro another process and the tag is simply an integer used to differentiate them. In this case the tag was set to 0. |
MPI_COMM_WORLD    Defines which communicator the processes belong to.
The function takes the address of the variable to be sent, effectively it takes a chunk of memory from that address and sends it. The size of the chunk of memory is defined by the ‚Äòtype‚Äô of variable and how many items it contains if it is an array. The variable MPI_INT is used to indicate that an integer is being sent. MPI supports all the common data types, e.g. MPI_FLOAT for float, MPI_DOUBLE for double and MPI_CHAR for char. It is possible to create your own MPI data type to send C type struct [typo?]. The corresponding receive function MPI_Recv(&tmp,1,MPI_INT,i,tag,MPI_COMM_WORLD,&status) can be understood as follows:
| Argument | Interpretation |
|---|---|
| &tmp | Address where the incoming value will be stored.
| 1 | Number of values to receive.
| MPI_INT   | ‚ÄúType‚Äù of value to be received.
| i | Process that is sending the data.
| tag   | Identifier for the message.
MPI_COMM_WORLD    Communicator for the processes.
&status A structure that is used to hold any error messages from the MPI function.
The output from the code when run with four processes is:
hello world from processes 1
hello world from processes 2
hello world from processes 3
The above simple examples illustrate how MPI can be used to pass data between processes. Perhaps the simplest use of MPI is for Monte Carlo simulations where the same code is executed a large number of times with slightly different parameters, perhaps generated with a random number. MPI can be used to start up a number of processes across different CPUs executing the same code but seeded with a different random number. For example, if the main function were called Monte_Carlo() and the random number generator were seeded with the function random_seed(int), the code would have the outline:
int main(int argc,char* argv[]); {
    int my_rank;
    MPI_Init(&argc, &argv);
    /* Get the rank of each process */
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    /* seed the random number generator with the rank of the process */
    random_seed(my_rank);
    /* execute the code */
    Monte_Carlo();
    MPI_Finalize();
    return 0;
}
The above code would open files Output.00, Output.01, Output.02, etc., depending on the number of processes, and each process would have its own file with pointer fp for output. Putting the two pieces of code together:
FILE *fp;
int main(int argc,char* argv[]); {
    int my_rank;
    char file[100];
    char c1,c2;
    MPI_Init(&argc, &argv);
    /* Get the rank of each process */
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    /* c1 and c2 are two characters used to represent my_rank */
    c1=my_rank/10 +'0';
    c2=my_rank%10 +'0';
    /* copy ""Output."" to the character array file */
    strcpy(file,""OutPut."");
    /* add c1 and c2 to the end of the character array file */
    strncat(file,&c1,1);
    strncat(file,&c2,1);
    /* open a file with the name held in the character array file */
    fp=fopen(file,""w"");
    /* seed the random number generator with the rank of the process */
    random_seed(my_rank);
    /* execute the code */
    Monte_Carlo(); MPI_Finalize();
    return 0;
}
The Fortran version:
```
program MPI_output
use mpi
implicit none
integer :: ierr
character(len=100):: filename
character(len=10) :: digit_string
integer :: p, my_rank
call MPI_Init(ierr)
! Get my_rank
call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, ierr)
!Get the number of Processes
call MPI_Comm_size(MPI_COMM_WORLD, p, ierr)
call seed_random_numbers( my_rank)
! Convert my_rank to a charactoer string
write(digit_string,'(i4.4)') my_rank
! Concat ""file"" and the string for my_rank
filename='file.'//digit_string(1:len_trim(digit_string) !
!Open a file for each process for I/O
open(unit=7,file=filename,status='unknown')
write(7,*)'Hello World from ',my_rank
call MONTE_CARLO() !Do actual work of the code
close(7)
call MPI_Finalize(ierr)
end program MPI_output
```"
rc-website-fork/content/userinfo/howtos/rivanna/launch-rserver.md,"+++
type = ""howto""
date = ""2021-04-05T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""r""
]
categories = [""howto""]
draft = false
title = ""Launching RStudio Server from an Apptainer Container""
description = ""How to launch RStudio Server from an Apptainer container""
author = ""RC Staff""
+++
Rocker provides many software containers for R. Due to the default permission settings of our file system, launching an RStudio Server session is not straightforward. If you are interested in using their containers on the HPC system, please follow these steps.
Pull container
Use Apptainer to pull the container. We will use geospatial in this example.
bash
module load apptainer
apptainer pull docker://rocker/geospatial
You should see geospatial_latest.sif in your current directory.
One-time setup
The commands in this section are to be executed as a one-time setup on the frontend. You may need to repeat the steps here when running a new rocker container.
Create a directory where you have write permissions, e.g. under $HOME:
bash
TMPDIR=~/rstudio-tmp # your choice
mkdir -p $TMPDIR/tmp/rstudio-server
uuidgen > $TMPDIR/tmp/rstudio-server/secure-cookie-key
chmod 600 $TMPDIR/tmp/rstudio-server/secure-cookie-key
mkdir -p $TMPDIR/var/{lib,run}
These directories will be bind-mounted at runtime when you launch the container.
Launch script
You must be consistent with the bind-mount paths that you set up in the previous section. We recommend putting the following commands in a script (e.g. run_rserver.sh) so that you will not need to type every time you launch RStudio Server.
```bash
!/bin/bash
specify path to container
SIF=$HOME/geospatial_latest.sif
specify path to tmp directory created in previous section
TMPDIR=$HOME/rstudio-tmp
module load apptainer
apptainer exec \
    -B $TMPDIR/var/lib:/var/lib/rstudio-server \
    -B $TMPDIR/var/run:/var/run/rstudio-server \
    -B $TMPDIR/tmp:/tmp \
    $SIF \
    rserver --www-address=127.0.0.1 --server-user=$USER
```
Change the script into an executable:
{{< code-snippet >}}
chmod +x run_rserver.sh
{{< /code-snippet >}}
Launch
We recommend launching this in a FastX Web (MATE) session for short runs or debugging on the frontend. For production runs you can request a Desktop interactive app. Both FastX and the Desktop can be accessed at our Open OnDemand portal.
Once in either FastX or a remote Desktop, start a terminal window.
To launch RStudio Server, execute:
{{< code-snippet >}}
./run_rserver.sh
{{< /code-snippet >}}
Nothing will happen in the terminal, which is normal. Open a browser (Firefox is available through the MATE desktop menu) and go to localhost:8787.
Your server should be running there."
rc-website-fork/content/userinfo/howtos/rivanna/image-processing.md,"+++
type = ""howto""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""HPC"",
  ""software"",
  ""howto""
]
date = ""2019-08-26T08:37:46-05:00""
tags = [""vis""
]
draft = false
shorttitle = ""Image Processing & Visualization""
title = ""Image Processing & Scientific Visualization on the HPC system""
description = ""Image Processing & Scientific Visualization Software in the HPC environment""
author = ""RC Staff""
+++
Available Software
To get an up-to-date list of the installed image processing and visualization tools, log on to UVA HPC and run the following command in a terminal window:
module keyword vis
To get more information about a specific module version, run the module spider command, for example:

module spider blender/2.78c


List of Image Processing and Visualization Software Modules
{{< rivanna-software moduleclasses=""vis"" >}}
Running Interactive Visualizations
Many of the provided image processing and visualization applications provide a graphical user interface (GUI). In order to use a GUI on the HPC system, users must log in through a client capable of displaying X11 graphics.  We recommend FastX Web which provides a GPU to accelerate rendering.
To start an applications GUI in an X11-enabled terminal, first load the software module and then run the GUI application executable, e.g.
module load blender
When connected to UVA HPC via FastX Web, rendering of the graphical user interface can be accelerated by executing this command:
module load blender
vglrun -c proxy blender &
The ampersand & returns the terminal to input mode while the application is running."
rc-website-fork/content/userinfo/howtos/rivanna/custom-jupyter-kernels.md,"+++
type = ""howto""
date = ""2020-03-03T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""jupyter"", ""apptainer""
]
categories = [""howto""]
draft = false
title = ""Custom Jupyter Kernels""
description = ""How to create custom Jupyter kernels""
author = ""RC Staff""
+++
You can create custom kernels from a conda environment or an Apptainer container.
In both cases you'll need to install the ipykernel package.
Jupyter kernel based on a conda environment
To create a custom kernel of the conda environment myenv that uses Python 3.7:
module load miniforge
conda create -n myenv python=3.7 ipykernel <other_packages>
source activate myenv
python -m ipykernel install --user --name myenv --display-name ""My Env""
Note:
- You can customize the display name for your kernel. It is shown when you hover over a tile in JupyterLab. If you do not specify a display name, the default Python [conda env:<ENV_NAME>] will be shown.
- A custom kernel cannot be created from the terimnal within an interactive JupyterLab session. This will create the kernel in an incorrect folder and the new tile will not be visible. Perform the above commands either in a FastX terminal, SSH connection, or using HPC Shell Access within Open OnDemand.
- For more information on Miniforge, please visit here.
Jupyter kernel based on Apptainer container
For this to work, the ipykernel Python package must be installed within the Apptainer container. To create a Jupyter kernel for the container, you can either use our automated script jkrollout or do it manually.
Automated script
Replace /path/to/sif with the actual image name or path:
jkrollout /path/to/sif ""My kernel""
If GPU is supported:
jkrollout /path/to/sif ""My kernel"" gpu
Manual
Custom kernels are stored under ~/.local/share/jupyter/kernels. If this directory does not already exist, run
mkdir -p ~/.local/share/jupyter/kernels
Next, cd into it and create a directory for your specific kernel, e.g. mykernel:
mkdir mykernel
Create two files in that directory, kernel.json and init.sh. (The former must be exactly kernel.json. The latter can be customized as long as you are consistent.)
kernel.json:
{
 ""argv"": [
  ""/home/your_id/.local/share/jupyter/kernels/mykernel/init.sh"",
  ""-f"",
  ""{connection_file}""
 ],
 ""display_name"": ""My kernel"",
 ""language"": ""python""
}
(Remember to replace your_id with your user ID.)
init.sh:
```
!/bin/bash
module load apptainer
apptainer exec /path/to/apptainer/image python -m ipykernel $@
```
(Remember to use the actual path of your Apptainer image.)
If the container has GPU support, add a --nv flag in the last line:
apptainer exec --nv /path/to/apptainer/image python -m ipykernel $@
Change init.sh into an executable:
chmod +x init.sh
You will see your custom kernel ""My kernel"" next time you use JupyterLab."
rc-website-fork/content/userinfo/howtos/rivanna/wdl-bioinformatics.md,"+++
type = ""howto""
date = ""2020-03-25T15:12:46-05:00""
tags = [
  ""rivanna"", ""software""
]
categories = [""howto""]
draft = false
title = ""Running a Bioinformatics Software Pipeline with Wdl/Cromwell""
description = ""Running a Bioinformatics Software Pipeline with Wdl""
author = ""RC Staff""
+++
{{% callout %}}
WDL (pronounced widdle) is a workflow description language to define tasks and workflows. WDL aims to describe tasks with abstract commands that have inputs, and once defined, allows you to wire them together to form complex workflows.
Learn More
{{% /callout %}}
{{% callout %}}
CROMWELL is the execution engine (written in Java) that supports running WDL scripts on three types of platforms: local machine (e.g. your laptop), a local cluster/compute farm accessed via a job scheduler (e.g. Slurm, GridEngine) or a cloud platform (e.g. Google Cloud or Amazon AWS).
Learn More
{{% /callout %}}
Introduction
Pre-requisites: This tutorial assumes that you have an understanding of the basic structure of a WDL script. Learn here
This tutorial will walk you through steps for creating a WDL script, and executing it on the HPC system. For the purpose of this document, we will write a (very) basic real-world workflow that does something useful!
Our workflow:
The processing with bwa-mem contains two tasks:

Alignment of sequence files to reference genome using bwa, followed by
SAM to BAM format conversion using picard.

The tasks are joined together using linear chaining, with output from bwa step used as input to the picard step.

Inputs:

Sample paired-end FASTQ files
hg38 reference fasta and BWA index files

UVA HPC modules:

wdltool
cromwell
bwa
picard


Set up your Working Environment

Login to UVA HPC and create a root working directory in your /scratch folder:

cd /scratch/$USER/
mkdir wdl_tutorial
cd wdl_tutorial


Get the Sample FASTQ files. Copy the sample paired-end fastq files to this folder. For this tutorial, we will use reads for NA12878, downloaded from here. You can download the dataset, or use DNA-seq data for any sample of your choice.


Get the reference genome files. We will use the hg38 reference fasta and BWA indexes from the genomes repo on the HPC system at /project/genomes/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/. All UVA HPC users have read access to these reference genomes, no need to download/copy them to the working directory!



WDL script
First, let's write our workflow! Open a blank text file in your favorite text editor and save it as bwaAln.wdl.
Workflow
Let‚Äôs begin with the workflow skeleton. Our workflow -- let's name it bwa_mem -- calls two tasks: align and samSort.
workflow bwa_mem {
    call align { input: }
    call samSort { input: } 
}
Next, we need to tell cromwell how to link the tasks together. We will tell samSort task to take the outsam from align task as its input. So let's update the workflow definition:
workflow bwa_mem {
    call align { input: }
    call samSort { 
        input: 
            insam = align.outsam
    } 
}
Finally, let‚Äôs add the inputs for our first task, as well as few variables each task is going to need.
```
workflow bwa_mem {
    String sample_name
    File r1fastq
    File r2fastq
    File ref_fasta
    File ref_fasta_amb
    File ref_fasta_sa
    File ref_fasta_bwt
    File ref_fasta_ann
    File ref_fasta_pac
call align {
    input:
        sample_name = sample_name,
        r1fastq = r1fastq,
        r2fastq = r2fastq,
        ref_fasta = ref_fasta,
        ref_fasta_amb = ref_fasta_amb,
        ref_fasta_sa = ref_fasta_sa,
        ref_fasta_bwt = ref_fasta_bwt,
        ref_fasta_ann = ref_fasta_ann,
        ref_fasta_pac = ref_fasta_pac
    }
call sortSam {
    input:
        sample_name = sample_name,
        insam = align.outsam
    }

}
```
Our workflow definition in bwaAln.wdl is complete!
Tasks
1. align
This task will align the paired-end reads to hg38 build of human reference genome, using bwa mem algorithm. Here's the skeleton definition:
task align {
    Inputs/Variables
    command {...}
    runtime {...}
    output {...}
}
Now let's define the alignment command and the variables we need to execute it:
```
task align {
    String sample_name
    File r1fastq
    File r2fastq
    File ref_fasta
    File ref_fasta_amb
    File ref_fasta_sa
    File ref_fasta_bwt
    File ref_fasta_ann
    File ref_fasta_pac
    Int threads
command {
    bwa mem -M -t ${threads} ${ref_fasta} ${r1fastq} ${r2fastq} > ${sample_name}.hg38-bwamem.sam
}

runtime {...}
output {...}

```
We are passing the paired-end fastq files for our sample, the reference fasta and its BWA indexes, as well as the number of threads for alignment as inputs to the task. Notice how to reference the variables in the command, using ${variable_name}.
Next, add runtime attributes, i.e. the number of cpus (same as num of threads passed as variable) and memory (16GB) required for the task.
...
    runtime {
        cpus: threads
        requested_memory_mb: 16000
    }
...
Finally, lets define out output:
...
    output {
        File outsam = ""${sample_name}.hg38-bwamem.sam""
    }
...
Our task is complete and should look like this:
```
task align {
    String sample_name
    File r1fastq
    File r2fastq
    File ref_fasta
    File ref_fasta_amb
    File ref_fasta_sa
    File ref_fasta_bwt
    File ref_fasta_ann
    File ref_fasta_pac
    Int threads
command {
    bwa mem -M -t ${threads} ${ref_fasta} ${r1fastq} ${r2fastq} > ${sample_name}.hg38-bwamem.sam
}
runtime {
    cpus: threads
    requested_memory_mb: 16000
}
output {
    File = ""${sample_name}.hg38-bwamem.sam""
}

}
```

2. sortSam
This task will take the output from alignment step in SAM format, convert it to BAM, sort it on coordinates and create the index using picard SortSam utility. This the task skeleton:
task sortSam {
    Inputs/Variables
    command {...}
    runtime {...}
    output {...}
}
Add variables ‚Ä¶
task sortSam {
    String sample_name
    File insam
    ...
}
Add command ‚Ä¶
... 
    command <<<
        java -jar $EBROOTPICARD/picard.jar \
            SortSam \
            I=${insam} \
            O=${sample_name}.hg38-bwamem.sorted.bam \
            SORT_ORDER=coordinate \
            CREATE_INDEX=true
    >>>
...
}
{{% callout %}}
Note: picard is available as a module on the HPC system. When you load the module to your environment (using module load picard), it also defines the $EBROOTPICARD environment variable, which defines the full path to the jar file for calling picard utilities.
{{% /callout %}}
Add output ‚Ä¶
...
    output {
         File outbam = ""${sample_name}.hg38-bwamem.sorted.bam""
         File outbamidx = ""${sample_name}.hg38-bwamem.sorted.bai""
    }
....
For this task, we don‚Äôt need custom runtime attributes, and will be using the default described in the backend configuration file for UVA HPC!
The complete sortSam task definition should look like this:
task sortSam {
    String sample_name
    File insam
    command <<<
        java -jar $EBROOTPICARD/picard.jar \
            SortSam \
            I=${insam} \
            O=${sample_name}.hg38-bwamem.sorted.bam \
            SORT_ORDER=coordinate \
            CREATE_INDEX=true
    >>>
    output {
         File outbam = ""${sample_name}.hg38-bwamem.sorted.bam""
         File outbamidx = ""${sample_name}.hg38-bwamem.sorted.bai""
    }
}

Complete WDL script - bwaAln.wdl
```

My First WDL/CROM workflow on Rivanna

Description:
This WDL workflow will align paired-end sequences of a sample to
hg38 build of human genome using bwa mem algorithm, followed by
sorting and indexing the alignment map using picard

This workflow is designed for demonstration purpose only!

Workflow

workflow bwa_mem {
    String sample_name
    File r1fastq
    File r2fastq
    File ref_fasta
    File ref_fasta_amb
    File ref_fasta_sa
    File ref_fasta_bwt
    File ref_fasta_ann
    File ref_fasta_pac
call align {
    input:
        sample_name = sample_name,
        r1fastq = r1fastq,
        r2fastq = r2fastq,
        ref_fasta = ref_fasta,
        ref_fasta_amb = ref_fasta_amb,
        ref_fasta_sa = ref_fasta_sa,
        ref_fasta_bwt = ref_fasta_bwt,
        ref_fasta_ann = ref_fasta_ann,
        ref_fasta_pac = ref_fasta_pac
    }
call sortSam {
    input:
        sample_name = sample_name,
        insam = align.outsam
}

}

Tasks #

1. This task will align the reads to reference
using bwa mem algorithm
task align {
    String sample_name
    File r1fastq
    File r2fastq
    File ref_fasta
    File ref_fasta_amb
    File ref_fasta_sa
    File ref_fasta_bwt
    File ref_fasta_ann
    File ref_fasta_pac
    Int threads
    command {
        bwa mem -M -t ${threads} ${ref_fasta} ${r1fastq} ${r2fastq} > ${sample_name}.hg38-bwamem.sam
    }
    runtime {
        cpu: threads
        requested_memory_mb: 16000
    }
    output {
        File outsam = ""${sample_name}.hg38-bwamem.sam""
    }
}
2. This task will sort sam by coordinate,
convert it to BAM, and index the BAM
task sortSam{
    String sample_name
    File insam
    command <<<
        java -jar $EBROOTPICARD/picard.jar \
            SortSam \
            I=${insam} \
            O=${sample_name}.hg38-bwamem.sorted.bam \
            SORT_ORDER=coordinate \
            CREATE_INDEX=true
    >>>
    output {
        File outbam = ""${sample_name}.hg38-bwamem.sorted.bam""
        File outbamidx = ""${sample_name}.hg38-bwamem.sorted.bai""
    }
}
```
Validate
Next, we will validate our script, make sure there are no syntax errors.
We will use wdltool utility toolkit that includes a syntax validation function. It is available as a module on the HPC system
module load wdltool
This will define a global environment variable, $WDLTOOLPATH, that stores the root directory path for the jar file. To validate our script, we simply call the validate function:
java -jar $WDLTOOLPATH/wdltool-0.14.jar validate bwaAln.wdl
This function parses the WDL script and alerts us to any syntax errors such missing curly braces, undefined variables, missing commas and so on. It will resolve imports, but note that it is not able to identify errors like typos in commands, specifying the wrong filename, or failing to provide required inputs to the programs that are run in the workflow.
No messages will be thrown if the syntax is valid!

Specify Inputs
Now, we will create a JSON file of inputs for our workflow. We will use the inputs function in the wdltool package to create a template to populate the values -
java -jar $WDLTOOLPATH/wdltool-0.14.jar inputs bwaAln.wdl > bwaAln.inputs.json
This will create a file called bwaAln.inputs.json that lists all the inputs to all the tasks in your script following the pattern below:
{
    ""<workflow name>.<task name>.<variable name>"": ""<variable type>""
}
This saves you from having to compile a list of all the tasks and their variables manually!
Now, populate the variables using your favorite text editor. You can verify the file's content with the cat command:
cat bwaAln.inputs.json
Output in terminal:
```
{
  ""bwa_mem.sample_name"": ""NA12878"",
  ""bwa_mem.r1fastq"": ""/FULL/PATH/TO/NA12878_R1.fastq"",
  ""bwa_mem.r2fastq"": ""/FULL/PATH/TO/NA12878_R2.fastq"",
""bwa_mem.ref_fasta"": ""/project/genomes/Homo_sapiens/GATK_bundle/hg38/BWAIndex/hg38.fa"",
  ""bwa_mem.ref_fasta_amb"": ""/project/genomes/Homo_sapiens/GATK_bundle/hg38/BWAIndex/hg38.fa.amb"",
  ""bwa_mem.ref_fasta_sa"": ""/project/genomes/Homo_sapiens/GATK_bundle/hg38/BWAIndex/hg38.fa.sa"",
  ""bwa_mem.ref_fasta_ann"": ""/project/genomes/Homo_sapiens/GATK_bundle/hg38/BWAIndex/hg38.fa.ann"",
  ""bwa_mem.ref_fasta_pac"": ""/project/genomes/Homo_sapiens/GATK_bundle/hg38/BWAIndex/hg38.fa.pac"",
  ""bwa_mem.ref_fasta_bwt"": ""/project/genomes/Homo_sapiens/GATK_bundle/hg38/BWAIndex/hg38.fa.bwt"",
""bwa_mem.align.threads"": 16
}
```
{{% callout %}}
Please provide FULL PATH to fastq files! There is no need to change file paths to hg38 reference fasta and BWA index files!
{{% /callout %}}

Execute
At the moment, Cromwell is the only fully-featured execution engine that supports WDL. It is available as a module on the HPC system.
module load cromwell
Basic syntax:
java -jar $CROMWELLPATH/cromwell-30.1.jar <action> <parameters>
Backend
In order to run each task of our workflow as a slurm job, we need to configure a SLRUM backend.
Create an empty text file, cromwell-rivanna.conf, and copy the contents described in this post.
Slurm batch submission script
Finally, we will write a simple Slurm submission script to execute our workflow. Create an empty file submit-bwaAln.sh and copy the below contents to it -
```
!/bin/bash
SBATCH --job-name=bwaAln       # Job name
SBATCH --cpus-per-task=16      # Number of CPU cores per task
SBATCH --mem=16gb              # Job Memory
SBATCH --time=5:00:00          # Time limit hrs:min:sec
SBATCH --output=bwaAln_%A.out  # Standard output log
SBATCH --error=bwaAln_%A.err   # Standard error log
SBATCH -A <YOUR_ALLOCATION     # allocation groups
SBATCH -p standard             # slurm queue
pwd; hostname; date
load modules
module load cromwell
module load bwa
module load picard
Submit cromwell job
java -Xmx8g -Dconfig.file=~/rivanna-cromwell.conf \
    -jar $CROMWELLPATH/cromwell-30.1.jar \
    run bwaAln.wdl \
    --inputs bwaAln.inputs.json 
```
Note: This assumes that the rivanna-crowell.conf backend configuration file is located in your home directory. Learn more about how to create this Cromwell configuration file here.
Submit the job
submit submit-bwaAln.sh
Monitor job progress
squeue -u $USER

Outputs
The outputs of the workflow will be written to /call-/execution/ folder! Please explore the directory structure for relevant files!"
rc-website-fork/content/userinfo/howtos/rivanna/convert-jupyter-pdf.md,"+++
type = ""howto""
date = ""2021-05-26T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""jupyter""
]
categories = [""howto""]
draft = false
title = ""Converting a Jupyter Notebook to a PDF""
description = ""Converting a Jupyter Notebook to a PDF""
author = ""RC Staff""
+++
Users cannot load modules inside the OpenOnDemand App for JupyterLab. Therefore it is not possible to convert a Jupyter Notebook to a PDF directly inside the JupyterLab Interactive App on OpenOnDemand.
There are 2 ways to convert a Jupyter Notebook to a PDF:


Both methods require Jupyter to be installed within a conda environment. 
  The following example will install Jupyter into a conda environment named 'jupyter':
  
  module load miniforge
  conda create -n jupyter
  source activate jupyter
  mamba install jupyter -y
  



Directly from the command line. ssh from your terminal and type the following: 
```
module load miniforge texlive

Load conda environment with Jupyter installed
source activate jupyter
jupyter nbconvert --to pdf you_script.ipynb 
```

If you want to use  GUI, please request a Desktop session. 
Fill out the form as you normally would for JupyterLab. After you get to a desktop, open a terminal (black box next to Firefox in the top bar) and type these commands:

```
module load miniforge texlive
Load conda environment with Jupyter installed
source activate jupyter
jupyter notebook
```
This will pull up JupyterLab. Now you will be able to use the Save and Export Notebook As function in JupyterLab.
"
rc-website-fork/content/userinfo/howtos/rivanna/jupyter-to-python-script.md,"+++
type = ""howto""
date = ""2021-07-24T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""jupyter""
]
categories = [""howto""]
draft = false
title = ""Converting a Jupyter Notebook to a Python Script""
description = ""Converting a Jupyter Notebook to a Python Script""
author = ""Paul Orndorff""
+++
Sometimes it may be useful to convert a Jupyter notebook into a Python executable script. Once your notebook is opened in OOD you can select File > Export Notebook As ... > Export Notebook to Executable Script:

This will download a Python executable with a '.py' extension into your local computer's Downloads folder. Your notebook may also show ""Download as"" instead of ""Export Notebook As ..."". Either of these selections will allow you to download a Python executable.
This script can be copied to the HPC system in the working directory where JupyterLab was accessing the notebook. Information on transferring files to and from Rivanna can be found here.
Notebooks can also be converted directly on the command line. This can be done by loading jupyterlab and running jupyter nbconvert --to script /path/to/ipynb, where /path/to/ipynb is the location of the notebook file:
module load jupyterlab
jupyter nbconvert --to script /path/to/ipynb
A Slurm submission script is required to execute the Python executable. JupyterLab on OOD provides a web form that requests resources for your job to run on a compute node. Let's look at the web form from OOD:

We see that the form requests a partition, the time in hours, the number of cores, the requested memory, the working directory, and your UVA HPC allocation. Once you click ""Launch"", your job is submitted and requests these specifications on a compute node. The same can be done by writing a Slurm submission script. The script below mirrors the specifications set in the web form. We'll call this script submit_jup.slurm.
```
!/bin/bash
SBATCH -p standard
SBATCH -t 02:00:00
SBATCH -c 1
SBATCH --mem=9GB
SBATCH -A hpc_build
module purge
module load miniforge
python JupyterNotebook_script.py
```
Now that everything is ready, all that needs to be done is to submit submit_jup.slurm. This is accomplished by typing sbatch submit_jup.slurm on the command line in the same directory of the Slurm script and Python script. Your job will launch on a compute node and write its output to a Slurm output file. Since this script only prints standard output the Slurm output file will contain the same information as the notebook.
Hello World with 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27)
[GCC 9.3.0]
More information on using Slurm on the HPC system can be found here.
If your code requires a specific conda environment, you can specify source activate <environment name> below the module commands in the Slurm script to activate the environment:
```
!/bin/bash
SBATCH -p standard
SBATCH -t 02:00:00
SBATCH -c 1
SBATCH --mem=9GB
SBATCH -A hpc_build
module purge
module load miniforge
source activate 
python JupyterNotebook_script.py
```"
rc-website-fork/content/userinfo/howtos/rivanna/clear-ood-session-files.md,"+++
type = ""howto""
date = ""2024-02-15T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""jupyter"", ""rstudio""
]
categories = [""howto""]
draft = false
title = ""Clear OOD Files""
description = ""How to clear your Open on Demand session files.""
author = ""RC Staff""
+++
To clear OOD Session files, the HPC system will need to be accessed via a terminal. See documentation for information on how to access via SSH. 
You can find the session files and logs for all Open on Demand apps at:
~/ondemand/data/sys/dashboard/batch_connect/sys
Under this directory you will see subdirectories for the Open on Demand applications that you have used before. Under each subdirectory you can find the files that are created when you launch a new session. 
To quickly clear all session files for OnDemand from your /home directory run:
rm -rf ondemand
Other directories related to Open on Demand such as .jupyter and .rstudio-desktop can be removed in the same fashion:
rm -rf .jupyter #removes JupyterLab session files
rm -rf .rstudio-desktop #removes RStudio session files"
rc-website-fork/content/userinfo/howtos/rivanna/migrate-python.md,"+++
type = ""howto""
date = ""2021-04-01T00:00:00-05:00""
tags = [
  ""rivanna"", ""software"", ""python""
]
categories = [""howto""]
draft = false
title = ""Migrating Python packages""
description = ""How to migrate Python packages from one Python version to another""
author = ""RC Staff""
+++
Scenario
You have installed Python packages locally in one version and now wish to use them in a different version.  For example, you have been using Python 3.6 but it is obsolete and will be removed soon, so you need to set up those packages for Python 3.8.  There are several ways to accomplish this, depending on the package manager. In this how-to we will discuss pip and conda.
You will need to load the module for the newer Python version. For this example,
{{< code-snippet >}}
module load miniforge
{{< /code-snippet >}}
Pip
The Python packages are installed in a hidden location under your home directory:
~/.local/lib/pythonx.y/site-packages
where x and y are the major and minor Python versions, respectively.
Preserve individual package versions
To preserve the versions for all individual packages, first freeze the environment into a file, say requirements.txt:
{{< code-snippet >}}
pip freeze --path ~/.local/lib/python3.6/site-packages > requirements.txt
{{< /code-snippet >}}
Next, install the packages:
{{< code-snippet >}}
pip install --user -r requirements.txt
{{< /code-snippet >}}
Use the latest versions or whichever are mutually compatible
If you have no preference on the package versions, you can remove the version requirements:
{{< code-snippet >}}
pip freeze --path ~/.local/lib/python3.6/site-packages | sed 's/==.*$//g' > requirements.txt
{{< /code-snippet >}}
Install the packages:
{{< code-snippet >}}
pip install --user -r requirements.txt
{{< /code-snippet >}}
Conda
You can create/load a conda environment that uses a different Python version with the Miniforge module. Suppose the environment name is myenv. You can either update the existing environment or create a new one.
Update Python in the old environment
bash
source activate myenv
conda install python=3.8
Note that if you have many packages in the environment, such an update could take very long due to conda's slow dependency resolution. Individual package versions are not preserved.
Create the new environment
It is better to create a new environment and let the dependency solver do its work from scratch:
bash
conda create -n mynewenv python=3.8 <list of packages>
Use the syntax <package>=<version> if you have version requirements.
Run conda list -n myenv to get a list of all packages in myenv. You can use the following command to show the same list in one line without version information:
{{< code-snippet >}}
conda list -n myenv | awk '{if($1 !~ /^#/) print $1}' | tr '\n' ' '
{{< /code-snippet >}}
Please also visit this page for more information."
rc-website-fork/content/userinfo/howtos/rivanna/compiler-howto.md,"+++
type = ""howto""
date = ""2020-04-01T15:12:46-05:00""
tags = [
  ""rivanna"", ""software"", ""compiler"", ""fortran""
]
categories = [""howto"",""software"",""rivanna""]
draft = false
title = ""Building Your Code on the HPC System""
description = ""Building on the HPC System""
author = ""RC Staff""
+++
Building your Application
Creating an executable from source with a compiled language requires two steps, compiling and linking.  The combination of these is generally called building.  The output of the compiler is generally an object file, which on Unix will end in a .o suffix.  Object files are machine code and are not human-readable, but they are not standalone and cannot be executed.  The linker, which is usually invoked through the compiler, takes all object files, along with any external libraries, and creates the executable (also called a binary).
Compilers are invoked on source files with a line such as
<compiler> <options> code.<suffix>
You must know the name of the compiler you wish to use as well as its options. Most compilers offer a large number of options that can control very detailed properties of the resulting executable, but the average user need only know a few of them.
Please see our compiler documentation for information about the available compilers on the HPC system.  For building and running parallel code, see the documentation.
Debugging and Profiling
To use a debugger you must compile with a special flag.  On all Unix compilers this is -g.  
Fortran programmers can add a flag to check that array accesses fall within the declared bounds.  For gfortran this flag is -fbounds-check, for ifort it is -CB, and for pgfortran it is -C or -Mprof.  Since array-bounds errors are the most common cause of segmentation violations in Fortran, this can be a very valuable flag, but it slows down the execution.
Examples:
gcc -g mycode.c
gfortran -g -fbounds-check mycode.f90
Options for profiling vary more by compiler.  For Gnu compilers it is a combination flag -pg.  Intel uses separate options -p -g.  The PGI compiler uses -Mprof.
Using debuggers and profilers is covered separately here.  If you write your own code, profiling is useful to increase the performance of your code.
Optimizing
Once your code is working, you should remove all debugging flags and compile from source.  Debugging flags inhibit optimizations and can cause your code to waste SUs.  The general optimization flag for all compilers is -O.  With no integer it will set optimization at the default level, which varies by compiler.  You can specify different levels of optimization (including none) with an integer immediately after O.  The number of available levels and where the default lies varies by compiler.  Gnu and Intel have three levels and the default is -O1.  PGI has four levels and the default is -O2.  The flag -O0 disables all optimizations, which can be useful for debugging; the -g flag may or may not imply -O0.
Examples:
gcc -O mycode.c
icc -O2 mycode.c
gfortran -O0 mycode.f90
As noted above, it is particularly important for Fortran programmers to remove the bounds-checking flag for production runs, as that can slow down execution considerably.
Compilers have many more options to fine-tune optimization levels.  However, users should be clear on what they need and why, and should avoid flags like -fast that may bind the executable too tightly to a specific architecture, since HPC nodes are of different ages and architectures.
Renaming the Executable
Unless otherwise specified, the name of your executable will be a.out.  To change that, add the flag -o <name>.  It is critical to include the name between the -o and the source file, or the compiler will overwrite your source file.
Examples:
g++ -o mycode mycode.cxx
ifort -o mycode mycode.f90
pgcc -o mycode mycode.c
Separating Compiling and Linking
Unless otherwise specified, the compiler will attempt to compile all the source files name to the end of the line, then to link them into an executable.  If there are no dependencies in the source files this can work.  However, frequently there are dependencies or other special circumstances, so that the compilation and link steps must be performed separately.  To suppress the link step use the -c option.  To execute the link step, the compiler must be provided with a list of all the resulting .o files that are needed to create the executable.
Example:
ifort -c -O mycode.f90
ifort -c -O mymod.f90
ifort -o mycode mycode.o mymod.o
Linking Libraries
Executables for more advanced programs frequently need to link external libraries, such as numerical packages or data-management libraries.  If the libraries are in the compiler's default search path, they are specified by adding flags after the last object file.  For known paths the flag is -l followed by the ""root"" name of the library, which drops the ""lib"" in front and everything including and after a period.  So we would specify libblas.so as -lblas.  Order matters because the linker resolves references from left to right.  Therefore if we also use lapack, we must have a line like
gfortran -o mycode mycode.o mymod.o -llapack -lblas
If these libraries are not located in the compiler's search path, the paths for both header or module files and library files must be provided to it.  Header paths must go on compile lines, whereas library paths go on the link line.  In the example above, if we wanted to link a library in the user home directory, we'd require the -I flag for including and the -L flag for the library:
gfortran -c -O -I/home/msk3k/myblas mycode.f90
gfortran -c -O mymod.f90
gfortran -o mycode -L/home/mst3k/myblas mycode.o mymod.o -lopenblas

Managing the Build
Typing a compiler line for each source file is tedious and error-prone.  There are several systems to help manage building.  The most widely used on Unix for C, C++, and Fortran is make.  Make uses rules to determine how to generate a particular file, called a target, from its dependencies.  It has a rather peculiar syntax so a more complete discussion can be found here.
Make does not set up paths to external libraries, or specify a compiler other than the default, or enable or disable different build options.  Other systems have been written to handle these initialization steps.  The autoconf/configure system is widely used on Unix.  Another popular system is CMake, which is cross-platform and works on Windows and Mac OSX as well as on Unix.
Autoconf
A code that uses autoconf will provide a script named configure which should be run before building.  Since the current directory is not in the default user path, usually this is invoked with
./configure <options>
Options to the configure script will vary depending on the software package.  To see a full list, type
./configure --help
The default installation prefix is usually /usr or /usr/local.  UVA HPC users are not allowed to write to either of these directories, so an alternative must be provided.   Normally you should use a directory in your home directory.  The minimum configure command would thus be
./configure --prefix=/home/yourid/your/directory/path
Other options to configure generally involve adding or removing options or specifying a path to libraries that it may not be able to find on its own.  Library paths are particularly important in a modules-based system such as ours, because those directories won't always be visible to configure.  The module should set an environment variable for the path that you can use.  For example, if you wish to link the HDF5 library corresponding to your toolchain, you can run
printenv | grep HDF
In this case we might specify a configure line such as
./configure --prefix=/home/mst3k/softpack --with-hdf=${HDF5_ROOT}
Configure generates a Makefile.  Once configure has completed, run
make
If that succeeds run
make install
CMake
CMake is an increasingly popular build system.  Whereas autoconf is generally tied to Unix, CMake is cross-platform.  It has several important differences from autoconf.  Most importantly, it pays no attention to user paths for binaries or libraries.  Each package has specific environment variables that must be set if it is to find binaries or libraries not in its internal default search path.  These environment variables can be set in the shell where cmake is run, or it can be specified at the cmake command line.
CMake usually expects to build packages in a separate directory that typically must be created by the user.  The user must change to this directory before invoking the cmake command.  This command looks for a file CMakeLists.txt, so it takes a command-line argument of the directory where that file is located.
The equivalent to the --prefix of autoconf is CMAKE_INSTALL_PREFIX.   The equivalent to the configure example above for our hypothetical package would be (assuming a separate build directory)
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=/home/mst3k/softpack ..
This assumes that HDF5_ROOT is already in the environment.  If it is not, another option -DHDF5_ROOT=/path/to/hdf5 must be included.
CMake often requires that you specify the compiler.  It does not follow paths, so even if you load a gcc module, it will use a hardcoded path that will generally pick up the system compiler, which is often quite old.  You can modify this with the flags
-DCMAKE_C_Compiler
-DCMAKE_Fortran_Compiler
-DCMAKE_CXX_Compiler
For example
cmake -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/home/mst3k/softpack ..
Other options can be set in different ways.  In order to see a list of the user-changeable options, you can run ccmake, which will bring up a simple text-based graphical user interface, where you can turn options on or off, set paths, and so forth.  More information is available at Kitware's CMake website.
Regardless of how CMake is run, it will generate a CMakeCache.txt file that will not be overwritten if cmake is run again; it must be removed in order to redo the configuration.
As for autoconf, on Unix the output of cmake is a Makefile, so
make
make install
is generally the recipe to build and install the program.
The default cmake on the HPC system is fairly old and most users will need to load a newer cmake module.  If any newer version will work, module load will suffice.  Otherwise module spider cmake will show the options.
For more detailed instructions on building and running compiled codes on the HPC system, please see our online tutorial."
rc-website-fork/content/userinfo/howtos/rivanna/add-packages-to-container.md,"+++
type = ""howto""
date = ""2022-08-21T15:12:46-05:00"" 
tags = [ ""rivanna"", ""software"", ""containers"" ] 
categories = [""howto""]
draft = false 
title = ""How to add packages to a container?""
description = ""How to add packages to a container?""
author = ""RC Staff""
+++
Basic Steps
Strictly speaking, you cannot add packages to an existing container since it is not editable. However, you can try to install missing packages locally. Using python-pip as an example:
module load apptainer
apptainer exec <container.sif> python -m pip install --user <package>
Replace <container.sif> with the actual filename of the container and <package> with the package name. The Python package will be installed in your home directory under .local/lib/pythonX.Y where X.Y is the Python version in the container.
If the installation results in a binary, it will often be placed in .local/bin. Remember to add this to your PATH:
export PATH=~/.local/bin:$PATH
You should be able to use the new package/binary in the container, as your entire home directory is mounted at runtime.
Handling Errors
Installation may fail in more complicated scenarios, where additional libraries are needed. As a first step, load related modules (e.g. if you see ""GLIBC not found"" then load gcc) and try again.
You are always welcome to reach out to us for support. To expedite the process, please let us know what you have tried and include any error messages that you encountered.
Advanced Users
Our Dockerfiles are hosted at https://github.com/uvarc/rivanna-docker. Please feel free to customize them and build your own version. For more information about our use of Docker Hub, please visit here."
rc-website-fork/content/userinfo/howtos/rivanna/gpu-training.md,"+++
type = ""howto""
date = ""2022-06-27T15:15:46-05:00""
tags = [
  ""rivanna"", ""software"", ""gpu"", ""machine learning""
]
categories = [""howto""]
draft = true
title = ""Running python ML models on gpu""
description = ""Running ML models on gpu""
author = ""RC Staff""
+++
Training Deep learning models on GPU(s)
Running large scale machine learning models typically requires the user leverage GPU's to speed up 
computation. In this how-to we overview how to use gpu's , and in particular multiple gpu's, to run large
tensorflow and pytorch machine learning models more efficiently.
Tensorflow
Tensorflow is capable of automatically detecting if there are GPU devices available, and will run on them by default.
To check what devices, including GPU's, are available to tensorflow in a given computing environment,
you can run  
device_list = tf.config.list_physical_devices(device_type=None)
Then specify which device you want to run computations on, merely set the 
environment variable CUDA_VISIBLE_DEVICES to the index of the device 
you want to run on in the devices list. ex: 
```
device_list = [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),
 PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
CUDA_VISIBLE_DEVICES = ""0"" # to run on CPU:0
CUDA_VISIBLE_DEVICES = ""1"" # to run on CPU:1
CUDA_VISIBLE_DEVICES = ""0,1"" # will run on GPU by default since both available
```
Tensorflow multi-gpu with Tf.distribute
Tensorflow GPU paralellization is principally handled through the Tf.distribute API.
We overview how to modify a tensorflow script to run on multiple GPU's.The below code defines a simple NN with 1 hidden layer for classification.
```
from keras.models import Sequential 
from keras.layers import Dense, Dropout 
from keras.utils import to_categorical 
from keras.optimizers import SGD
def define_model():
    model = Sequential() 
model.add(Dense(30, activation='relu', input_dim=30)) 
model.add(Dropout(0.5)) 
model.add(Dense(60, activation='relu')) 
model.add(Dropout(0.5)) 
model.add(Dense(2, activation='softmax')) 
return model

```
In order to distribute this model onto multiple GPU's it suffices to wrap our model definition in the 
mirrored_strategy.scope() class provided by Tf.distribute, as below:
```
mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    def define_parallel_model():
    model = Sequential()

    model.add(Dense(30, activation='relu', input_dim=30)) 
    model.add(Dropout(0.5)) 
    model.add(Dense(60, activation='relu')) 
    model.add(Dropout(0.5)) 
    model.add(Dense(2, activation='softmax')) 
    return model

```
To run on selected GPU's pass a list containing those device names into the Mirrored_Strategy invocation, 
Ex: mirrored_strategy = tf.distribute.MirroredStrategy([[""GPU:0"", ""GPU:1""]])
The 'strategy' in MirroredStrategy refers to a particular way of distributing computations between devices.
mirror_strategy will be the most common used in mutli_gpu training, it creates a version of each model variable
on each device, and assigns some subset of the data to each. All mirror versions of each variable are used collectively 
to update the aggregate 'full model' version of the variable.
There are other such strategies available through the API, including TPUStrategy for training on TPU's
MultiWorkerMirroredStrategy, which is like MirroredStrategy except every worker can be assigned to more than 1 GPU,
and others. tf.distribute is typically used with the model.fit training functionality from
keras, but also provides supports custom training loops thought this 
requires more extensive code modification. For instruction on using tf.distribute with custom 
 training loops, as well as other additional docs, see : tf.distribute docs
Pytorch
Unlike tensorflow, pytorch does not automatically detect GPU devices and set models run on 
them preferentially. In order to get pytorch models onto GPU(s) one must manually assign the model
and all related tensors, to a GPU device. Example code is given below:
```
def train_model(epochs = 5):
    net = My_NN()
    optimizer = optim.Adam(net.parameters(), lr = 0.001)
    for epoch in range(epochs):
        for data in trainset:
            #data is batch of image tensor and label tensor
            X, y = data
        net.zero_grad()
        output = net(X.view(-1, 28*28))
        loss = F.nll_loss(output, y)
        loss.backward()
        optimizer.step()

```
We detect if a GPU device are available, and if so assign it to the 'device' variable
```
if torch.cuda.is_available():
    device = torch.device(""cuda:0"")
    print('Running on GPU')
else: 
    device = torch.device(""cpu"")
    print('Running on CPU')
```
Now we assign the model and all relevant tensors to the device with the .to method:
```
def train_model(epochs = 5):
    net = My_NN().to(device)                    # model assigned to cuda device
    optimizer = optim.Adam(net.parameters(), lr = 0.001)
    for epoch in range(epochs):
        for data in trainset:
            #data is batch of image tensor and label tensor
            X, y = data
            X, y = X.to(device), y.to(device)   # model assigned to  cuda device
        net.zero_grad()
        output = net(X.view(-1, 28*28))
        loss = F.nll_loss(output, y)
        loss.backward()
        optimizer.step()

```
Pytorch multi-gpu with Pytorch_Lightning
The easiest way to parallelize training of pytorch models to multiple GPU's and even multiple nodes is to 
use the Pytorch lighting API. This is a wrapper on top of pytorch which automates a lot of the boiler and 
facilitates parallelization. We explain the structure of Pytorch lighting code below.
```
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import torch.utils.data as data
import torch.nn.functional as F
import matplotlib.pyplot as plt
import pytorch_lightning as pl
from pytorch_lightning import Trainer
class My_NN(pl.LightningModule):
def __init__(self, input_size, hidden_size, num_classes): 
    super(LitNeuralNet, self).__init__()
    self.input_size = input_size
    self.l1 = nn.Linear(input_size, hidden_size)
    self.relu = nn.ReLU()
    self.l2 = nn.Linear(hidden_size, num_classes)

def forward(self, x):
    out = self.l1(x)
    out = self.relu(out)
    out = self.l2(out)
    # no activation and no softmax at the end
    return out

```
The basis for the code is to create our model as a derived class of the pl.LightningModule class we include the standard init and forward methods, as well as other methods delineated by pytorch lightning.
```
    def training_step(self, batch, batch_idx):
        images, labels = batch
        images = images.reshape(-1, 28 * 28)
    # Forward pass
    outputs = self(images)
    loss = F.cross_entropy(outputs, labels)

    tensorboard_logs = {'train_loss': loss}
    # use key 'log'
    return {""loss"": loss, 'log': tensorboard_logs}

# define what happens for testing here

```
The above training step would correspond to this training loop:
```
def train_model(epochs = 5):
optimizer = optim.Adam(net.parameters(), lr = 0.001)
 #learning rate, improves convergence rate and regularization
for epoch in range(epochs):
    t_0 = datetime.now()
    for data in trainset:
        #data is batch of image tensor and label tensor
        X, y = data

        X = X.to(device)
        y = y.to(device)

        net.zero_grad()
        output = net(X.view(-1, 28*28))
        loss = F.nll_loss(output, y)
        loss.backward()
        optimizer.step()

```
The training_step method delineates the content of the training loop, including last minute pre-processing
the forward pass of a batch of data through the model, and calculation of loss. The backward pass is taken care of
automatically.
```
    def prepare_data(self):
        torchvision.datasets.MNIST('data', train=True, transform=transforms.ToTensor(), download=True)
        # Takes place on 1 gpu, do 1 time operations here
def setup(self, stage=None):
    train_data  = torchvision.datasets.MNIST('data', train=True, transform=transforms.ToTensor(), download=False)
    self.train_split, self.val_split = data.random_split(train_data, [55000, 5000])
    # Takes place on all gpu's

```
The setup and prepare_data methods are used in paralellization. prepare_data is called once, on one device. This should
be used for any computationally intensive data preprocessing or acquisition which should not be repeated on all devices/gpu's.
The setup method is called on every GPU. Its function is to set up that gpu's instance of the data. 
```
    def train_dataloader(self):
       # Data loader
        train_loader = torch.utils.data.DataLoader(dataset=self.train_split, batch_size=batch_size, shuffle=False)
        return train_loader
def val_dataloader(self):
    val_loader = torch.utils.data.DataLoader(dataset=self.val_split, batch_size=batch_size,  shuffle=False)
    return val_loader

```
The dataloader methods function to load the data into batches, both for training and validation.
```
    def validation_step(self, batch, batch_idx):
        images, labels = batch
        images = images.reshape(-1, 28 * 28)
    # Forward pass
    outputs = self(images)

    loss = F.cross_entropy(outputs, labels)
    return {""val_loss"": loss}

```
The validation step method is the analog to training_step, but for the validation loop.
```
    def validation_epoch_end(self, outputs):
        # outputs = list of dictionaries
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'avg_val_loss': avg_loss}
        # use key 'log'
        return {'val_loss': avg_loss, 'log': tensorboard_logs}
def configure_optimizers(self):
    return torch.optim.Adam(self.parameters(), lr=learning_rate)

```
The validation_epoch_end method is called at the end of a validation epoch, calculates metrics such as loss 
returns in dict. The configure_optimizers method sets the optimizer, in this case Adam.
```
class My_NN(pl.LightningModule):
    def init(self, ...
def forward(self, ...

def training_step(self, ...

def prepare_data(self, ...        # if doing mutli gpu

def setup(self, ...               # if doing mutli gpu

def train_dataloader(self, ...

def val_dataloader(self, ...

def validation_step(self, ...

def validation_epoch_end(self, ...

def configure_optimizers(self, ...

def <additioanl method 1>(self, ...

def <additioanl method n>(self, ...

if name == 'main':
    model = LitNeuralNet(input_size, hidden_size, num_classes)
    trainer = Trainer(max_epochs=num_epochs, gpus=2, nodes = 1, accelerator='dp')
    trainer.fit(model)
```
After defining the pl.LightningModule class with the methods above, and whatever other custom or pl.LightningModule
are needed, the model is trained using the pytorch_lightning.Trainer class. This class allows one to automate
parellelization to as many gpus and nodes as one has available.
See additional docs at: pytorch-lightning docs"
rc-website-fork/content/userinfo/howtos/rivanna/_index.md,"+++
type = ""howto""
date = ""2021-03-21T15:12:46-05:00""
tags = [
  ""howto"",
  ""rivanna"",
]
categories = [""howto""]
draft = false
title = ""How-To Guides for UVA HPC Users""
description = ""How-Tos for UVA HPC""
author = ""RC Staff""
layout = ""single""
+++
Guides

Building compiled code
Using make
Building and running MPI Code
Bioinformatics on UVA HPC
Clear OOD Session Files
Convert Jupyter Notebook to PDF
Convert Jupyter Notebook to Python Script
Custom Jupyter kernels
Loading Modules in Jupyter
Docker images on UVA HPC
Adding packages to a container
Migrate Python packages
Launch RStudio Server from an Apptainer container


More Documentation
Connecting

Using SSH
Using a browser
Using FastX

Jobs / Slurm / Queues

Slurm Overview
Queues

Storage and File Transfer

Storage overview
Data transfer methods

Allocations

Allocations Overview
"
rc-website-fork/content/userinfo/howtos/storage/globus-cli.md,"+++
type = ""howto""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""howto"",
  ""storage"",
]
date = ""2020-03-09T13:53:46-05:00""
tags = [""howto"",""storage"",""globus"",""cli""]
draft = false
shorttitle = ""Globus CLI""
title = ""Globus Command-Line Interface""
description = ""Using Globus through its command-line interface.""
author = ""RC Staff""
+++
Install the Globus CLI
The Globus CLI is available through the pip installer:
If you have administrator privileges type
{{< code >}}
pip install globus-cli
{{< /code >}}
Otherwise use
{{< code >}}
pip install --user globus-cli
{{< /code >}}
The project is open source, so you can also download the source at https://github.com/globus/globus-cli
If you would like to use the CLI from UVA HPC, please follow these directions below.
Authenticate using the Globus CLI
Log in against your institutional authentication provider. In the case of UVA, we use NetBadge for signing in:
{{< code >}}
globus login
{{< /code >}}
This will open a page in your web browser where you select your institution and proceed to log in:

Select ‚ÄúUniversity of Virginia‚Äù and then click Continue. You are then taken to an authorization page where you agree to allow the Globus CLI to access your Globus account. Click on Allow.

You may now close your browser window. After you log in, there is one more step to complete authentication. Now, use the following command:
{{< code >}}
globus session consent ""urn:globus:auth:scope:transfer.api.globus.org:all[*https://auth.globus.org/scopes/af187d15-768f-4449-8670-d00e1eb1ce6a/data_access]""
{{< /code >}}
This will open the same browser that pops up when you log in. Follow the same steps to complete the authentication. Once done, you may close the browser. Your command-line tools are now authenticated and ready to use.
Basic Commands
Issue the base command for the tools and you will see the primary set of commands:
{{< code >}}
globus
{{< /code >}}
{{< code >}}
  --jmespath, --jq TEXT     A JMESPath expression to apply to json output.
                            Takes precedence over any specified '--format' and
                            forces the format to be json processed by this
                            expression
  --map-http-status TEXT    Map HTTP statuses to any of these exit codes:
                            0,1,50-99. e.g. ""404=50,403=51""
Commands:
  bookmark        Manage endpoint bookmarks
  config          Manage your Globus config file. (Advanced Users)
  delete          Submit a delete task
  endpoint        Manage Globus endpoint definitions
  get-identities  Lookup Globus Auth Identities
  list-commands   List all CLI Commands
  login           Log into Globus to get credentials for the Globus CLI
  logout          Logout of the Globus CLI
  ls              List endpoint directory contents
  mkdir           Make a directory on an endpoint
  rename          Rename a file or directory on an endpoint
  task            Manage asynchronous tasks
  transfer        Submit a transfer task
  update          Update the Globus CLI to its latest version
  version         Show the version and exit
  whoami          Show the currently logged-in identity.
{{< /code >}}
For a full list of commands available:
{{< code >}}
globus list-commands
{{< /code >}}
List/Search Endpoints using the Globus CLI
Find a Globus endpoint. Here is how you might find the UVA Main DTN:
{{< code >}}
globus endpoint search ""uva#main""
ID                                   | Owner            | Display Name
------------------------------------ | ---------------- | ------------
c4d80096-7612-11e7-8b5e-22000b9923ef | uva@globusid.org | uva#main-DTN
{{< /code >}}
Or search more broadly for all UVA endpoints in Globus:
{{< code >}}
globus endpoint search ""uva""
ID                                   | Owner                | Display Name   
------------------------------------ | -------------------- | -----------------
c4d80096-7612-11e7-8b5e-22000b9923ef | uva@globusid.org     | uva#main-DTN   
67b9cb38-301c-11e7-bcac-22000b9a448b | uva@globusid.org     | uva#portable-DTN 
e1c6b195-6d04-11e5-ba46-22000b92c6ec | uvastro@globusid.org | uvastro#almuhit
31a68704-2422-11e6-bfeb-22000b1701d1 | uvastro@globusid.org | uvastro#scandium 
7bb92d80-6d04-11e5-ba46-22000b92c6ec | uvacse@globusid.org  | uvacse#fir     
de463ced-6d04-11e5-ba46-22000b92c6ec | uvastro@globusid.org | uvastro#tupungato
de463ce4-6d04-11e5-ba46-22000b92c6ec | uvastro@globusid.org | uvastro#helix  
a9a9ae5d-6d04-11e5-ba46-22000b92c6ec | uvacse@globusid.org  | uvacse#cooper  
df70ec7d-6d04-11e5-ba46-22000b92c6ec | uvastro@globusid.org | uvastro#cavi   
24b0ca0c-3013-11e7-bcab-22000b9a448b | ars9ac@virginia.edu  | UVA Portable DTN 
{{< /code >}}
For transfers and file operations, reference endpoints by their unique ID. Names are only convenient tags to help humans differentiate between endpoints.
Traverse Directory Trees using the Globus CLI
Once you know the ID of a specific endpoint, you can list directories visible to you. Here are some paths open to users of the HPC cluster:
{{< code >}}
globus ls c4d80096-7612-11e7-8b5e-22000b9923ef
home/
nv/
scratch/
{{< /code >}}
To drill deeper, append directories to the endpoint ID with a colon : 
{{< code >}}
globus ls c4d80096-7612-11e7-8b5e-22000b9923ef:home/mst3k/
directory1/
directory2/
archive1.tar.gz
file1.txt
file2.txt
my-file.txt
{{< /code >}}
If we would like to transfer a file, we will need the full Globus ID and path of the source file:
{{< code >}}
c4d80096-7612-11e7-8b5e-22000b9923ef:home/mst3k/archive1.tar.gz
{{< /code >}}
Transfer Files using the Globus CLI
To transfer a file you will also need the ID and path to a destination directory and new filename ‚Äì the place to which you would like to copy the remote file from above. It should look something like this:
{{< code >}}
39e0bf8a-3037-11e7-bcae-22000b9a448b:/home/user1/archive1.tar.gz
{{< /code >}}
With the full path to a source file and a full path to a destination, we can now request a transfer. The simplest form of a transfer request looks like this:
{{< code >}}
globus transfer c4d80096-7612-11e7-8b5e-22000b9923ef:home/mst3k/my-file.txt c4d80096-7612-11e7-8b5e-22000b9923ed:nv/vol179/staff/my-new-file.txt
Message: The transfer has been accepted and a task has been created and queued for execution
Task ID: 94d15980-9c94-11e7-acbc-22000a92523b
{{< /code >}}
Note: If you wish to encrypt your transfer, add the --encrypt flag:
{{< code >}}
globus transfer --encrypt c4d80096-7612-11e7-8b5e-22000b9923ef:home/mst3k/my-file.txt c4d80096-7612-11e7-8b5e-22000b9923ed:nv/vol179/staff/my-new-file.txt
{{< /code >}}
Get the Status of a Transfer
Using the Task ID returned from a request, you can get the status of a task:
{{< code >}}
globus task show 94d15980-9c94-11e7-acbc-22000a92523b
Label:                   None
Task ID:                 94d15980-9c94-11e7-acbc-22000a92523b
Is Paused:               False
Type:                    TRANSFER
Directories:             0
Files:                   1
Status:                  SUCCEEDED
Request Time:            2017-09-18 17:12:43+00:00
Faults:                  0
Total Subtasks:          1
Subtasks Succeeded:      1
Subtasks Pending:        0
Subtasks Retrying:       0
Subtasks Failed:         0
Subtasks Canceled:       0
Subtasks Expired:        0
Completion Time:         2017-09-18 17:12:44+00:00
Source Endpoint:         uva#main-DTN
Source Endpoint ID:      c4d80096-7612-11e7-8b5e-22000b9923ef
Destination Endpoint:    uva#main-DTN
Destination Endpoint ID: c4d80096-7612-11e7-8b5e-22000b9923ed
Bytes Transferred:       2812
Bytes Per Second:        2468
{{< /code >}}
Script Transfers Against the Globus CLI
Using the commands above, automated file transfers should not be difficult if run under a user account that has already authenticated. A simple bash script run via cron should work well for automated file shipments. Each shipment will trigger an automatic success/failure email, so there is no need to set up additional notifications.
Single file transfers
Transfer a single file at a time to another DTN, via script:
{{< gist nmagee d9f606ff7edfe1710ce81f1eb23ca654 >}}
Folder sync transfers
Synchronize an entire folder and all contents with another DTN, via script:
{{< gist nmagee 6f4ad4d32dbd0415528d1fb11242fd09 >}}
Run your script:
{{< code >}}
user@host$ ./sync-directories.sh 
{{< /code >}}
Either operation should result in a confirmation message like this:
{{< code >}}
Message: The transfer has been accepted and a task has been created and queued for execution
Task ID: 5ffe3058-5543-11e8-90ce-0a6d4e044368
{{< /code >}}
Automating your scripts

In a Unix/Linux/macOS environment, you can set any script or application to run on any schedule using cron.
In Windows, we recommend writing the above into a PowerShell script, which can then be scheduled using the ‚ÄúTask Scheduler‚Äù tool from the Windows menu.

More Information

See our main page introducing Globus Connect
See the in-depth Globus CLI Documentation or Globus CLI Reference
Globus also has an API and Python SDK.
For other technical details, see Globus Documentation.

Use the Globus-CLI from your UVA HPC {#use-the-globus-cli-from-your-hpc-account}

Load the globus-cli module and its dependencies:

{{< code >}}
module load gcc openmpi globus_cli
{{< /code >}}

Authorize globus-cli

Run this command:
{{< code >}}
globus login
{{< /code >}}
You will then be given an Oauth2 login URL. Start a Web browser, either through FastX or through an X11 server on your local computer.  Copy and paste this URL into the web browser, and authorize your connection as instructed in the topic above.
Be sure to give your authorization a useful name, such as -rivanna, i.e. mst3k-rivanna. This will help you distinguish it in your list of Globus authorizations.
{{% callout %}}
Return to the top of the page for information about using the Globus CLI generally.
{{% /callout %}}
When referencing the globus binary in scripts, you may want to issue a which globus command to find its path as that may change over time with new versions. If used with backticks this can be used to populate a variable in a script:
{{< code >}}
globus=which globus
{{< /code >}}
Please note that users are not permitted to run cron jobs on the HPC system.  Scheduling should be done from another system."
rc-website-fork/content/userinfo/howtos/storage/aws-s3.md,"+++
type = ""howto""
categories = [
  ""howto"",
  ""storage"",
]
date = ""2020-12-17T13:53:46-05:00""
tags = [""howto"",""storage"",""aws"",""cloud"",""cli"",""boto3"",""curl""]
draft = false
shorttitle = ""AWS CLI""
title = ""Transfer Files Using Amazon S3""
description = ""Access AWS resources through its command-line interface.""
author = ""RC Staff""
+++
Setup
You will need to install and configure the awscli package in order to access objects in S3.
Install the AWS CLI
The AWS CLI is available through the pip/pip3 installer:
If you have administrator privileges type
{{< code-snippet >}}
pip install awscli
{{< /code-snippet >}}
Otherwise use
{{< code-snippet >}}
pip install --user awscli
{{< /code-snippet >}}
The project is open source, so you can also download the source at https://github.com/aws/aws-cli
{{% alert-green %}}
UVA HPC Users have two options:

Load the awscli module:
{{< code-snippet >}}
module load awscli
{{< /code-snippet >}}
  If you need a different version, install it in your user directory:
{{< code-snippet >}}
pip install --user awscli==1.19.29
{{< /code-snippet >}}

{{% /alert-green %}}
Authenticate the CLI to Amazon Web Services
Once the aws package is installed, you will need to configure it:
{{< code-snippet >}}
aws configure
{{< /code-snippet >}}
You will be prompted to enter four values:

Your AWS access keys: aws_access_key_id / aws_secret_access_key: These can be obtained from within your AWS account. See this AWS documentation for how to retrieve these two keys. If you are the root account owner, go to your account security settings to retrieve these. It is highly advised NOT to use root credentials for access in this way.
Your default AWS region: Unless you know your S3 bucket is scoped to another region, enter us-east-1.
Your default output format: Your options are text, json and table output.

The AWS account you enter in these steps must have at least read permissions to access the resources you want to download.
Access S3 using the AWS CLI

aws s3 ls - List Buckets
```
aws s3 ls
2018-01-18 16:16:17 mybucket1
2018-06-20 13:18:38 mybucket2
2017-08-15 09:35:08 my-new-bucket
2018-06-28 09:19:29 my-other-bucket
```

aws s3 mb - Make a new bucket
aws s3 mb s3://mybucket3
Remember that S3 bucket names must be globally unique from all other AWS customers.

aws s3 rm - Remove a bucket
aws s3 rm s3://mybucket3
Remember that S3 buckets must be emptied of all contents before they can be removed. Once removed the bucket name is available for other users.

aws s3 ls - List the contents of a bucket
aws s3 ls s3://mybucket1/
                           PRE keys/
                           PRE status/
                           PRE zip/
2020-06-26 09:50:08      10451 index.json
2020-06-26 09:50:09         64 robots.txt
{{% alert-blue %}}
FOLDERS IN S3 - Contrary to how it appears, S3 is not a file system in the ordinary sense. Instead, it is a web-based, API-driven object storage
service containing KEYS and VALUES. The key (name) of a file (object) is arbitrary after the name of the bucket itself, but must obey certain rules such as
using no unusual characters. The typical form of grouping objects under ""subfolders"" uses the same naming convention as regular filesystems with a ""key"" such as: 
mybucket1/folder/subfolder/filename.txt
The value (contents) of that key are the actual contents of the file itself. But it is important to remember that folders as they appear in the path of
an S3 object are simply a mental convenience.
{{% /alert-blue %}}

aws s3 cp - Download a file
aws s3 cp s3://mybucket1/robots.txt ./

aws s3 cp - Upload a file
aws s3 cp local-file.txt s3://mybucket1/
To upload a file and make it publicly available via HTTPS, add an acl property to it:
aws s3 cp --acl public-read local-file.txt s3://mybucket1/
{{}}
Files that have been made public-readable can be retrieved using other command-line tools such as curl and wget. S3 is an HTTPS web
endpoint, and without the need for authentication you can work with them as if they were regular public web resources:


curl -O https://mybucket1.s3.amazonaws.com/path/to/myfile.txt

{{}}

aws s3 sync - Synchronize to/from an S3 bucket
aws s3 sync ./local-dir s3://mybucket1/remote-dir/
You can synchronize between any source/destination so long as at least one of them is S3:

Sync from local workstation to S3
Sync from S3 to local workstation
Sync from S3 bucket to another S3 bucket


aws s3 rm - Remove a file from S3
aws s3 rm s3://mybucket1/file-not-wanted.pdf

aws s3 mv - Move a file within S3
aws s3 mv s3://mybucket1/original-file.csv s3://mybucket1/moved-file.csv

aws s3 presign - Presign an S3 URL
In some cases users want to share a file with a remote party without creating access keys or for a limited amount of time. The presign feature
is useful in this case since it creates a unique signed URL that expires after a set amount of time. 
To set the expiry time, calculate the length of time you want the signature to last in seconds. This value will be used with the --expires-in flag.
```
aws s3 presign --expires-in 600 s3://mybucket1/path/file-to-share.tar.gz
https://mybucket1.s3.amazonaws.com/path/file-to-share.tar.gz?AWSAccessKeyId=AKICMAJHNXKQDLN34VZJ&Signature=sCH2pRjn7M02P5D8JnAyBq%2FP7kQ%3D&Expires=1593196195
```
{{}}
NOTE: This URL works regardless of who uses it, and requires no authentication. Therefore, be careful with the distribution of signed URLs, and keep their expiry time as short as possible.
{{}}

Access S3 using boto3 in Python
The boto3 package is the standard library enabling programmatic access to AWS using Python. boto3 can access all AWS services and is helpful for creating,
managing, or removing remote resources and infrastructure dynamically. The steps below refer to using boto3 for working with files in S3.
Install boto3
{{< code-snippet >}}
pip install boto3
{{< /code-snippet >}}
boto3 will obtain its credentials from one of a few various locations:

Hard-coded credentials within the application code itself. This is not recommended.
Inherited credentials from the ~/.aws/ directory within your home directory. This is common for remote development.
Injected as environment variables of the environment in which your code is running.
Inherited credentials from the IAM role of the EC2 instance running your code. This is a best practice for production systems in AWS.

Use boto3
Import the library as you would for any other Python package, and set up a client or resource for the AWS service:
```
import boto3
s3 = boto3.client('s3')
```

Upload a file to S3
{{< gist nmagee c2be9caa4479bb11bb1b6097d7269946 >}}

Download a file from S3
{{< gist nmagee a8b42a126235a0366f7472efd4965d18 >}}

More Information about boto3
Documentation is available here."
rc-website-fork/content/userinfo/howtos/storage/drive-mapping.md,"+++
type = ""howto""
images = [
  ""/2016/10/image.jpg"",
]
categories = [
  ""howto"",
  ""storage"",
]
date = ""2022-09-06T08:37:46-05:00""
tags = [""howto"",""storage"",""mapping""]
draft = false
shorttitle = ""Drive Mapping""
title = ""Drive Mapping on Windows or Mac OSX""
description = ""Mapping Research Standard or Research Project Storage to your Desktop""
author = ""RC Staff""
+++
Research Standard and Research Project storage can be mapped to your Windows or Mac computer as a remote drive.  If you are off Grounds you must be running a VPN, such as the UVA Anywhere or the More Secure VPN from ITS.  We recommend the More Secure VPN if that is available to you.
Windows

Open a File Explorer page.  In the left column, right-click on This PC.  In the drop-down box that appears, look for the Map Network Drive option. If you do not see this option, click on Show more Options, and then click on Map network drive.... 
When the Map Network Drive dialog box appears, select a letter for the drive.This will be the location on your PC where you will be able to access your storage.

Also in the Map Network Drive dialog box, type the path for your storage location in the Folder field. 

For Research Standard storage, the path starts with \\standard.hpc.virginia.edu\ followed by your storage share name, for example \\standard.hpc.virginia.edu\mylab-storage
If you have ceph storage (standard.hpc.virginia.edu) you may have to enter ESERVICES\mst3k (with your own user ID) rather than your user ID alone.
For Research Project storage, the path starts with \\project.hpc.virginia.edu followed by your storage share name, for example \\project.hpc.virginia.edu\mylab-storage. 



If you want the mapped drive to remain after a shut-down or reboot, check the box for Reconnect at sign-in

If your laptop does not use you UVA ID and password for logging in, check the box for Connect using different credentials.  When it asks you to authenticate, use your UVA id (e.g. mst3k) and your Eservices password.
Select Finish.

{{% callout %}}
Note that you must use backslashes even if the path provided to you used forward slashes.
{{% /callout %}}

Mac OSX

From the Finder menu, select Go->Go To Folder->Connect To Server.  A dialog box should appear with smb:// filled in. 

Type the path you were given.

For Research Standard storage the path starts with //standard.hpc.virginia.edu/ followed by your storage share name, for example smb://standard.hpc.virginia.edu/mylab-storage
        * If you have ceph storage (standard.hpc.virginia.edu) you may have to enter eservices\mst3k (with your own user ID) rather than your user ID alone.
For Research Project storage the path starts with //project.hpc.virginia.edu followed by your storage share name, for example smb://project.hpc.virginia.edu/mylab-storage.  



Enter your Eservices credentials when prompted, then click Connect.


{{% callout %}}
Note that for Mac OSX we use forward slashes. 
{{% /callout %}} 

Linux
Special arrangements must be made with Research Computing to export shares to a Linux workstation."
rc-website-fork/content/userinfo/howtos/storage/_index.md,"+++
type = ""howto""
date = ""2020-06-21T15:12:46-05:00""
tags = [
  ""howto"",
  ""storage"",
]
categories = [""howto""]
draft = false
title = ""How To Tips for Storage ""
description = ""How Tos for Storage""
author = ""RC Staff""
layout = ""single""
+++

Map your Research Standard or Research Project Storage to your Desktop
Use Globus from the Command Line
Work with files in Amazon S3
"
rc-website-fork/content/service/dtc/contact.md,"+++
author = ""RC Staff""
description = """"
title = ""Digital Technology Core Contact Information""
date = ""2025-04-29T00:00:00-05:00""
draft = false
tags = []
categories = []
images = [ ]
+++
To contact the DTC for any reason:
* email rc-dtc@virginia.edu
* submit a ticket to the Digital Technology Core"
rc-website-fork/content/service/dtc/grants.md,"+++
author = ""RC Staff""
description = """"
title = ""Digital Technology Core Seed Grants""
date = ""2025-04-2T00:00:00-05:00""
draft = false
tags = []
categories = []
images = [ ]
+++

DTC Mission and Seed Grants
The Digital Technology Core's mission is to provide UVA researchers with support for research utilizing mobile apps and wearable devices. As part of this mission the DTC offers seed grants to research faculty with two primary aims:
1. Launch new research projects in pursuit of competitive grants
2. Develop new DTC software functionality to support UVA researchers
Seed Grant Application Guidelines

Eligibility: At least one principal investigator must hold faculty status at UVA to be eligible for a DTC seed grant.
Maximum Duration: DTC seed grant projects should have a one year timeline; however, they may last two or three years if a strong case can be made.
Approved Activities: The proposal should be related to: (1) adding functionality to the DTC ecosystem in-support of research (e.g., collecting data from a new wearable device or new functionality in the DTC app) or (2) running a study in the existing DTC ecosystem.
Scope: DTC seed grants are intended to provide approximately $10,000 in DTC services. In practice, this provides enough support for one new feature in the DTC ecosystem (e.g., data collection from a new device or a new kind of user interaction in DTC apps) and direct support for a two month study.

Consultation for Seed Grant Applications
Before you apply we highly recommend that you have a consultation meeting with the DTC. We can help you improve your application and appropriately scope your proposal. You can request a consultation by contacting us through any avenue on our contact page.
Application and Response Timelines
Applications for DTC seed grants can be submitted at any time. We will respond to your proposal within one month.
Application Content and Template
To apply for a DTC seed grant, the PI will submit an application (2 pages maximum not including references) that includes:

The PI's name, computing ID, and department
A title for the project
A description of the project and its goals
A description of how the seed grant, and the DTC, will support the goals of the project
A timeline for completing the proposed project
A plan for how the seed grant will lead to future support (e.g., external grants) for this or subsequent projects

We provide a seed grant application template. You are not required to use this template.
Evaluation of Applications
Applications will be evaluated on:

The completeness of the submission
The clarity of the descriptions - the reviewers may not be familiar with the PI's discipline
How this grant will advance your research or will enhance the community of researchers at UVA

Disbursement of Seed Grants
Seed grant recipients will not receive any cash or vouchers from the DTC. Instead, DTC staff will work with recipients to allocate DTC staff time to your project. DTC staff will continue to work with you until all proposed software functionality has been provided and/or the research study has been completed.
Required Publicity for Application Awards
PI's must acknowledge the DTC (and its parent organization Research Computing) in any publication, presentation, or poster that results from the work done for the seed grant. Additionally, at the end of the award, the PI's must agree to have a blurb about the research project posted on the DTC website."
rc-website-fork/content/service/dac/awards.md,"+++
title = ""DAC Analytics Resource Awards""
draft = false
date = ""2023-10-10""
images = [""""]
author = ""Staff""
tags = [""dac"",""award""]
+++
The mission of the Data Analytics Center (DAC) is to provide UVA researchers with support for the management and analysis of large datasets on Research Computing platforms.
To fulfill this mission and to empower faculty in advancing their research, we offer Awards for access to Research Comuting (RC) resources.  These Awards aim to 
i) launch new research projects,
ii) promote interdisciplinary collaborations,
iii) cultivate communities of researchers with shared interests in data and analysis methods.
The Awards
The DAC offers two types of Awards:  Small Awards (up to $10,000 worth of services) and Large Awards (up to $100,000 of services).
Recipients of the Awards will receive vouchers that may be used toward RC services, such as collaborations with DAC team members or purchase of data storage, or for support from any of the UVA Cores.
Small Data Analytics Resource Awards
The Small Analytics Resource Awards are intended to support research tasks for an individual PI or lab. 
Large Data Analytics Resource Awards
The Large Analytics Resource Awards are aimed to build data or analytics infrastructure with the potential to impact multiple researchers at UVA. Funding up to $100,000 in resources, this award aims to enable the hosting of data collections, the movement of data or the development of analytics infrastructure that will enable the research workflows of a broad community at UVA.¬†In addition award funds may be allocated for in-depth collaborations with DAC team members.
Award Guidelines
Eligibility:  The principal investigator must hold faculty status at UVA to be eligible for an Analytics Research Award.
Funding Limit:  Although the Awards will be in the form of vouchers, the use of the voucher may not exceed the amount of the Award.
Maximum Proposal Duration:  Eighteen  months; however, Large Awards may last two or three years if a strong case may be made.
Approved Activities:  The activities in the proposal should be related to the acquisition, storage, or analysis of data as it relates to a research project. Priority will be given to proposals that request collaborative services of the Research Computing team.

Timelines for Small Awards: 
We accept Small Award proposals at any time throughout the year; however, your best chance for receiving an award will be if you submit it at the end of a quarter.  The following table shows the upcoming deadlines for proposals.







Proposal  Deadline
Award  Announcements
Award  Start Date 


30 Jun. 2025
21 Jul. 2025
11 Aug. 2025


29 Sep. 2025
20 Oct. 2025
10 Nov. 2025


29 Mar. 2026
19 Apr. 2026
9 May 2026


29 Jun. 2026
20 Jul. 2026
10 Aug. 2026




Timelines for Large Awards: 
We will accept proposals for the Large Awards on specified dates.  The following table provides the deadlines for the upcoming Large Award submissions.







Proposal  Deadline
Award  Announcements
Award  Start Date 


20 Oct. 2025
8 Dec. 2025
12 Jan. 2026


1 Jun. 2026
20 Jul. 2026
24 Aug. 2026




Before you apply:  We highly recommend that you have a consultation meeting with a DAC member to discuss the services that you are planning and what the budget will look like for those services. You can request a consultation by submitting this 
  online support request form  .


Application Requirements for Small Awards:  
To apply for the Small Analytics Research Award, the PI will submit a proposal (2 pages maximum, Calibri 11 pt. font) that includes:

The PI‚Äôs name, computing ID, and department,
A title for the project,
A description of the project and its goals,
A description of the type(s) of data collected for the project and how the data will be used to meet the goals of the project,
A description of how the Analytics Research Award will support the goals of the project, including the use of vouchers to obtain services,
A plan for how the Analytics Research Award will lead to future support (e.g., external grants or other funding) for this or subsequent projects. Priority will be given to proposals that state how a DAC team member will be included in future funding proposals.
An attachment with a budget showing the services and expected costs that the Award would cover.  The attachment will not count toward the two-page limit.

Application Requirements for Large Awards:  
The objective of the Large Award is to develop comprehensive data resources that will foster a collaborative spirit among researchers at UVA.  Therefore, preference will be given to projects that include principal investigators or co-investigators from diverse fields.
For the Large Analytics Research Award, the proposal must include the following:


A cover page that includes the project title, and name, title, department, and e-mail address of each the PI(s) and Co-I(s). The Principal Investigator(s) will be the individual(s) responsible for the scientific or technical direction of the project. If more than one PI is included, the first one listed will have primary responsibility for communications with the DAC and the submission of reports. Any listed Co-Is will not have overall responsibility or spending authority as the PI. 


A project summary or abstract, limited to 1 page


A research plan, limited to 3 pages and addresses the following:


Specific objectives of the project, including how the project spans labs, departments, or schools

Approach and workplan, including the responsibilities of DAC team members or the need for resources, such as data storage

Outcomes, including planned proposals for external funding 


Budget. Please contact the DAC (RC-DAC@virginia.edu) to request a consultation for preparing the  budget.

Timeline (include milestones) for completing the scope of the work
Works Cited/References
List of targeted funding sources for follow-up work (for each source include funding agency, anticipated funding request, deadline, link to RFP if available) 

Please use Calibri, 11-point font, single-spaced with 1-inch page margins. All pages, except for the cover page, should be numbered consecutively throughout the proposal.
Submission Process: Email your proposal, in PDF format, to RC-DAC@virginia.edu.  The subject line for the email must be ""Analytics Resource Proposal"". You may include a brief cover letter in the body of the email.
We encourage you to follow these instructions precisely to ensure that your proposal is received and processed correctly. After your email is received, we will secure a copy of your proposal and review that copy. We will not consider any changes that you make to the proposal after we have received it.
Evaluation Criteria: The proposals will be evaluated on:

The completeness of the submission,
The clarity of the descriptions ‚Äì the reviewers may not be familiar with the PI‚Äôs discipline,
The diversity of the data,
How this Award will advance your research or will enhance the community of researchers at UVA.

Voucher Disbursement
After the Awards recipients have been announced, the Data Analytics Center will work with the PI to determine the disbursement of the Award vouchers.
Publicity & Communication

The PI must acknowledge Research Computing in any publication, presentation, or poster that results from the work done on the Award.
At the end of the Award, the PI must agree to have a blurb about the research project posted on the Research Computing website.
"
rc-website-fork/content/service/dac/past_awardees.md,"+++
title = ""DAC Award Winners""
draft = false
date = ""2025-03-06""
images = [""""]
author = ""Staff""
tags = [""dac"",""award""]
+++
2025

Douglas Bayliss, Department of Pharmacology
Characterizing RTN Population Activity Following Hypercapnia and Drub Challenges Using in vitro Calcium Imaging
Zezhou Cheng, Department of Computer Science
Detect, Track, and Relate Anythin in 3D
Nick Guagliardo, Department of Pharmacology
Analysis of Calcium Activity in Ex Vivio Adrenal Slices
Yangfeng Ji, Department of Computer Science
Legal Document Analysis via Large Language Models: A Preliminary Study on Constitutional Court of Colombia
Sheng Li, School of Data Science
Towards Efficient and Reliable Reasoning with Large Language Models
Thomas Loughran, Division of Hematology & Oncology
LGL Leukemia Medical Data Extraction and Search Program for Translational Research
Nikhil Shukla, Charles L. Brown Department of Electrical and Computer Engineering, Department of Materials Science and Engineering
Scalable Combinatorics Accelerator Inspired by Physics
Bon Trinh, Division of Experimental Pathology
Virtual Compound Screening for Cancer-Causing Proteins

2024

Alan Bergland, Department of Biology
Drosophila Evolution through Space and Time 2.0 -- Data Repository Updates
Josh Colston, Division of Infectious Diseases & International Health
Pipeline Feasibility Study and Dashboard Implementation for the Planetary Child Health & Enterics Observatory Plan (Plan-EO)
Matthew Crawford, Division of Infectious Diseases & International Health
Research Computing to Guide the Functional Optimization of Anti-bacterial Peptides
Gretchen Gamrat, McIntire School of Commerce
To Buy or Not to Buy: Effectiveness of Boycotts in Consumer Goods
Andres Norambuena, Department of Biology
Using AlphFold2 to Unveil Seminal Molecular Dysfunctions in Alzheimer's Disease
Alexander Podboy, Division of Gastroenterology and Hepatology
Outcomes of Dexmedetomidine during ERCP on post-ERCP Pancreatitis
David Rekosh, Department of Microbiology, Immunology, and Cancer Biology
Analysis of ONT Long-Read Data to Elucidate Differential mRNA Isoform Expression Induced by Viral and Cellular Proteins
Heman Shakeri, School of Data Science
CLEAR-iOP: Comprehensive Learning Engine for Assessment of Risk in IntraOcular Pressure
Sarah Siegrist, Department of Biology 
Identifying Cell Types within Three Dimensional Tissues across Populations
Jeffrey Smith, Department of Biochemistry and Molecular Genetics
Dissecting the Role of Acetyl-Coenzyme A Synthetase Coding Genes in Calori Restriction-Driven Longivity
Cynthia Tong, Department of Psychology
Intensive Longitudinal Data Analysis and Compositional Predictors
Justin Yaworsky, Department of Emergency Medicine
STEMI Database Development and Deep Learning ECG Algorithm for STEMI Diagnosis in Emergency Department Patients

2023

Steven Johnson, McIntire School of Commerce
Distribution and Discovery of Digital Information (3Di)
Jingjing Li, McIntire School of Commerce
Responsible-by-Design: Combating Biases in Generative AI Applications
"
rc-website-fork/content/service/dac/ai.md,"+++
images = [""""]
author = ""Staff""
description = """"
date = ""2024-06-17T00:00:00-05:00""
title = ""AI as a Research Tool""
draft = false
tags = [""AI""]
+++

AI is more than ChatGPT







Artificial Intelligence, or AI, is a combination of computing platforms and programs that emulate human thought processes for making decisions and solving complex problems. The technology behind AI is much more than GenAI (i.e., applications like ChatGPT or CoPilot).  While ChatGPT is a popular example of AI because it simulates conversations with people, AI itself includes many other technologies and applications that can be used to explore research data.  Other terms that may be used with AI, but have subtle differences, are Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). 



Using AI to Explore Your Data
AI can be used to explore research data by employing sophisticated algorithms to analyze vast datasets efficiently and accurately. Through machine learning techniques, AI can identify complex patterns, correlations, and trends that might be overlooked by human analysis. This enables researchers to generate new hypotheses, validate existing theories, and uncover novel insights. For instance, in medical research, AI can sift through millions of patient records to discover potential treatment effects, predict disease outbreaks, and personalize patient care. Additionally, AI-powered tools can automate data collection, organization, and visualization, thereby streamlining the research process and allowing scientists to focus on interpreting the results and advancing their fields.
How RC can help you



Available Platforms


For AI algorithms to perform efficiently, they should run on computers with graphics processing units (GPUs). Although GPUs were originally designed to process images, their ability to process thousands of mathematical computations makes them ideal for simulating the relationships within your data. 

Research Computing has many GPUs available on our standard cluster (i.e., Rivanna/Afton) and on our high security systems (i.e., Ivy/Rio). Access to the GPUs on Rivanna/Afton is available at no cost to all researchers with an allocation. For those who have been provisioned on our high security systems, the GPUs are available at no additional cost. 


If you are writing a grant proposal for research where you will be using GPUs, you can include our [facilities statement](https://www.rc.virginia.edu/userinfo/computing-environments/) to describe the resources available to you.



Available Software


RC has a variety of AI software packages installed on our standard and high security systems.  These packages were built to use the GPUs on our systems. However, you will need to ensure that you are running the software on a compute node that has GPUs available. For many of the packages, you will need to customize the software to work with your data. 

RC does have a team of consultants who can help with these tasks. To request assistance with using AI software, please submit a [support request form](https://www.rc.virginia.edu/form/support-request/). 



Available Workshops and Tutorials


RC conducts workshops on how to use our platforms and software.  The workshops tend to be offered at the start of each semester.  You can check on upcoming workshops [here](https://www.rc.virginia.edu/education/workshops/). 

Many of our previous workshops have been converted to tutorials.  You can find several of the AI tutorials [here](https://learning.rc.virginia.edu/tag/deep_learning/).  A good starting point for someone new to AI is the tutorial [Machine Learning for Python](https://learning.rc.virginia.edu/notes/python-machine-learning/). 




Available Support


RC provides a variety of support options. You can submit questions to our ticketing system by filling out our [Support Request Form](https://rc.virginia.edu/form/support-request/). 

RC also has virtual office hours on Tuesdays and Thursdays.  The times and Zoom links are available on our [Support Options webpage](https://rc.virginia.edu/support/#office-hours). 

Finally, you may request a consultation or collaboration through the Data Analytics Center (RC-DAC@virginia.edu).  You can even discuss with us the possibility of writing into your grant proposals support from one of the DAC team members.





Additional Information
The UVA library has great information about GenAI.
Plus, any member of the UVA community (i.e., an individual with Netbadge capabilities) has access to Coursera For UVA) and O‚ÄôReilly books, where you can learn more about AI. Courses that you may want to consider are Bayesian Machine Learning from UC Santa Cruz, and any of the Machine Learning courses from Stanford. 
If you prefer books, you could start with Machine Learning with PyTorch and Scikit-Learn by Sebastian Raschka, Yuxi Liu, and Vahid Mirialili, and Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf."
